[sbatch-master] running on nid006931
[sbatch-master] SLURM_NODELIST: nid[006931,006995]
[sbatch-master] SLURM_NNODES: 2
[sbatch-master] SLURM_NODEID: 0
[Master] Master node: nid006931
[Master] World size: 8
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
[srun] rank=0 host=nid006931 noderank=0 localrank=0
[srun] rank=1 host=nid006995 noderank=1 localrank=0
W0518 22:27:13.682000 103761 torch/distributed/run.py:792] 
W0518 22:27:13.682000 103761 torch/distributed/run.py:792] *****************************************
W0518 22:27:13.682000 103761 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0518 22:27:13.682000 103761 torch/distributed/run.py:792] *****************************************
W0518 22:27:15.035000 166897 torch/distributed/run.py:792] 
W0518 22:27:15.035000 166897 torch/distributed/run.py:792] *****************************************
W0518 22:27:15.035000 166897 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0518 22:27:15.035000 166897 torch/distributed/run.py:792] *****************************************
2025-05-18 22:27:27,774 - root - INFO - [Distributed Init] Rank 0 initialized on node 0 on GPU 0.
2025-05-18 22:27:27,796 - root - INFO - [Distributed Init] Rank 4 initialized on node 1 on GPU 0.
[rank0]:[W518 22:27:28.927967700 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W518 22:27:28.617671980 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 22:27:28,268 - root - INFO - [Distributed Init] Rank 6 initialized on node 1 on GPU 2.
[rank6]:[W518 22:27:28.672739189 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 6]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 22:27:28,322 - root - INFO - [Distributed Init] Rank 3 initialized on node 0 on GPU 3.
2025-05-18 22:27:28,322 - root - INFO - [Distributed Init] Rank 1 initialized on node 0 on GPU 1.
[rank3]:[W518 22:27:28.052653269 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W518 22:27:28.052653589 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 22:27:28,328 - root - INFO - [Distributed Init] Rank 7 initialized on node 1 on GPU 3.
2025-05-18 22:27:28,328 - root - INFO - [Distributed Init] Rank 5 initialized on node 1 on GPU 1.
[rank7]:[W518 22:27:28.733151400 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 7]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W518 22:27:28.733150984 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 5]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 22:27:28,362 - root - INFO - [Distributed Init] Rank 2 initialized on node 0 on GPU 2.
[rank2]:[W518 22:27:28.092457429 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 22:27:33,342 - root - INFO - [Rank 0] All ranks ready!
2025-05-18 22:27:33,342 - root - INFO - Distributed training enabled: 8 processes
2025-05-18 22:27:33,342 - root - INFO - Master process: 0 on cuda:0
2025-05-18 22:27:33,342 - root - INFO - Experiment args: Namespace(dataset='/capstor/scratch/cscs/kasparr/project/train_data.parquet', dataset_type='padded', pretokenized=False, tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=10, profile=False, profile_step_start=10, profile_step_end=10, grad_max_norm=1, model_dtype='bf16', compile=False, distributed=True, backend='nccl', find_unused_parameters=False)
2025-05-18 22:27:33,342 - root - INFO - Setting up Tokenizer...
2025-05-18 22:27:33,908 - root - INFO - Setting up DataLoaders...
2025-05-18 22:27:33,908 - root - INFO - Using padded ParquetDataset with on-the-fly tokenization
2025-05-18 22:27:41,261 - root - INFO - Setting up Model...
2025-05-18 22:27:50,101 - root - INFO - Model wrapped with DistributedDataParallel
2025-05-18 22:27:50,102 - root - INFO - Global batch size: 64 (local: 8 Ã— 8 processes)
2025-05-18 22:27:50,102 - root - INFO - Starting training!
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
2025-05-18 22:27:51,524 - root - INFO - Step: 1 | Loss: 11.95 | Tokens per second: 11528.78 | Training tokens per second (%): 3.37 | MFU (%): 10.80 | TFLOPs: 106.77 | Global batch size: 64 | Global tokens/sec: 92230.22 | Global MFU (%): 10.80 | Global TFLOPs: 854.18 | 
2025-05-18 22:27:55,318 - root - INFO - Step: 10 | Loss: 11.93 | Tokens per second: 38869.17 | Training tokens per second (%): 4.30 | MFU (%): 36.40 | TFLOPs: 359.98 | Global batch size: 64 | Global tokens/sec: 310953.38 | Global MFU (%): 36.40 | Global TFLOPs: 2879.87 | 
2025-05-18 22:27:59,914 - root - INFO - Step: 20 | Loss: 11.70 | Tokens per second: 35651.36 | Training tokens per second (%): 4.30 | MFU (%): 33.39 | TFLOPs: 330.18 | Global batch size: 64 | Global tokens/sec: 285210.86 | Global MFU (%): 33.39 | Global TFLOPs: 2641.46 | 
2025-05-18 22:28:04,136 - root - INFO - Step: 30 | Loss: 11.16 | Tokens per second: 38814.28 | Training tokens per second (%): 4.73 | MFU (%): 36.35 | TFLOPs: 359.48 | Global batch size: 64 | Global tokens/sec: 310514.22 | Global MFU (%): 36.35 | Global TFLOPs: 2875.81 | 
2025-05-18 22:28:08,519 - root - INFO - Step: 40 | Loss: 10.09 | Tokens per second: 37383.34 | Training tokens per second (%): 5.05 | MFU (%): 35.01 | TFLOPs: 346.22 | Global batch size: 64 | Global tokens/sec: 299066.68 | Global MFU (%): 35.01 | Global TFLOPs: 2769.78 | 
2025-05-18 22:28:12,723 - root - INFO - Step: 50 | Loss: 9.45 | Tokens per second: 38986.50 | Training tokens per second (%): 5.21 | MFU (%): 36.51 | TFLOPs: 361.07 | Global batch size: 64 | Global tokens/sec: 311892.04 | Global MFU (%): 36.51 | Global TFLOPs: 2888.57 | 
2025-05-18 22:28:16,904 - root - INFO - Step: 60 | Loss: 8.88 | Tokens per second: 39191.24 | Training tokens per second (%): 4.69 | MFU (%): 36.70 | TFLOPs: 362.97 | Global batch size: 64 | Global tokens/sec: 313529.89 | Global MFU (%): 36.70 | Global TFLOPs: 2903.73 | 
2025-05-18 22:28:21,435 - root - INFO - Step: 70 | Loss: 8.39 | Tokens per second: 36165.91 | Training tokens per second (%): 4.03 | MFU (%): 33.87 | TFLOPs: 334.95 | Global batch size: 64 | Global tokens/sec: 289327.31 | Global MFU (%): 33.87 | Global TFLOPs: 2679.58 | 
2025-05-18 22:28:25,621 - root - INFO - Step: 80 | Loss: 7.98 | Tokens per second: 39140.17 | Training tokens per second (%): 4.61 | MFU (%): 36.65 | TFLOPs: 362.49 | Global batch size: 64 | Global tokens/sec: 313121.35 | Global MFU (%): 36.65 | Global TFLOPs: 2899.95 | 
2025-05-18 22:28:29,884 - root - INFO - Step: 90 | Loss: 7.66 | Tokens per second: 38446.18 | Training tokens per second (%): 4.75 | MFU (%): 36.00 | TFLOPs: 356.07 | Global batch size: 64 | Global tokens/sec: 307569.46 | Global MFU (%): 36.00 | Global TFLOPs: 2848.53 | 
2025-05-18 22:28:34,083 - root - INFO - Step: 100 | Loss: 7.52 | Tokens per second: 39021.72 | Training tokens per second (%): 4.15 | MFU (%): 36.54 | TFLOPs: 361.40 | Global batch size: 64 | Global tokens/sec: 312173.72 | Global MFU (%): 36.54 | Global TFLOPs: 2891.17 | 
2025-05-18 22:28:38,415 - root - INFO - Step: 110 | Loss: 7.29 | Tokens per second: 37823.37 | Training tokens per second (%): 4.72 | MFU (%): 35.42 | TFLOPs: 350.30 | Global batch size: 64 | Global tokens/sec: 302586.99 | Global MFU (%): 35.42 | Global TFLOPs: 2802.39 | 
2025-05-18 22:28:42,759 - root - INFO - Step: 120 | Loss: 7.26 | Tokens per second: 37723.45 | Training tokens per second (%): 4.71 | MFU (%): 35.33 | TFLOPs: 349.37 | Global batch size: 64 | Global tokens/sec: 301787.64 | Global MFU (%): 35.33 | Global TFLOPs: 2794.98 | 
2025-05-18 22:28:46,977 - root - INFO - Step: 130 | Loss: 7.18 | Tokens per second: 38848.32 | Training tokens per second (%): 4.61 | MFU (%): 36.38 | TFLOPs: 359.79 | Global batch size: 64 | Global tokens/sec: 310786.59 | Global MFU (%): 36.38 | Global TFLOPs: 2878.33 | 
2025-05-18 22:28:51,385 - root - INFO - Step: 140 | Loss: 7.12 | Tokens per second: 37179.55 | Training tokens per second (%): 5.28 | MFU (%): 34.82 | TFLOPs: 344.34 | Global batch size: 64 | Global tokens/sec: 297436.38 | Global MFU (%): 34.82 | Global TFLOPs: 2754.69 | 
2025-05-18 22:28:55,540 - root - INFO - Step: 150 | Loss: 7.04 | Tokens per second: 39433.29 | Training tokens per second (%): 4.44 | MFU (%): 36.93 | TFLOPs: 365.21 | Global batch size: 64 | Global tokens/sec: 315466.32 | Global MFU (%): 36.93 | Global TFLOPs: 2921.67 | 
2025-05-18 22:28:59,755 - root - INFO - Step: 160 | Loss: 7.04 | Tokens per second: 38879.69 | Training tokens per second (%): 5.21 | MFU (%): 36.41 | TFLOPs: 360.08 | Global batch size: 64 | Global tokens/sec: 311037.52 | Global MFU (%): 36.41 | Global TFLOPs: 2880.65 | 
2025-05-18 22:29:03,937 - root - INFO - Step: 170 | Loss: 6.96 | Tokens per second: 39184.05 | Training tokens per second (%): 4.73 | MFU (%): 36.69 | TFLOPs: 362.90 | Global batch size: 64 | Global tokens/sec: 313472.38 | Global MFU (%): 36.69 | Global TFLOPs: 2903.20 | 
2025-05-18 22:29:08,128 - root - INFO - Step: 180 | Loss: 6.85 | Tokens per second: 39096.92 | Training tokens per second (%): 4.50 | MFU (%): 36.61 | TFLOPs: 362.09 | Global batch size: 64 | Global tokens/sec: 312775.35 | Global MFU (%): 36.61 | Global TFLOPs: 2896.75 | 
2025-05-18 22:29:12,412 - root - INFO - Step: 190 | Loss: 6.80 | Tokens per second: 38247.18 | Training tokens per second (%): 4.11 | MFU (%): 35.82 | TFLOPs: 354.22 | Global batch size: 64 | Global tokens/sec: 305977.45 | Global MFU (%): 35.82 | Global TFLOPs: 2833.79 | 
2025-05-18 22:29:16,631 - root - INFO - Step: 200 | Loss: 6.79 | Tokens per second: 38846.76 | Training tokens per second (%): 4.74 | MFU (%): 36.38 | TFLOPs: 359.78 | Global batch size: 64 | Global tokens/sec: 310774.08 | Global MFU (%): 36.38 | Global TFLOPs: 2878.21 | 
2025-05-18 22:29:20,830 - root - INFO - Step: 210 | Loss: 6.75 | Tokens per second: 39025.18 | Training tokens per second (%): 4.14 | MFU (%): 36.54 | TFLOPs: 361.43 | Global batch size: 64 | Global tokens/sec: 312201.45 | Global MFU (%): 36.54 | Global TFLOPs: 2891.43 | 
2025-05-18 22:29:25,040 - root - INFO - Step: 220 | Loss: 6.85 | Tokens per second: 38919.68 | Training tokens per second (%): 4.58 | MFU (%): 36.45 | TFLOPs: 360.45 | Global batch size: 64 | Global tokens/sec: 311357.41 | Global MFU (%): 36.45 | Global TFLOPs: 2883.61 | 
2025-05-18 22:29:29,230 - root - INFO - Step: 230 | Loss: 6.77 | Tokens per second: 39105.87 | Training tokens per second (%): 5.11 | MFU (%): 36.62 | TFLOPs: 362.18 | Global batch size: 64 | Global tokens/sec: 312846.94 | Global MFU (%): 36.62 | Global TFLOPs: 2897.41 | 
2025-05-18 22:29:33,397 - root - INFO - Step: 240 | Loss: 6.62 | Tokens per second: 39323.52 | Training tokens per second (%): 5.21 | MFU (%): 36.82 | TFLOPs: 364.19 | Global batch size: 64 | Global tokens/sec: 314588.17 | Global MFU (%): 36.82 | Global TFLOPs: 2913.54 | 
2025-05-18 22:29:37,685 - root - INFO - Step: 250 | Loss: 6.57 | Tokens per second: 38213.90 | Training tokens per second (%): 4.81 | MFU (%): 35.79 | TFLOPs: 353.92 | Global batch size: 64 | Global tokens/sec: 305711.17 | Global MFU (%): 35.79 | Global TFLOPs: 2831.32 | 
2025-05-18 22:29:41,872 - root - INFO - Step: 260 | Loss: 6.57 | Tokens per second: 39141.83 | Training tokens per second (%): 4.62 | MFU (%): 36.65 | TFLOPs: 362.51 | Global batch size: 64 | Global tokens/sec: 313134.67 | Global MFU (%): 36.65 | Global TFLOPs: 2900.07 | 
2025-05-18 22:29:46,123 - root - INFO - Step: 270 | Loss: 6.59 | Tokens per second: 38542.98 | Training tokens per second (%): 5.07 | MFU (%): 36.09 | TFLOPs: 356.96 | Global batch size: 64 | Global tokens/sec: 308343.82 | Global MFU (%): 36.09 | Global TFLOPs: 2855.70 | 
2025-05-18 22:29:50,328 - root - INFO - Step: 280 | Loss: 6.56 | Tokens per second: 38970.28 | Training tokens per second (%): 4.40 | MFU (%): 36.49 | TFLOPs: 360.92 | Global batch size: 64 | Global tokens/sec: 311762.22 | Global MFU (%): 36.49 | Global TFLOPs: 2887.36 | 
2025-05-18 22:29:54,798 - root - INFO - Step: 290 | Loss: 6.37 | Tokens per second: 36662.92 | Training tokens per second (%): 4.98 | MFU (%): 34.33 | TFLOPs: 339.55 | Global batch size: 64 | Global tokens/sec: 293303.39 | Global MFU (%): 34.33 | Global TFLOPs: 2716.41 | 
2025-05-18 22:29:59,166 - root - INFO - Step: 300 | Loss: 6.47 | Tokens per second: 37512.62 | Training tokens per second (%): 4.91 | MFU (%): 35.13 | TFLOPs: 347.42 | Global batch size: 64 | Global tokens/sec: 300100.98 | Global MFU (%): 35.13 | Global TFLOPs: 2779.36 | 
2025-05-18 22:30:03,357 - root - INFO - Step: 310 | Loss: 6.42 | Tokens per second: 39097.82 | Training tokens per second (%): 4.68 | MFU (%): 36.61 | TFLOPs: 362.10 | Global batch size: 64 | Global tokens/sec: 312782.54 | Global MFU (%): 36.61 | Global TFLOPs: 2896.81 | 
2025-05-18 22:30:07,567 - root - INFO - Step: 320 | Loss: 6.38 | Tokens per second: 38926.04 | Training tokens per second (%): 5.02 | MFU (%): 36.45 | TFLOPs: 360.51 | Global batch size: 64 | Global tokens/sec: 311408.31 | Global MFU (%): 36.45 | Global TFLOPs: 2884.09 | 
2025-05-18 22:30:11,721 - root - INFO - Step: 330 | Loss: 6.39 | Tokens per second: 39441.84 | Training tokens per second (%): 4.87 | MFU (%): 36.94 | TFLOPs: 365.29 | Global batch size: 64 | Global tokens/sec: 315534.75 | Global MFU (%): 36.94 | Global TFLOPs: 2922.30 | 
2025-05-18 22:30:15,934 - root - INFO - Step: 340 | Loss: 6.28 | Tokens per second: 38902.20 | Training tokens per second (%): 4.53 | MFU (%): 36.43 | TFLOPs: 360.29 | Global batch size: 64 | Global tokens/sec: 311217.61 | Global MFU (%): 36.43 | Global TFLOPs: 2882.32 | 
2025-05-18 22:30:20,158 - root - INFO - Step: 350 | Loss: 6.30 | Tokens per second: 38787.54 | Training tokens per second (%): 5.10 | MFU (%): 36.32 | TFLOPs: 359.23 | Global batch size: 64 | Global tokens/sec: 310300.35 | Global MFU (%): 36.32 | Global TFLOPs: 2873.82 | 
2025-05-18 22:30:24,371 - root - INFO - Step: 360 | Loss: 6.34 | Tokens per second: 38900.33 | Training tokens per second (%): 4.41 | MFU (%): 36.43 | TFLOPs: 360.27 | Global batch size: 64 | Global tokens/sec: 311202.67 | Global MFU (%): 36.43 | Global TFLOPs: 2882.18 | 
2025-05-18 22:30:28,571 - root - INFO - Step: 370 | Loss: 6.25 | Tokens per second: 39013.61 | Training tokens per second (%): 4.47 | MFU (%): 36.53 | TFLOPs: 361.32 | Global batch size: 64 | Global tokens/sec: 312108.84 | Global MFU (%): 36.53 | Global TFLOPs: 2890.57 | 
2025-05-18 22:30:32,784 - root - INFO - Step: 380 | Loss: 6.15 | Tokens per second: 38895.57 | Training tokens per second (%): 4.54 | MFU (%): 36.42 | TFLOPs: 360.23 | Global batch size: 64 | Global tokens/sec: 311164.57 | Global MFU (%): 36.42 | Global TFLOPs: 2881.83 | 
2025-05-18 22:30:36,970 - root - INFO - Step: 390 | Loss: 6.17 | Tokens per second: 39150.85 | Training tokens per second (%): 4.85 | MFU (%): 36.66 | TFLOPs: 362.59 | Global batch size: 64 | Global tokens/sec: 313206.81 | Global MFU (%): 36.66 | Global TFLOPs: 2900.74 | 
2025-05-18 22:30:41,155 - root - INFO - Step: 400 | Loss: 6.10 | Tokens per second: 39149.10 | Training tokens per second (%): 5.21 | MFU (%): 36.66 | TFLOPs: 362.58 | Global batch size: 64 | Global tokens/sec: 313192.83 | Global MFU (%): 36.66 | Global TFLOPs: 2900.61 | 
2025-05-18 22:30:45,584 - root - INFO - Step: 410 | Loss: 6.00 | Tokens per second: 37001.48 | Training tokens per second (%): 4.86 | MFU (%): 34.65 | TFLOPs: 342.69 | Global batch size: 64 | Global tokens/sec: 296011.84 | Global MFU (%): 34.65 | Global TFLOPs: 2741.49 | 
2025-05-18 22:30:49,939 - root - INFO - Step: 420 | Loss: 6.10 | Tokens per second: 37624.55 | Training tokens per second (%): 5.45 | MFU (%): 35.23 | TFLOPs: 348.46 | Global batch size: 64 | Global tokens/sec: 300996.40 | Global MFU (%): 35.23 | Global TFLOPs: 2787.66 | 
2025-05-18 22:30:54,109 - root - INFO - Step: 430 | Loss: 6.10 | Tokens per second: 39296.57 | Training tokens per second (%): 4.95 | MFU (%): 36.80 | TFLOPs: 363.94 | Global batch size: 64 | Global tokens/sec: 314372.56 | Global MFU (%): 36.80 | Global TFLOPs: 2911.54 | 
2025-05-18 22:30:58,256 - root - INFO - Step: 440 | Loss: 6.12 | Tokens per second: 39512.05 | Training tokens per second (%): 5.33 | MFU (%): 37.00 | TFLOPs: 365.94 | Global batch size: 64 | Global tokens/sec: 316096.38 | Global MFU (%): 37.00 | Global TFLOPs: 2927.50 | 
2025-05-18 22:31:02,560 - root - INFO - Step: 450 | Loss: 6.08 | Tokens per second: 38080.40 | Training tokens per second (%): 4.34 | MFU (%): 35.66 | TFLOPs: 352.68 | Global batch size: 64 | Global tokens/sec: 304643.18 | Global MFU (%): 35.66 | Global TFLOPs: 2821.43 | 
2025-05-18 22:31:06,750 - root - INFO - Step: 460 | Loss: 6.12 | Tokens per second: 39108.84 | Training tokens per second (%): 4.68 | MFU (%): 36.62 | TFLOPs: 362.20 | Global batch size: 64 | Global tokens/sec: 312870.73 | Global MFU (%): 36.62 | Global TFLOPs: 2897.63 | 
2025-05-18 22:31:11,015 - root - INFO - Step: 470 | Loss: 6.07 | Tokens per second: 38418.25 | Training tokens per second (%): 4.94 | MFU (%): 35.98 | TFLOPs: 355.81 | Global batch size: 64 | Global tokens/sec: 307346.01 | Global MFU (%): 35.98 | Global TFLOPs: 2846.46 | 
2025-05-18 22:31:15,232 - root - INFO - Step: 480 | Loss: 6.07 | Tokens per second: 38861.79 | Training tokens per second (%): 4.81 | MFU (%): 36.39 | TFLOPs: 359.92 | Global batch size: 64 | Global tokens/sec: 310894.34 | Global MFU (%): 36.39 | Global TFLOPs: 2879.33 | 
2025-05-18 22:31:19,491 - root - INFO - Step: 490 | Loss: 6.07 | Tokens per second: 38472.10 | Training tokens per second (%): 5.47 | MFU (%): 36.03 | TFLOPs: 356.31 | Global batch size: 64 | Global tokens/sec: 307776.77 | Global MFU (%): 36.03 | Global TFLOPs: 2850.45 | 
2025-05-18 22:31:23,707 - root - INFO - Step: 500 | Loss: 6.03 | Tokens per second: 38867.43 | Training tokens per second (%): 5.16 | MFU (%): 36.40 | TFLOPs: 359.97 | Global batch size: 64 | Global tokens/sec: 310939.48 | Global MFU (%): 36.40 | Global TFLOPs: 2879.74 | 
2025-05-18 22:31:27,891 - root - INFO - Step: 510 | Loss: 5.97 | Tokens per second: 39164.67 | Training tokens per second (%): 5.13 | MFU (%): 36.68 | TFLOPs: 362.72 | Global batch size: 64 | Global tokens/sec: 313317.35 | Global MFU (%): 36.68 | Global TFLOPs: 2901.77 | 
2025-05-18 22:31:32,080 - root - INFO - Step: 520 | Loss: 5.94 | Tokens per second: 39116.99 | Training tokens per second (%): 4.63 | MFU (%): 36.63 | TFLOPs: 362.28 | Global batch size: 64 | Global tokens/sec: 312935.92 | Global MFU (%): 36.63 | Global TFLOPs: 2898.23 | 
2025-05-18 22:31:36,294 - root - INFO - Step: 530 | Loss: 5.92 | Tokens per second: 38887.76 | Training tokens per second (%): 5.57 | MFU (%): 36.42 | TFLOPs: 360.16 | Global batch size: 64 | Global tokens/sec: 311102.06 | Global MFU (%): 36.42 | Global TFLOPs: 2881.25 | 
2025-05-18 22:31:40,467 - root - INFO - Step: 540 | Loss: 6.01 | Tokens per second: 39264.84 | Training tokens per second (%): 4.63 | MFU (%): 36.77 | TFLOPs: 363.65 | Global batch size: 64 | Global tokens/sec: 314118.73 | Global MFU (%): 36.77 | Global TFLOPs: 2909.19 | 
2025-05-18 22:31:44,700 - root - INFO - Step: 550 | Loss: 5.89 | Tokens per second: 38710.65 | Training tokens per second (%): 5.21 | MFU (%): 36.25 | TFLOPs: 358.52 | Global batch size: 64 | Global tokens/sec: 309685.17 | Global MFU (%): 36.25 | Global TFLOPs: 2868.13 | 
2025-05-18 22:31:48,918 - root - INFO - Step: 560 | Loss: 5.82 | Tokens per second: 38848.12 | Training tokens per second (%): 4.39 | MFU (%): 36.38 | TFLOPs: 359.79 | Global batch size: 64 | Global tokens/sec: 310784.94 | Global MFU (%): 36.38 | Global TFLOPs: 2878.31 | 
2025-05-18 22:31:53,145 - root - INFO - Step: 570 | Loss: 5.87 | Tokens per second: 38764.91 | Training tokens per second (%): 4.25 | MFU (%): 36.30 | TFLOPs: 359.02 | Global batch size: 64 | Global tokens/sec: 310119.24 | Global MFU (%): 36.30 | Global TFLOPs: 2872.15 | 
2025-05-18 22:31:57,434 - root - INFO - Step: 580 | Loss: 5.85 | Tokens per second: 38208.22 | Training tokens per second (%): 4.15 | MFU (%): 35.78 | TFLOPs: 353.86 | Global batch size: 64 | Global tokens/sec: 305665.79 | Global MFU (%): 35.78 | Global TFLOPs: 2830.90 | 
2025-05-18 22:32:01,703 - root - INFO - Step: 590 | Loss: 5.95 | Tokens per second: 38382.26 | Training tokens per second (%): 4.61 | MFU (%): 35.94 | TFLOPs: 355.47 | Global batch size: 64 | Global tokens/sec: 307058.06 | Global MFU (%): 35.94 | Global TFLOPs: 2843.80 | 
2025-05-18 22:32:05,946 - root - INFO - Step: 600 | Loss: 5.96 | Tokens per second: 38621.33 | Training tokens per second (%): 4.90 | MFU (%): 36.17 | TFLOPs: 357.69 | Global batch size: 64 | Global tokens/sec: 308970.65 | Global MFU (%): 36.17 | Global TFLOPs: 2861.51 | 
2025-05-18 22:32:10,389 - root - INFO - Step: 610 | Loss: 5.79 | Tokens per second: 36887.40 | Training tokens per second (%): 4.49 | MFU (%): 34.54 | TFLOPs: 341.63 | Global batch size: 64 | Global tokens/sec: 295099.23 | Global MFU (%): 34.54 | Global TFLOPs: 2733.04 | 
2025-05-18 22:32:14,712 - root - INFO - Step: 620 | Loss: 5.87 | Tokens per second: 37898.05 | Training tokens per second (%): 5.09 | MFU (%): 35.49 | TFLOPs: 350.99 | Global batch size: 64 | Global tokens/sec: 303184.43 | Global MFU (%): 35.49 | Global TFLOPs: 2807.92 | 
2025-05-18 22:32:18,937 - root - INFO - Step: 630 | Loss: 5.71 | Tokens per second: 38791.37 | Training tokens per second (%): 4.80 | MFU (%): 36.33 | TFLOPs: 359.26 | Global batch size: 64 | Global tokens/sec: 310330.96 | Global MFU (%): 36.33 | Global TFLOPs: 2874.11 | 
2025-05-18 22:32:23,105 - root - INFO - Step: 640 | Loss: 5.63 | Tokens per second: 39312.08 | Training tokens per second (%): 5.12 | MFU (%): 36.81 | TFLOPs: 364.09 | Global batch size: 64 | Global tokens/sec: 314496.66 | Global MFU (%): 36.81 | Global TFLOPs: 2912.69 | 
2025-05-18 22:32:27,272 - root - INFO - Step: 650 | Loss: 5.74 | Tokens per second: 39330.61 | Training tokens per second (%): 3.82 | MFU (%): 36.83 | TFLOPs: 364.26 | Global batch size: 64 | Global tokens/sec: 314644.92 | Global MFU (%): 36.83 | Global TFLOPs: 2914.06 | 
2025-05-18 22:32:31,509 - root - INFO - Step: 660 | Loss: 5.75 | Tokens per second: 38672.71 | Training tokens per second (%): 4.36 | MFU (%): 36.21 | TFLOPs: 358.16 | Global batch size: 64 | Global tokens/sec: 309381.71 | Global MFU (%): 36.21 | Global TFLOPs: 2865.32 | 
2025-05-18 22:32:35,766 - root - INFO - Step: 670 | Loss: 5.64 | Tokens per second: 38487.95 | Training tokens per second (%): 4.46 | MFU (%): 36.04 | TFLOPs: 356.45 | Global batch size: 64 | Global tokens/sec: 307903.58 | Global MFU (%): 36.04 | Global TFLOPs: 2851.63 | 
2025-05-18 22:32:39,981 - root - INFO - Step: 680 | Loss: 5.73 | Tokens per second: 38877.02 | Training tokens per second (%): 4.69 | MFU (%): 36.41 | TFLOPs: 360.06 | Global batch size: 64 | Global tokens/sec: 311016.14 | Global MFU (%): 36.41 | Global TFLOPs: 2880.45 | 
2025-05-18 22:32:44,198 - root - INFO - Step: 690 | Loss: 5.67 | Tokens per second: 38858.24 | Training tokens per second (%): 4.71 | MFU (%): 36.39 | TFLOPs: 359.88 | Global batch size: 64 | Global tokens/sec: 310865.88 | Global MFU (%): 36.39 | Global TFLOPs: 2879.06 | 
2025-05-18 22:32:48,359 - root - INFO - Step: 700 | Loss: 5.60 | Tokens per second: 39379.71 | Training tokens per second (%): 5.18 | MFU (%): 36.88 | TFLOPs: 364.71 | Global batch size: 64 | Global tokens/sec: 315037.70 | Global MFU (%): 36.88 | Global TFLOPs: 2917.70 | 
2025-05-18 22:32:52,631 - root - INFO - Step: 710 | Loss: 5.82 | Tokens per second: 38363.67 | Training tokens per second (%): 4.95 | MFU (%): 35.93 | TFLOPs: 355.30 | Global batch size: 64 | Global tokens/sec: 306909.35 | Global MFU (%): 35.93 | Global TFLOPs: 2842.42 | 
2025-05-18 22:32:56,886 - root - INFO - Step: 720 | Loss: 5.69 | Tokens per second: 38511.52 | Training tokens per second (%): 5.15 | MFU (%): 36.06 | TFLOPs: 356.67 | Global batch size: 64 | Global tokens/sec: 308092.14 | Global MFU (%): 36.06 | Global TFLOPs: 2853.37 | 
2025-05-18 22:33:01,223 - root - INFO - Step: 730 | Loss: 5.72 | Tokens per second: 37777.83 | Training tokens per second (%): 5.19 | MFU (%): 35.38 | TFLOPs: 349.88 | Global batch size: 64 | Global tokens/sec: 302222.67 | Global MFU (%): 35.38 | Global TFLOPs: 2799.01 | 
2025-05-18 22:33:05,521 - root - INFO - Step: 740 | Loss: 5.61 | Tokens per second: 38126.88 | Training tokens per second (%): 5.26 | MFU (%): 35.70 | TFLOPs: 353.11 | Global batch size: 64 | Global tokens/sec: 305015.00 | Global MFU (%): 35.70 | Global TFLOPs: 2824.87 | 
2025-05-18 22:33:09,966 - root - INFO - Step: 750 | Loss: 5.72 | Tokens per second: 36865.64 | Training tokens per second (%): 4.92 | MFU (%): 34.52 | TFLOPs: 341.43 | Global batch size: 64 | Global tokens/sec: 294925.15 | Global MFU (%): 34.52 | Global TFLOPs: 2731.43 | 
2025-05-18 22:33:14,167 - root - INFO - Step: 760 | Loss: 5.54 | Tokens per second: 39009.66 | Training tokens per second (%): 4.78 | MFU (%): 36.53 | TFLOPs: 361.29 | Global batch size: 64 | Global tokens/sec: 312077.30 | Global MFU (%): 36.53 | Global TFLOPs: 2890.28 | 
2025-05-18 22:33:18,373 - root - INFO - Step: 770 | Loss: 5.53 | Tokens per second: 38954.83 | Training tokens per second (%): 4.06 | MFU (%): 36.48 | TFLOPs: 360.78 | Global batch size: 64 | Global tokens/sec: 311638.67 | Global MFU (%): 36.48 | Global TFLOPs: 2886.22 | 
2025-05-18 22:33:22,584 - root - INFO - Step: 780 | Loss: 5.61 | Tokens per second: 38919.25 | Training tokens per second (%): 4.70 | MFU (%): 36.45 | TFLOPs: 360.45 | Global batch size: 64 | Global tokens/sec: 311354.02 | Global MFU (%): 36.45 | Global TFLOPs: 2883.58 | 
2025-05-18 22:33:26,773 - root - INFO - Step: 790 | Loss: 5.59 | Tokens per second: 39117.38 | Training tokens per second (%): 4.80 | MFU (%): 36.63 | TFLOPs: 362.28 | Global batch size: 64 | Global tokens/sec: 312939.06 | Global MFU (%): 36.63 | Global TFLOPs: 2898.26 | 
2025-05-18 22:33:30,974 - root - INFO - Step: 800 | Loss: 5.58 | Tokens per second: 39004.62 | Training tokens per second (%): 4.71 | MFU (%): 36.53 | TFLOPs: 361.24 | Global batch size: 64 | Global tokens/sec: 312036.95 | Global MFU (%): 36.53 | Global TFLOPs: 2889.91 | 
2025-05-18 22:33:35,425 - root - INFO - Step: 810 | Loss: 5.52 | Tokens per second: 36813.55 | Training tokens per second (%): 5.27 | MFU (%): 34.47 | TFLOPs: 340.95 | Global batch size: 64 | Global tokens/sec: 294508.42 | Global MFU (%): 34.47 | Global TFLOPs: 2727.57 | 
2025-05-18 22:33:39,658 - root - INFO - Step: 820 | Loss: 5.47 | Tokens per second: 38718.58 | Training tokens per second (%): 4.71 | MFU (%): 36.26 | TFLOPs: 358.59 | Global batch size: 64 | Global tokens/sec: 309748.61 | Global MFU (%): 36.26 | Global TFLOPs: 2868.71 | 
2025-05-18 22:33:43,987 - root - INFO - Step: 830 | Loss: 5.49 | Tokens per second: 37845.13 | Training tokens per second (%): 5.38 | MFU (%): 35.44 | TFLOPs: 350.50 | Global batch size: 64 | Global tokens/sec: 302761.05 | Global MFU (%): 35.44 | Global TFLOPs: 2804.00 | 
2025-05-18 22:33:48,189 - root - INFO - Step: 840 | Loss: 5.49 | Tokens per second: 39000.58 | Training tokens per second (%): 4.81 | MFU (%): 36.52 | TFLOPs: 361.20 | Global batch size: 64 | Global tokens/sec: 312004.63 | Global MFU (%): 36.52 | Global TFLOPs: 2889.61 | 
2025-05-18 22:33:52,392 - root - INFO - Step: 850 | Loss: 5.42 | Tokens per second: 38986.25 | Training tokens per second (%): 5.90 | MFU (%): 36.51 | TFLOPs: 361.07 | Global batch size: 64 | Global tokens/sec: 311890.01 | Global MFU (%): 36.51 | Global TFLOPs: 2888.55 | 
2025-05-18 22:33:56,562 - root - INFO - Step: 860 | Loss: 5.55 | Tokens per second: 39295.21 | Training tokens per second (%): 3.98 | MFU (%): 36.80 | TFLOPs: 363.93 | Global batch size: 64 | Global tokens/sec: 314361.66 | Global MFU (%): 36.80 | Global TFLOPs: 2911.44 | 
2025-05-18 22:34:00,803 - root - INFO - Step: 870 | Loss: 5.49 | Tokens per second: 38644.27 | Training tokens per second (%): 4.48 | MFU (%): 36.19 | TFLOPs: 357.90 | Global batch size: 64 | Global tokens/sec: 309154.17 | Global MFU (%): 36.19 | Global TFLOPs: 2863.21 | 
2025-05-18 22:34:05,127 - root - INFO - Step: 880 | Loss: 5.47 | Tokens per second: 37893.76 | Training tokens per second (%): 4.31 | MFU (%): 35.49 | TFLOPs: 350.95 | Global batch size: 64 | Global tokens/sec: 303150.07 | Global MFU (%): 35.49 | Global TFLOPs: 2807.60 | 
2025-05-18 22:34:09,378 - root - INFO - Step: 890 | Loss: 5.39 | Tokens per second: 38543.64 | Training tokens per second (%): 5.25 | MFU (%): 36.09 | TFLOPs: 356.97 | Global batch size: 64 | Global tokens/sec: 308349.15 | Global MFU (%): 36.09 | Global TFLOPs: 2855.75 | 
2025-05-18 22:34:13,795 - root - INFO - Step: 900 | Loss: 5.45 | Tokens per second: 37103.41 | Training tokens per second (%): 4.48 | MFU (%): 34.75 | TFLOPs: 343.63 | Global batch size: 64 | Global tokens/sec: 296827.28 | Global MFU (%): 34.75 | Global TFLOPs: 2749.04 | 
2025-05-18 22:34:18,009 - root - INFO - Step: 910 | Loss: 5.38 | Tokens per second: 38888.09 | Training tokens per second (%): 5.14 | MFU (%): 36.42 | TFLOPs: 360.16 | Global batch size: 64 | Global tokens/sec: 311104.72 | Global MFU (%): 36.42 | Global TFLOPs: 2881.27 | 
2025-05-18 22:34:22,260 - root - INFO - Step: 920 | Loss: 5.44 | Tokens per second: 38539.79 | Training tokens per second (%): 4.50 | MFU (%): 36.09 | TFLOPs: 356.93 | Global batch size: 64 | Global tokens/sec: 308318.31 | Global MFU (%): 36.09 | Global TFLOPs: 2855.47 | 
2025-05-18 22:34:26,539 - root - INFO - Step: 930 | Loss: 5.45 | Tokens per second: 38302.92 | Training tokens per second (%): 4.55 | MFU (%): 35.87 | TFLOPs: 354.74 | Global batch size: 64 | Global tokens/sec: 306423.40 | Global MFU (%): 35.87 | Global TFLOPs: 2837.92 | 
2025-05-18 22:34:30,726 - root - INFO - Step: 940 | Loss: 5.35 | Tokens per second: 39132.97 | Training tokens per second (%): 4.79 | MFU (%): 36.65 | TFLOPs: 362.43 | Global batch size: 64 | Global tokens/sec: 313063.79 | Global MFU (%): 36.65 | Global TFLOPs: 2899.42 | 
2025-05-18 22:34:34,924 - root - INFO - Step: 950 | Loss: 5.35 | Tokens per second: 39031.63 | Training tokens per second (%): 5.07 | MFU (%): 36.55 | TFLOPs: 361.49 | Global batch size: 64 | Global tokens/sec: 312253.05 | Global MFU (%): 36.55 | Global TFLOPs: 2891.91 | 
2025-05-18 22:34:39,142 - root - INFO - Step: 960 | Loss: 5.37 | Tokens per second: 38850.99 | Training tokens per second (%): 5.28 | MFU (%): 36.38 | TFLOPs: 359.82 | Global batch size: 64 | Global tokens/sec: 310807.93 | Global MFU (%): 36.38 | Global TFLOPs: 2878.53 | 
2025-05-18 22:34:43,346 - root - INFO - Step: 970 | Loss: 5.38 | Tokens per second: 38974.38 | Training tokens per second (%): 4.45 | MFU (%): 36.50 | TFLOPs: 360.96 | Global batch size: 64 | Global tokens/sec: 311795.07 | Global MFU (%): 36.50 | Global TFLOPs: 2887.67 | 
2025-05-18 22:34:47,535 - root - INFO - Step: 980 | Loss: 5.38 | Tokens per second: 39126.33 | Training tokens per second (%): 4.34 | MFU (%): 36.64 | TFLOPs: 362.37 | Global batch size: 64 | Global tokens/sec: 313010.61 | Global MFU (%): 36.64 | Global TFLOPs: 2898.93 | 
2025-05-18 22:34:51,706 - root - INFO - Step: 990 | Loss: 5.33 | Tokens per second: 39287.19 | Training tokens per second (%): 5.27 | MFU (%): 36.79 | TFLOPs: 363.86 | Global batch size: 64 | Global tokens/sec: 314297.49 | Global MFU (%): 36.79 | Global TFLOPs: 2910.84 | 
2025-05-18 22:34:56,011 - root - INFO - Step: 1000 | Loss: 5.41 | Tokens per second: 38057.11 | Training tokens per second (%): 4.76 | MFU (%): 35.64 | TFLOPs: 352.46 | Global batch size: 64 | Global tokens/sec: 304456.87 | Global MFU (%): 35.64 | Global TFLOPs: 2819.71 | 
2025-05-18 22:34:56,011 - root - INFO - Training completed
[sbatch-master] task finished
