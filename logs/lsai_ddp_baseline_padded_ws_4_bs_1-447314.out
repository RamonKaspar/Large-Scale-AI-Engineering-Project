[sbatch-master] running on nid006646
[sbatch-master] SLURM_NODELIST: nid006646
[sbatch-master] SLURM_NNODES: 1
[sbatch-master] SLURM_NODEID: 0
[Master] Master node: nid006646
[Master] World size: 4
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
[srun] rank=0 host=nid006646 noderank=0 localrank=0
W0518 20:37:27.324000 150900 torch/distributed/run.py:792] 
W0518 20:37:27.324000 150900 torch/distributed/run.py:792] *****************************************
W0518 20:37:27.324000 150900 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0518 20:37:27.324000 150900 torch/distributed/run.py:792] *****************************************
2025-05-18 20:37:40,971 - root - INFO - [Distributed Init] Rank 0 initialized on node 0 on GPU 0.
[rank0]:[W518 20:37:41.556289641 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 20:37:41,579 - root - INFO - [Distributed Init] Rank 3 initialized on node 0 on GPU 3.
[rank3]:[W518 20:37:41.712639479 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 20:37:41,638 - root - INFO - [Distributed Init] Rank 1 initialized on node 0 on GPU 1.
[rank1]:[W518 20:37:41.771936975 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 20:37:41,648 - root - INFO - [Distributed Init] Rank 2 initialized on node 0 on GPU 2.
[rank2]:[W518 20:37:41.784245528 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 20:37:48,460 - root - INFO - [Rank 0] All ranks ready!
2025-05-18 20:37:48,460 - root - INFO - Distributed training enabled: 4 processes
2025-05-18 20:37:48,460 - root - INFO - Master process: 0 on cuda:0
2025-05-18 20:37:48,460 - root - INFO - Experiment args: Namespace(dataset='/capstor/scratch/cscs/kasparr/project/train_data.parquet', dataset_type='padded', pretokenized=False, tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=1, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=10, profile=False, profile_step_start=10, profile_step_end=10, grad_max_norm=1, model_dtype='bf16', compile=False, distributed=True, backend='nccl', find_unused_parameters=False)
2025-05-18 20:37:48,460 - root - INFO - Setting up Tokenizer...
2025-05-18 20:37:53,955 - root - INFO - Setting up DataLoaders...
2025-05-18 20:37:53,955 - root - INFO - Using padded ParquetDataset with on-the-fly tokenization
2025-05-18 20:37:58,690 - root - INFO - Setting up Model...
2025-05-18 20:38:32,478 - root - INFO - Model wrapped with DistributedDataParallel
2025-05-18 20:38:32,480 - root - INFO - Global batch size: 4 (local: 1 Ã— 4 processes)
2025-05-18 20:38:32,480 - root - INFO - Starting training!
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
2025-05-18 20:38:34,093 - root - INFO - Step: 1 | Loss: 11.96 | Tokens per second: 1270.74 | Training tokens per second (%): 12.26 | MFU (%): 6.62 | TFLOPs: 65.50 | Global batch size: 4 | Global tokens/sec: 5082.96 | Global MFU (%): 6.62 | Global TFLOPs: 261.98 | 
2025-05-18 20:38:37,587 - root - INFO - Step: 10 | Loss: 11.78 | Tokens per second: 5276.16 | Training tokens per second (%): 12.93 | MFU (%): 27.50 | TFLOPs: 271.94 | Global batch size: 4 | Global tokens/sec: 21104.64 | Global MFU (%): 27.50 | Global TFLOPs: 1087.76 | 
2025-05-18 20:38:41,411 - root - INFO - Step: 20 | Loss: 10.90 | Tokens per second: 5356.84 | Training tokens per second (%): 5.56 | MFU (%): 27.92 | TFLOPs: 276.10 | Global batch size: 4 | Global tokens/sec: 21427.37 | Global MFU (%): 27.92 | Global TFLOPs: 1104.39 | 
2025-05-18 20:38:45,298 - root - INFO - Step: 30 | Loss: 9.67 | Tokens per second: 5271.17 | Training tokens per second (%): 12.04 | MFU (%): 27.47 | TFLOPs: 271.68 | Global batch size: 4 | Global tokens/sec: 21084.69 | Global MFU (%): 27.47 | Global TFLOPs: 1086.73 | 
2025-05-18 20:38:49,140 - root - INFO - Step: 40 | Loss: 9.25 | Tokens per second: 5332.07 | Training tokens per second (%): 11.64 | MFU (%): 27.79 | TFLOPs: 274.82 | Global batch size: 4 | Global tokens/sec: 21328.30 | Global MFU (%): 27.79 | Global TFLOPs: 1099.29 | 
2025-05-18 20:38:52,993 - root - INFO - Step: 50 | Loss: 8.52 | Tokens per second: 5316.14 | Training tokens per second (%): 10.09 | MFU (%): 27.70 | TFLOPs: 274.00 | Global batch size: 4 | Global tokens/sec: 21264.57 | Global MFU (%): 27.70 | Global TFLOPs: 1096.00 | 
2025-05-18 20:38:56,842 - root - INFO - Step: 60 | Loss: 8.41 | Tokens per second: 5321.60 | Training tokens per second (%): 10.65 | MFU (%): 27.73 | TFLOPs: 274.28 | Global batch size: 4 | Global tokens/sec: 21286.39 | Global MFU (%): 27.73 | Global TFLOPs: 1097.13 | 
2025-05-18 20:39:00,676 - root - INFO - Step: 70 | Loss: 8.11 | Tokens per second: 5342.86 | Training tokens per second (%): 8.27 | MFU (%): 27.84 | TFLOPs: 275.38 | Global batch size: 4 | Global tokens/sec: 21371.45 | Global MFU (%): 27.84 | Global TFLOPs: 1101.51 | 
2025-05-18 20:39:04,506 - root - INFO - Step: 80 | Loss: 7.89 | Tokens per second: 5349.42 | Training tokens per second (%): 7.70 | MFU (%): 27.88 | TFLOPs: 275.72 | Global batch size: 4 | Global tokens/sec: 21397.67 | Global MFU (%): 27.88 | Global TFLOPs: 1102.86 | 
2025-05-18 20:39:08,351 - root - INFO - Step: 90 | Loss: 7.28 | Tokens per second: 5326.82 | Training tokens per second (%): 7.23 | MFU (%): 27.76 | TFLOPs: 274.55 | Global batch size: 4 | Global tokens/sec: 21307.27 | Global MFU (%): 27.76 | Global TFLOPs: 1098.20 | 
2025-05-18 20:39:12,210 - root - INFO - Step: 100 | Loss: 7.35 | Tokens per second: 5308.25 | Training tokens per second (%): 11.36 | MFU (%): 27.66 | TFLOPs: 273.59 | Global batch size: 4 | Global tokens/sec: 21232.98 | Global MFU (%): 27.66 | Global TFLOPs: 1094.37 | 
2025-05-18 20:39:16,074 - root - INFO - Step: 110 | Loss: 7.46 | Tokens per second: 5301.34 | Training tokens per second (%): 10.52 | MFU (%): 27.63 | TFLOPs: 273.24 | Global batch size: 4 | Global tokens/sec: 21205.34 | Global MFU (%): 27.63 | Global TFLOPs: 1092.95 | 
2025-05-18 20:39:19,948 - root - INFO - Step: 120 | Loss: 7.82 | Tokens per second: 5289.08 | Training tokens per second (%): 11.77 | MFU (%): 27.56 | TFLOPs: 272.61 | Global batch size: 4 | Global tokens/sec: 21156.33 | Global MFU (%): 27.56 | Global TFLOPs: 1090.42 | 
2025-05-18 20:39:23,808 - root - INFO - Step: 130 | Loss: 7.70 | Tokens per second: 5306.35 | Training tokens per second (%): 5.61 | MFU (%): 27.65 | TFLOPs: 273.50 | Global batch size: 4 | Global tokens/sec: 21225.42 | Global MFU (%): 27.65 | Global TFLOPs: 1093.98 | 
2025-05-18 20:39:27,730 - root - INFO - Step: 140 | Loss: 7.50 | Tokens per second: 5223.39 | Training tokens per second (%): 10.55 | MFU (%): 27.22 | TFLOPs: 269.22 | Global batch size: 4 | Global tokens/sec: 20893.55 | Global MFU (%): 27.22 | Global TFLOPs: 1076.88 | 
2025-05-18 20:39:31,609 - root - INFO - Step: 150 | Loss: 7.12 | Tokens per second: 5279.91 | Training tokens per second (%): 9.19 | MFU (%): 27.52 | TFLOPs: 272.13 | Global batch size: 4 | Global tokens/sec: 21119.65 | Global MFU (%): 27.52 | Global TFLOPs: 1088.53 | 
2025-05-18 20:39:35,452 - root - INFO - Step: 160 | Loss: 6.82 | Tokens per second: 5331.37 | Training tokens per second (%): 10.34 | MFU (%): 27.78 | TFLOPs: 274.79 | Global batch size: 4 | Global tokens/sec: 21325.47 | Global MFU (%): 27.78 | Global TFLOPs: 1099.14 | 
2025-05-18 20:39:39,304 - root - INFO - Step: 170 | Loss: 7.01 | Tokens per second: 5317.85 | Training tokens per second (%): 5.38 | MFU (%): 27.71 | TFLOPs: 274.09 | Global batch size: 4 | Global tokens/sec: 21271.41 | Global MFU (%): 27.71 | Global TFLOPs: 1096.35 | 
2025-05-18 20:39:43,158 - root - INFO - Step: 180 | Loss: 7.11 | Tokens per second: 5315.49 | Training tokens per second (%): 9.33 | MFU (%): 27.70 | TFLOPs: 273.97 | Global batch size: 4 | Global tokens/sec: 21261.94 | Global MFU (%): 27.70 | Global TFLOPs: 1095.87 | 
2025-05-18 20:39:47,037 - root - INFO - Step: 190 | Loss: 7.04 | Tokens per second: 5280.31 | Training tokens per second (%): 6.30 | MFU (%): 27.52 | TFLOPs: 272.15 | Global batch size: 4 | Global tokens/sec: 21121.24 | Global MFU (%): 27.52 | Global TFLOPs: 1088.61 | 
2025-05-18 20:39:50,927 - root - INFO - Step: 200 | Loss: 7.09 | Tokens per second: 5265.79 | Training tokens per second (%): 12.70 | MFU (%): 27.44 | TFLOPs: 271.40 | Global batch size: 4 | Global tokens/sec: 21063.14 | Global MFU (%): 27.44 | Global TFLOPs: 1085.62 | 
2025-05-18 20:39:54,774 - root - INFO - Step: 210 | Loss: 6.94 | Tokens per second: 5325.96 | Training tokens per second (%): 11.42 | MFU (%): 27.76 | TFLOPs: 274.51 | Global batch size: 4 | Global tokens/sec: 21303.84 | Global MFU (%): 27.76 | Global TFLOPs: 1098.03 | 
2025-05-18 20:39:58,645 - root - INFO - Step: 220 | Loss: 6.81 | Tokens per second: 5290.73 | Training tokens per second (%): 9.92 | MFU (%): 27.57 | TFLOPs: 272.69 | Global batch size: 4 | Global tokens/sec: 21162.91 | Global MFU (%): 27.57 | Global TFLOPs: 1090.76 | 
2025-05-18 20:40:02,472 - root - INFO - Step: 230 | Loss: 7.20 | Tokens per second: 5353.60 | Training tokens per second (%): 6.84 | MFU (%): 27.90 | TFLOPs: 275.93 | Global batch size: 4 | Global tokens/sec: 21414.41 | Global MFU (%): 27.90 | Global TFLOPs: 1103.72 | 
2025-05-18 20:40:06,338 - root - INFO - Step: 240 | Loss: 6.64 | Tokens per second: 5298.18 | Training tokens per second (%): 8.74 | MFU (%): 27.61 | TFLOPs: 273.07 | Global batch size: 4 | Global tokens/sec: 21192.71 | Global MFU (%): 27.61 | Global TFLOPs: 1092.30 | 
2025-05-18 20:40:10,190 - root - INFO - Step: 250 | Loss: 6.96 | Tokens per second: 5318.31 | Training tokens per second (%): 8.09 | MFU (%): 27.72 | TFLOPs: 274.11 | Global batch size: 4 | Global tokens/sec: 21273.23 | Global MFU (%): 27.72 | Global TFLOPs: 1096.45 | 
2025-05-18 20:40:14,065 - root - INFO - Step: 260 | Loss: 6.73 | Tokens per second: 5286.72 | Training tokens per second (%): 15.72 | MFU (%): 27.55 | TFLOPs: 272.48 | Global batch size: 4 | Global tokens/sec: 21146.88 | Global MFU (%): 27.55 | Global TFLOPs: 1089.94 | 
2025-05-18 20:40:17,934 - root - INFO - Step: 270 | Loss: 6.46 | Tokens per second: 5293.41 | Training tokens per second (%): 9.73 | MFU (%): 27.59 | TFLOPs: 272.83 | Global batch size: 4 | Global tokens/sec: 21173.63 | Global MFU (%): 27.59 | Global TFLOPs: 1091.31 | 
2025-05-18 20:40:21,796 - root - INFO - Step: 280 | Loss: 5.62 | Tokens per second: 5304.20 | Training tokens per second (%): 6.87 | MFU (%): 27.64 | TFLOPs: 273.38 | Global batch size: 4 | Global tokens/sec: 21216.78 | Global MFU (%): 27.64 | Global TFLOPs: 1093.54 | 
2025-05-18 20:40:25,662 - root - INFO - Step: 290 | Loss: 6.21 | Tokens per second: 5298.60 | Training tokens per second (%): 13.30 | MFU (%): 27.61 | TFLOPs: 273.10 | Global batch size: 4 | Global tokens/sec: 21194.40 | Global MFU (%): 27.61 | Global TFLOPs: 1092.38 | 
2025-05-18 20:40:29,558 - root - INFO - Step: 300 | Loss: 6.27 | Tokens per second: 5258.48 | Training tokens per second (%): 12.93 | MFU (%): 27.40 | TFLOPs: 271.03 | Global batch size: 4 | Global tokens/sec: 21033.91 | Global MFU (%): 27.40 | Global TFLOPs: 1084.11 | 
2025-05-18 20:40:33,419 - root - INFO - Step: 310 | Loss: 6.42 | Tokens per second: 5305.27 | Training tokens per second (%): 10.80 | MFU (%): 27.65 | TFLOPs: 273.44 | Global batch size: 4 | Global tokens/sec: 21221.08 | Global MFU (%): 27.65 | Global TFLOPs: 1093.76 | 
2025-05-18 20:40:37,279 - root - INFO - Step: 320 | Loss: 6.12 | Tokens per second: 5306.91 | Training tokens per second (%): 9.89 | MFU (%): 27.66 | TFLOPs: 273.52 | Global batch size: 4 | Global tokens/sec: 21227.62 | Global MFU (%): 27.66 | Global TFLOPs: 1094.10 | 
2025-05-18 20:40:41,150 - root - INFO - Step: 330 | Loss: 6.11 | Tokens per second: 5291.69 | Training tokens per second (%): 7.54 | MFU (%): 27.58 | TFLOPs: 272.74 | Global batch size: 4 | Global tokens/sec: 21166.76 | Global MFU (%): 27.58 | Global TFLOPs: 1090.96 | 
2025-05-18 20:40:45,029 - root - INFO - Step: 340 | Loss: 6.37 | Tokens per second: 5281.02 | Training tokens per second (%): 8.98 | MFU (%): 27.52 | TFLOPs: 272.19 | Global batch size: 4 | Global tokens/sec: 21124.08 | Global MFU (%): 27.52 | Global TFLOPs: 1088.76 | 
2025-05-18 20:40:48,876 - root - INFO - Step: 350 | Loss: 6.13 | Tokens per second: 5325.45 | Training tokens per second (%): 9.01 | MFU (%): 27.75 | TFLOPs: 274.48 | Global batch size: 4 | Global tokens/sec: 21301.80 | Global MFU (%): 27.75 | Global TFLOPs: 1097.92 | 
2025-05-18 20:40:52,716 - root - INFO - Step: 360 | Loss: 6.07 | Tokens per second: 5334.35 | Training tokens per second (%): 9.66 | MFU (%): 27.80 | TFLOPs: 274.94 | Global batch size: 4 | Global tokens/sec: 21337.38 | Global MFU (%): 27.80 | Global TFLOPs: 1099.75 | 
2025-05-18 20:40:56,575 - root - INFO - Step: 370 | Loss: 6.12 | Tokens per second: 5308.73 | Training tokens per second (%): 8.56 | MFU (%): 27.67 | TFLOPs: 273.62 | Global batch size: 4 | Global tokens/sec: 21234.92 | Global MFU (%): 27.67 | Global TFLOPs: 1094.47 | 
2025-05-18 20:41:00,415 - root - INFO - Step: 380 | Loss: 6.69 | Tokens per second: 5334.22 | Training tokens per second (%): 9.63 | MFU (%): 27.80 | TFLOPs: 274.93 | Global batch size: 4 | Global tokens/sec: 21336.87 | Global MFU (%): 27.80 | Global TFLOPs: 1099.73 | 
2025-05-18 20:41:04,254 - root - INFO - Step: 390 | Loss: 6.37 | Tokens per second: 5336.05 | Training tokens per second (%): 7.51 | MFU (%): 27.81 | TFLOPs: 275.03 | Global batch size: 4 | Global tokens/sec: 21344.20 | Global MFU (%): 27.81 | Global TFLOPs: 1100.11 | 
2025-05-18 20:41:08,138 - root - INFO - Step: 400 | Loss: 5.91 | Tokens per second: 5274.58 | Training tokens per second (%): 8.27 | MFU (%): 27.49 | TFLOPs: 271.86 | Global batch size: 4 | Global tokens/sec: 21098.31 | Global MFU (%): 27.49 | Global TFLOPs: 1087.43 | 
2025-05-18 20:41:12,017 - root - INFO - Step: 410 | Loss: 6.25 | Tokens per second: 5280.98 | Training tokens per second (%): 10.90 | MFU (%): 27.52 | TFLOPs: 272.19 | Global batch size: 4 | Global tokens/sec: 21123.91 | Global MFU (%): 27.52 | Global TFLOPs: 1088.75 | 
2025-05-18 20:41:15,876 - root - INFO - Step: 420 | Loss: 6.54 | Tokens per second: 5308.57 | Training tokens per second (%): 8.82 | MFU (%): 27.67 | TFLOPs: 273.61 | Global batch size: 4 | Global tokens/sec: 21234.27 | Global MFU (%): 27.67 | Global TFLOPs: 1094.44 | 
2025-05-18 20:41:19,753 - root - INFO - Step: 430 | Loss: 6.11 | Tokens per second: 5283.04 | Training tokens per second (%): 6.97 | MFU (%): 27.53 | TFLOPs: 272.29 | Global batch size: 4 | Global tokens/sec: 21132.15 | Global MFU (%): 27.53 | Global TFLOPs: 1089.18 | 
2025-05-18 20:41:23,632 - root - INFO - Step: 440 | Loss: 6.07 | Tokens per second: 5281.61 | Training tokens per second (%): 8.43 | MFU (%): 27.52 | TFLOPs: 272.22 | Global batch size: 4 | Global tokens/sec: 21126.44 | Global MFU (%): 27.52 | Global TFLOPs: 1088.88 | 
2025-05-18 20:41:27,465 - root - INFO - Step: 450 | Loss: 6.08 | Tokens per second: 5344.50 | Training tokens per second (%): 9.95 | MFU (%): 27.85 | TFLOPs: 275.46 | Global batch size: 4 | Global tokens/sec: 21378.00 | Global MFU (%): 27.85 | Global TFLOPs: 1101.85 | 
2025-05-18 20:41:31,340 - root - INFO - Step: 460 | Loss: 6.11 | Tokens per second: 5285.74 | Training tokens per second (%): 7.37 | MFU (%): 27.55 | TFLOPs: 272.43 | Global batch size: 4 | Global tokens/sec: 21142.95 | Global MFU (%): 27.55 | Global TFLOPs: 1089.73 | 
2025-05-18 20:41:35,186 - root - INFO - Step: 470 | Loss: 6.19 | Tokens per second: 5325.95 | Training tokens per second (%): 8.34 | MFU (%): 27.76 | TFLOPs: 274.51 | Global batch size: 4 | Global tokens/sec: 21303.80 | Global MFU (%): 27.76 | Global TFLOPs: 1098.02 | 
2025-05-18 20:41:39,078 - root - INFO - Step: 480 | Loss: 5.62 | Tokens per second: 5263.46 | Training tokens per second (%): 9.74 | MFU (%): 27.43 | TFLOPs: 271.29 | Global batch size: 4 | Global tokens/sec: 21053.86 | Global MFU (%): 27.43 | Global TFLOPs: 1085.14 | 
2025-05-18 20:41:42,952 - root - INFO - Step: 490 | Loss: 6.38 | Tokens per second: 5288.29 | Training tokens per second (%): 10.00 | MFU (%): 27.56 | TFLOPs: 272.56 | Global batch size: 4 | Global tokens/sec: 21153.15 | Global MFU (%): 27.56 | Global TFLOPs: 1090.26 | 
2025-05-18 20:41:46,808 - root - INFO - Step: 500 | Loss: 6.19 | Tokens per second: 5311.50 | Training tokens per second (%): 7.05 | MFU (%): 27.68 | TFLOPs: 273.76 | Global batch size: 4 | Global tokens/sec: 21246.01 | Global MFU (%): 27.68 | Global TFLOPs: 1095.04 | 
2025-05-18 20:41:50,656 - root - INFO - Step: 510 | Loss: 5.38 | Tokens per second: 5324.08 | Training tokens per second (%): 9.58 | MFU (%): 27.75 | TFLOPs: 274.41 | Global batch size: 4 | Global tokens/sec: 21296.33 | Global MFU (%): 27.75 | Global TFLOPs: 1097.64 | 
2025-05-18 20:41:54,481 - root - INFO - Step: 520 | Loss: 4.85 | Tokens per second: 5355.08 | Training tokens per second (%): 5.13 | MFU (%): 27.91 | TFLOPs: 276.01 | Global batch size: 4 | Global tokens/sec: 21420.32 | Global MFU (%): 27.91 | Global TFLOPs: 1104.03 | 
2025-05-18 20:41:58,375 - root - INFO - Step: 530 | Loss: 5.49 | Tokens per second: 5260.35 | Training tokens per second (%): 10.79 | MFU (%): 27.41 | TFLOPs: 271.12 | Global batch size: 4 | Global tokens/sec: 21041.39 | Global MFU (%): 27.41 | Global TFLOPs: 1084.50 | 
2025-05-18 20:42:02,211 - root - INFO - Step: 540 | Loss: 4.41 | Tokens per second: 5340.40 | Training tokens per second (%): 7.94 | MFU (%): 27.83 | TFLOPs: 275.25 | Global batch size: 4 | Global tokens/sec: 21361.61 | Global MFU (%): 27.83 | Global TFLOPs: 1101.00 | 
2025-05-18 20:42:06,079 - root - INFO - Step: 550 | Loss: 5.05 | Tokens per second: 5295.85 | Training tokens per second (%): 7.44 | MFU (%): 27.60 | TFLOPs: 272.95 | Global batch size: 4 | Global tokens/sec: 21183.39 | Global MFU (%): 27.60 | Global TFLOPs: 1091.82 | 
2025-05-18 20:42:09,942 - root - INFO - Step: 560 | Loss: 4.47 | Tokens per second: 5303.53 | Training tokens per second (%): 11.20 | MFU (%): 27.64 | TFLOPs: 273.35 | Global batch size: 4 | Global tokens/sec: 21214.12 | Global MFU (%): 27.64 | Global TFLOPs: 1093.40 | 
2025-05-18 20:42:13,824 - root - INFO - Step: 570 | Loss: 4.82 | Tokens per second: 5276.37 | Training tokens per second (%): 7.73 | MFU (%): 27.50 | TFLOPs: 271.95 | Global batch size: 4 | Global tokens/sec: 21105.48 | Global MFU (%): 27.50 | Global TFLOPs: 1087.80 | 
2025-05-18 20:42:17,660 - root - INFO - Step: 580 | Loss: 4.81 | Tokens per second: 5340.66 | Training tokens per second (%): 10.23 | MFU (%): 27.83 | TFLOPs: 275.26 | Global batch size: 4 | Global tokens/sec: 21362.65 | Global MFU (%): 27.83 | Global TFLOPs: 1101.06 | 
2025-05-18 20:42:21,535 - root - INFO - Step: 590 | Loss: 5.36 | Tokens per second: 5285.88 | Training tokens per second (%): 10.85 | MFU (%): 27.55 | TFLOPs: 272.44 | Global batch size: 4 | Global tokens/sec: 21143.53 | Global MFU (%): 27.55 | Global TFLOPs: 1089.76 | 
2025-05-18 20:42:25,417 - root - INFO - Step: 600 | Loss: 5.89 | Tokens per second: 5277.31 | Training tokens per second (%): 9.94 | MFU (%): 27.50 | TFLOPs: 272.00 | Global batch size: 4 | Global tokens/sec: 21109.23 | Global MFU (%): 27.50 | Global TFLOPs: 1088.00 | 
2025-05-18 20:42:29,278 - root - INFO - Step: 610 | Loss: 4.95 | Tokens per second: 5304.48 | Training tokens per second (%): 10.40 | MFU (%): 27.64 | TFLOPs: 273.40 | Global batch size: 4 | Global tokens/sec: 21217.92 | Global MFU (%): 27.64 | Global TFLOPs: 1093.60 | 
2025-05-18 20:42:33,113 - root - INFO - Step: 620 | Loss: 5.62 | Tokens per second: 5341.60 | Training tokens per second (%): 8.68 | MFU (%): 27.84 | TFLOPs: 275.31 | Global batch size: 4 | Global tokens/sec: 21366.40 | Global MFU (%): 27.84 | Global TFLOPs: 1101.25 | 
2025-05-18 20:42:36,962 - root - INFO - Step: 630 | Loss: 4.82 | Tokens per second: 5322.80 | Training tokens per second (%): 7.55 | MFU (%): 27.74 | TFLOPs: 274.34 | Global batch size: 4 | Global tokens/sec: 21291.18 | Global MFU (%): 27.74 | Global TFLOPs: 1097.37 | 
2025-05-18 20:42:40,835 - root - INFO - Step: 640 | Loss: 5.67 | Tokens per second: 5288.67 | Training tokens per second (%): 11.81 | MFU (%): 27.56 | TFLOPs: 272.58 | Global batch size: 4 | Global tokens/sec: 21154.67 | Global MFU (%): 27.56 | Global TFLOPs: 1090.34 | 
2025-05-18 20:42:44,737 - root - INFO - Step: 650 | Loss: 4.83 | Tokens per second: 5250.53 | Training tokens per second (%): 9.17 | MFU (%): 27.36 | TFLOPs: 270.62 | Global batch size: 4 | Global tokens/sec: 21002.13 | Global MFU (%): 27.36 | Global TFLOPs: 1082.48 | 
2025-05-18 20:42:48,602 - root - INFO - Step: 660 | Loss: 5.91 | Tokens per second: 5300.25 | Training tokens per second (%): 12.83 | MFU (%): 27.62 | TFLOPs: 273.18 | Global batch size: 4 | Global tokens/sec: 21200.99 | Global MFU (%): 27.62 | Global TFLOPs: 1092.72 | 
2025-05-18 20:42:52,490 - root - INFO - Step: 670 | Loss: 4.89 | Tokens per second: 5268.11 | Training tokens per second (%): 9.32 | MFU (%): 27.45 | TFLOPs: 271.52 | Global batch size: 4 | Global tokens/sec: 21072.44 | Global MFU (%): 27.45 | Global TFLOPs: 1086.10 | 
2025-05-18 20:42:56,374 - root - INFO - Step: 680 | Loss: 5.16 | Tokens per second: 5274.28 | Training tokens per second (%): 14.27 | MFU (%): 27.49 | TFLOPs: 271.84 | Global batch size: 4 | Global tokens/sec: 21097.10 | Global MFU (%): 27.49 | Global TFLOPs: 1087.37 | 
2025-05-18 20:43:00,231 - root - INFO - Step: 690 | Loss: 5.53 | Tokens per second: 5310.85 | Training tokens per second (%): 6.17 | MFU (%): 27.68 | TFLOPs: 273.73 | Global batch size: 4 | Global tokens/sec: 21243.39 | Global MFU (%): 27.68 | Global TFLOPs: 1094.91 | 
2025-05-18 20:43:04,066 - root - INFO - Step: 700 | Loss: 4.46 | Tokens per second: 5340.93 | Training tokens per second (%): 8.71 | MFU (%): 27.83 | TFLOPs: 275.28 | Global batch size: 4 | Global tokens/sec: 21363.72 | Global MFU (%): 27.83 | Global TFLOPs: 1101.11 | 
2025-05-18 20:43:07,929 - root - INFO - Step: 710 | Loss: 4.90 | Tokens per second: 5302.95 | Training tokens per second (%): 10.39 | MFU (%): 27.64 | TFLOPs: 273.32 | Global batch size: 4 | Global tokens/sec: 21211.81 | Global MFU (%): 27.64 | Global TFLOPs: 1093.28 | 
2025-05-18 20:43:11,801 - root - INFO - Step: 720 | Loss: 5.61 | Tokens per second: 5290.76 | Training tokens per second (%): 6.28 | MFU (%): 27.57 | TFLOPs: 272.69 | Global batch size: 4 | Global tokens/sec: 21163.02 | Global MFU (%): 27.57 | Global TFLOPs: 1090.77 | 
2025-05-18 20:43:15,645 - root - INFO - Step: 730 | Loss: 5.35 | Tokens per second: 5329.46 | Training tokens per second (%): 5.26 | MFU (%): 27.77 | TFLOPs: 274.69 | Global batch size: 4 | Global tokens/sec: 21317.83 | Global MFU (%): 27.77 | Global TFLOPs: 1098.75 | 
2025-05-18 20:43:19,481 - root - INFO - Step: 740 | Loss: 5.44 | Tokens per second: 5339.94 | Training tokens per second (%): 7.17 | MFU (%): 27.83 | TFLOPs: 275.23 | Global batch size: 4 | Global tokens/sec: 21359.78 | Global MFU (%): 27.83 | Global TFLOPs: 1100.91 | 
2025-05-18 20:43:23,383 - root - INFO - Step: 750 | Loss: 5.70 | Tokens per second: 5249.24 | Training tokens per second (%): 11.14 | MFU (%): 27.36 | TFLOPs: 270.55 | Global batch size: 4 | Global tokens/sec: 20996.97 | Global MFU (%): 27.36 | Global TFLOPs: 1082.21 | 
2025-05-18 20:43:27,274 - root - INFO - Step: 760 | Loss: 4.45 | Tokens per second: 5264.60 | Training tokens per second (%): 9.47 | MFU (%): 27.44 | TFLOPs: 271.34 | Global batch size: 4 | Global tokens/sec: 21058.41 | Global MFU (%): 27.44 | Global TFLOPs: 1085.38 | 
2025-05-18 20:43:31,169 - root - INFO - Step: 770 | Loss: 4.34 | Tokens per second: 5259.74 | Training tokens per second (%): 13.64 | MFU (%): 27.41 | TFLOPs: 271.09 | Global batch size: 4 | Global tokens/sec: 21038.94 | Global MFU (%): 27.41 | Global TFLOPs: 1084.37 | 
2025-05-18 20:43:35,012 - root - INFO - Step: 780 | Loss: 4.44 | Tokens per second: 5329.97 | Training tokens per second (%): 8.24 | MFU (%): 27.78 | TFLOPs: 274.71 | Global batch size: 4 | Global tokens/sec: 21319.90 | Global MFU (%): 27.78 | Global TFLOPs: 1098.85 | 
2025-05-18 20:43:38,868 - root - INFO - Step: 790 | Loss: 4.20 | Tokens per second: 5313.55 | Training tokens per second (%): 9.60 | MFU (%): 27.69 | TFLOPs: 273.87 | Global batch size: 4 | Global tokens/sec: 21254.18 | Global MFU (%): 27.69 | Global TFLOPs: 1095.47 | 
2025-05-18 20:43:42,756 - root - INFO - Step: 800 | Loss: 4.42 | Tokens per second: 5267.84 | Training tokens per second (%): 7.75 | MFU (%): 27.45 | TFLOPs: 271.51 | Global batch size: 4 | Global tokens/sec: 21071.36 | Global MFU (%): 27.45 | Global TFLOPs: 1086.04 | 
2025-05-18 20:43:46,601 - root - INFO - Step: 810 | Loss: 3.79 | Tokens per second: 5328.25 | Training tokens per second (%): 8.58 | MFU (%): 27.77 | TFLOPs: 274.62 | Global batch size: 4 | Global tokens/sec: 21313.01 | Global MFU (%): 27.77 | Global TFLOPs: 1098.50 | 
2025-05-18 20:43:50,468 - root - INFO - Step: 820 | Loss: 4.90 | Tokens per second: 5296.50 | Training tokens per second (%): 9.21 | MFU (%): 27.60 | TFLOPs: 272.99 | Global batch size: 4 | Global tokens/sec: 21185.99 | Global MFU (%): 27.60 | Global TFLOPs: 1091.95 | 
2025-05-18 20:43:54,335 - root - INFO - Step: 830 | Loss: 4.68 | Tokens per second: 5298.01 | Training tokens per second (%): 9.89 | MFU (%): 27.61 | TFLOPs: 273.07 | Global batch size: 4 | Global tokens/sec: 21192.02 | Global MFU (%): 27.61 | Global TFLOPs: 1092.26 | 
2025-05-18 20:43:58,213 - root - INFO - Step: 840 | Loss: 4.53 | Tokens per second: 5281.68 | Training tokens per second (%): 13.02 | MFU (%): 27.53 | TFLOPs: 272.22 | Global batch size: 4 | Global tokens/sec: 21126.70 | Global MFU (%): 27.53 | Global TFLOPs: 1088.90 | 
2025-05-18 20:44:02,074 - root - INFO - Step: 850 | Loss: 4.27 | Tokens per second: 5306.64 | Training tokens per second (%): 10.97 | MFU (%): 27.66 | TFLOPs: 273.51 | Global batch size: 4 | Global tokens/sec: 21226.56 | Global MFU (%): 27.66 | Global TFLOPs: 1094.04 | 
2025-05-18 20:44:05,951 - root - INFO - Step: 860 | Loss: 3.68 | Tokens per second: 5283.33 | Training tokens per second (%): 11.34 | MFU (%): 27.53 | TFLOPs: 272.31 | Global batch size: 4 | Global tokens/sec: 21133.33 | Global MFU (%): 27.53 | Global TFLOPs: 1089.24 | 
2025-05-18 20:44:09,801 - root - INFO - Step: 870 | Loss: 4.37 | Tokens per second: 5319.86 | Training tokens per second (%): 7.67 | MFU (%): 27.72 | TFLOPs: 274.19 | Global batch size: 4 | Global tokens/sec: 21279.42 | Global MFU (%): 27.72 | Global TFLOPs: 1096.77 | 
2025-05-18 20:44:13,684 - root - INFO - Step: 880 | Loss: 3.63 | Tokens per second: 5275.54 | Training tokens per second (%): 11.27 | MFU (%): 27.49 | TFLOPs: 271.91 | Global batch size: 4 | Global tokens/sec: 21102.17 | Global MFU (%): 27.49 | Global TFLOPs: 1087.63 | 
2025-05-18 20:44:17,545 - root - INFO - Step: 890 | Loss: 4.37 | Tokens per second: 5306.60 | Training tokens per second (%): 6.13 | MFU (%): 27.66 | TFLOPs: 273.51 | Global batch size: 4 | Global tokens/sec: 21226.41 | Global MFU (%): 27.66 | Global TFLOPs: 1094.03 | 
2025-05-18 20:44:21,416 - root - INFO - Step: 900 | Loss: 4.04 | Tokens per second: 5290.65 | Training tokens per second (%): 11.78 | MFU (%): 27.57 | TFLOPs: 272.69 | Global batch size: 4 | Global tokens/sec: 21162.62 | Global MFU (%): 27.57 | Global TFLOPs: 1090.75 | 
2025-05-18 20:44:25,264 - root - INFO - Step: 910 | Loss: 3.51 | Tokens per second: 5323.67 | Training tokens per second (%): 9.42 | MFU (%): 27.74 | TFLOPs: 274.39 | Global batch size: 4 | Global tokens/sec: 21294.69 | Global MFU (%): 27.74 | Global TFLOPs: 1097.55 | 
2025-05-18 20:44:29,151 - root - INFO - Step: 920 | Loss: 4.84 | Tokens per second: 5270.30 | Training tokens per second (%): 11.08 | MFU (%): 27.47 | TFLOPs: 271.64 | Global batch size: 4 | Global tokens/sec: 21081.18 | Global MFU (%): 27.47 | Global TFLOPs: 1086.55 | 
2025-05-18 20:44:33,035 - root - INFO - Step: 930 | Loss: 4.56 | Tokens per second: 5274.76 | Training tokens per second (%): 11.91 | MFU (%): 27.49 | TFLOPs: 271.87 | Global batch size: 4 | Global tokens/sec: 21099.05 | Global MFU (%): 27.49 | Global TFLOPs: 1087.47 | 
2025-05-18 20:44:36,933 - root - INFO - Step: 940 | Loss: 5.08 | Tokens per second: 5255.09 | Training tokens per second (%): 12.13 | MFU (%): 27.39 | TFLOPs: 270.85 | Global batch size: 4 | Global tokens/sec: 21020.34 | Global MFU (%): 27.39 | Global TFLOPs: 1083.41 | 
2025-05-18 20:44:40,795 - root - INFO - Step: 950 | Loss: 4.50 | Tokens per second: 5303.59 | Training tokens per second (%): 10.72 | MFU (%): 27.64 | TFLOPs: 273.35 | Global batch size: 4 | Global tokens/sec: 21214.35 | Global MFU (%): 27.64 | Global TFLOPs: 1093.41 | 
2025-05-18 20:44:44,649 - root - INFO - Step: 960 | Loss: 4.41 | Tokens per second: 5314.87 | Training tokens per second (%): 8.67 | MFU (%): 27.70 | TFLOPs: 273.93 | Global batch size: 4 | Global tokens/sec: 21259.50 | Global MFU (%): 27.70 | Global TFLOPs: 1095.74 | 
2025-05-18 20:44:48,517 - root - INFO - Step: 970 | Loss: 4.03 | Tokens per second: 5297.17 | Training tokens per second (%): 7.96 | MFU (%): 27.61 | TFLOPs: 273.02 | Global batch size: 4 | Global tokens/sec: 21188.69 | Global MFU (%): 27.61 | Global TFLOPs: 1092.09 | 
2025-05-18 20:44:52,409 - root - INFO - Step: 980 | Loss: 4.19 | Tokens per second: 5262.38 | Training tokens per second (%): 9.82 | MFU (%): 27.42 | TFLOPs: 271.23 | Global batch size: 4 | Global tokens/sec: 21049.52 | Global MFU (%): 27.42 | Global TFLOPs: 1084.92 | 
2025-05-18 20:44:56,286 - root - INFO - Step: 990 | Loss: 4.88 | Tokens per second: 5283.80 | Training tokens per second (%): 6.67 | MFU (%): 27.54 | TFLOPs: 272.33 | Global batch size: 4 | Global tokens/sec: 21135.19 | Global MFU (%): 27.54 | Global TFLOPs: 1089.33 | 
2025-05-18 20:45:00,136 - root - INFO - Step: 1000 | Loss: 3.73 | Tokens per second: 5320.79 | Training tokens per second (%): 11.27 | MFU (%): 27.73 | TFLOPs: 274.24 | Global batch size: 4 | Global tokens/sec: 21283.14 | Global MFU (%): 27.73 | Global TFLOPs: 1096.96 | 
2025-05-18 20:45:00,136 - root - INFO - Training completed
[sbatch-master] task finished
