[sbatch-master] running on nid006596
[sbatch-master] SLURM_NODELIST: nid[006596,006600,006605]
[sbatch-master] SLURM_NNODES: 3
[sbatch-master] SLURM_NODEID: 0
[Master] Master node: nid006596
[Master] World size: 12
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
[srun] rank=0 host=nid006596 noderank=0 localrank=0
[srun] rank=2 host=nid006605 noderank=2 localrank=0
[srun] rank=1 host=nid006600 noderank=1 localrank=0
W0519 09:11:52.846000 215822 torch/distributed/run.py:792] 
W0519 09:11:52.846000 215822 torch/distributed/run.py:792] *****************************************
W0519 09:11:52.846000 215822 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0519 09:11:52.846000 215822 torch/distributed/run.py:792] *****************************************
W0519 09:11:53.332000 165904 torch/distributed/run.py:792] 
W0519 09:11:53.332000 165904 torch/distributed/run.py:792] *****************************************
W0519 09:11:53.332000 165904 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0519 09:11:53.332000 165904 torch/distributed/run.py:792] *****************************************
W0519 09:11:53.427000 211388 torch/distributed/run.py:792] 
W0519 09:11:53.427000 211388 torch/distributed/run.py:792] *****************************************
W0519 09:11:53.427000 211388 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0519 09:11:53.427000 211388 torch/distributed/run.py:792] *****************************************
2025-05-19 09:12:05,426 - root - INFO - [Distributed Init] Rank 8 initialized on node 2 on GPU 0.
2025-05-19 09:12:05,540 - root - INFO - [Distributed Init] Rank 0 initialized on node 0 on GPU 0.
2025-05-19 09:12:05,587 - root - INFO - [Distributed Init] Rank 4 initialized on node 1 on GPU 0.
[rank8]:[W519 09:12:05.408478800 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 8]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 09:12:05,920 - root - INFO - [Distributed Init] Rank 11 initialized on node 2 on GPU 3.
[rank11]:[W519 09:12:05.492481129 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 11]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 09:12:05,990 - root - INFO - [Distributed Init] Rank 10 initialized on node 2 on GPU 2.
[rank10]:[W519 09:12:05.562830083 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 10]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 09:12:06,000 - root - INFO - [Distributed Init] Rank 9 initialized on node 2 on GPU 1.
[rank9]:[W519 09:12:05.572084073 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 9]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank0]:[W519 09:12:05.925884114 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 09:12:06,046 - root - INFO - [Distributed Init] Rank 2 initialized on node 0 on GPU 2.
[rank2]:[W519 09:12:06.969172966 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W519 09:12:06.460701457 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 09:12:06,068 - root - INFO - [Distributed Init] Rank 5 initialized on node 1 on GPU 1.
[rank5]:[W519 09:12:06.482354447 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 5]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 09:12:06,098 - root - INFO - [Distributed Init] Rank 7 initialized on node 1 on GPU 3.
[rank7]:[W519 09:12:06.512363471 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 7]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 09:12:06,108 - root - INFO - [Distributed Init] Rank 6 initialized on node 1 on GPU 2.
[rank6]:[W519 09:12:06.522073420 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 6]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 09:12:06,120 - root - INFO - [Distributed Init] Rank 1 initialized on node 0 on GPU 1.
2025-05-19 09:12:06,120 - root - INFO - [Distributed Init] Rank 3 initialized on node 0 on GPU 3.
[rank1]:[W519 09:12:06.043336380 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W519 09:12:06.043346427 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 09:12:11,286 - root - INFO - [Rank 0] All ranks ready!
2025-05-19 09:12:11,286 - root - INFO - Distributed training enabled: 12 processes
2025-05-19 09:12:11,286 - root - INFO - Master process: 0 on cuda:0
2025-05-19 09:12:11,286 - root - INFO - Experiment args: Namespace(dataset='/capstor/scratch/cscs/kasparr/project/train_data_tokenized_padded_snappy.parquet', dataset_type='padded', pretokenized=True, tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=10, profile=False, profile_step_start=10, profile_step_end=10, grad_max_norm=1, model_dtype='bf16', compile=False, distributed=True, backend='nccl', find_unused_parameters=False)
2025-05-19 09:12:11,286 - root - INFO - Setting up Tokenizer...
2025-05-19 09:12:16,815 - root - INFO - Setting up DataLoaders...
2025-05-19 09:12:16,815 - root - INFO - Using pretokenized data: /capstor/scratch/cscs/kasparr/project/train_data_tokenized_padded_snappy.parquet
Loaded pretokenized dataset with 785906 samples
Loaded pretokenized dataset with 785906 samples
Loaded pretokenized dataset with 785906 samples
Loaded pretokenized dataset with 785906 samples
Loaded pretokenized dataset with 785906 samples
Loaded pretokenized dataset with 785906 samples
Loaded pretokenized dataset with 785906 samples
Loaded pretokenized dataset with 785906 samples
Loaded pretokenized dataset with 785906 samples
Loaded pretokenized dataset with 785906 samples
Loaded pretokenized dataset with 785906 samples
2025-05-19 09:12:49,084 - root - INFO - Setting up Model...
Loaded pretokenized dataset with 785906 samples
2025-05-19 09:12:57,810 - root - INFO - Model wrapped with DistributedDataParallel
2025-05-19 09:12:57,811 - root - INFO - Global batch size: 96 (local: 8 Ã— 12 processes)
2025-05-19 09:12:57,811 - root - INFO - Starting training!
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
2025-05-19 09:12:59,471 - root - INFO - Step: 1 | Loss: 11.98 | Tokens per second: 9871.14 | Training tokens per second (%): 3.60 | MFU (%): 9.24 | TFLOPs: 91.42 | Global batch size: 96 | Global tokens/sec: 118453.71 | Global MFU (%): 9.24 | Global TFLOPs: 1097.05 | 
2025-05-19 09:13:03,634 - root - INFO - Step: 10 | Loss: 11.90 | Tokens per second: 35428.97 | Training tokens per second (%): 2.97 | MFU (%): 33.18 | TFLOPs: 328.12 | Global batch size: 96 | Global tokens/sec: 425147.65 | Global MFU (%): 33.18 | Global TFLOPs: 3937.47 | 
2025-05-19 09:13:07,837 - root - INFO - Step: 20 | Loss: 11.40 | Tokens per second: 38988.74 | Training tokens per second (%): 2.51 | MFU (%): 36.51 | TFLOPs: 361.09 | Global batch size: 96 | Global tokens/sec: 467864.91 | Global MFU (%): 36.51 | Global TFLOPs: 4333.10 | 
2025-05-19 09:13:12,069 - root - INFO - Step: 30 | Loss: 10.84 | Tokens per second: 38724.64 | Training tokens per second (%): 3.23 | MFU (%): 36.26 | TFLOPs: 358.65 | Global batch size: 96 | Global tokens/sec: 464695.64 | Global MFU (%): 36.26 | Global TFLOPs: 4303.75 | 
2025-05-19 09:13:16,273 - root - INFO - Step: 40 | Loss: 10.13 | Tokens per second: 38978.03 | Training tokens per second (%): 3.00 | MFU (%): 36.50 | TFLOPs: 360.99 | Global batch size: 96 | Global tokens/sec: 467736.30 | Global MFU (%): 36.50 | Global TFLOPs: 4331.91 | 
2025-05-19 09:13:20,483 - root - INFO - Step: 50 | Loss: 9.56 | Tokens per second: 38921.79 | Training tokens per second (%): 3.23 | MFU (%): 36.45 | TFLOPs: 360.47 | Global batch size: 96 | Global tokens/sec: 467061.51 | Global MFU (%): 36.45 | Global TFLOPs: 4325.66 | 
2025-05-19 09:13:24,704 - root - INFO - Step: 60 | Loss: 9.00 | Tokens per second: 38825.24 | Training tokens per second (%): 3.30 | MFU (%): 36.36 | TFLOPs: 359.58 | Global batch size: 96 | Global tokens/sec: 465902.84 | Global MFU (%): 36.36 | Global TFLOPs: 4314.93 | 
2025-05-19 09:13:28,920 - root - INFO - Step: 70 | Loss: 8.50 | Tokens per second: 38874.65 | Training tokens per second (%): 3.24 | MFU (%): 36.40 | TFLOPs: 360.03 | Global batch size: 96 | Global tokens/sec: 466495.76 | Global MFU (%): 36.40 | Global TFLOPs: 4320.42 | 
2025-05-19 09:13:33,226 - root - INFO - Step: 80 | Loss: 8.03 | Tokens per second: 38053.84 | Training tokens per second (%): 3.47 | MFU (%): 35.64 | TFLOPs: 352.43 | Global batch size: 96 | Global tokens/sec: 456646.07 | Global MFU (%): 35.64 | Global TFLOPs: 4229.19 | 
2025-05-19 09:13:37,254 - root - INFO - Step: 90 | Loss: 7.70 | Tokens per second: 40686.21 | Training tokens per second (%): 3.08 | MFU (%): 38.10 | TFLOPs: 376.81 | Global batch size: 96 | Global tokens/sec: 488234.53 | Global MFU (%): 38.10 | Global TFLOPs: 4521.75 | 
2025-05-19 09:13:41,464 - root - INFO - Step: 100 | Loss: 7.45 | Tokens per second: 38918.82 | Training tokens per second (%): 3.33 | MFU (%): 36.45 | TFLOPs: 360.44 | Global batch size: 96 | Global tokens/sec: 467025.89 | Global MFU (%): 36.45 | Global TFLOPs: 4325.33 | 
2025-05-19 09:13:45,674 - root - INFO - Step: 110 | Loss: 7.33 | Tokens per second: 38926.60 | Training tokens per second (%): 3.48 | MFU (%): 36.45 | TFLOPs: 360.52 | Global batch size: 96 | Global tokens/sec: 467119.17 | Global MFU (%): 36.45 | Global TFLOPs: 4326.19 | 
2025-05-19 09:13:49,899 - root - INFO - Step: 120 | Loss: 7.27 | Tokens per second: 38786.11 | Training tokens per second (%): 3.14 | MFU (%): 36.32 | TFLOPs: 359.21 | Global batch size: 96 | Global tokens/sec: 465433.32 | Global MFU (%): 36.32 | Global TFLOPs: 4310.58 | 
2025-05-19 09:13:54,113 - root - INFO - Step: 130 | Loss: 7.11 | Tokens per second: 38884.99 | Training tokens per second (%): 3.39 | MFU (%): 36.41 | TFLOPs: 360.13 | Global batch size: 96 | Global tokens/sec: 466619.93 | Global MFU (%): 36.41 | Global TFLOPs: 4321.57 | 
2025-05-19 09:13:58,326 - root - INFO - Step: 140 | Loss: 7.01 | Tokens per second: 38896.23 | Training tokens per second (%): 2.82 | MFU (%): 36.42 | TFLOPs: 360.23 | Global batch size: 96 | Global tokens/sec: 466754.79 | Global MFU (%): 36.42 | Global TFLOPs: 4322.82 | 
2025-05-19 09:14:02,568 - root - INFO - Step: 150 | Loss: 7.00 | Tokens per second: 38638.41 | Training tokens per second (%): 3.41 | MFU (%): 36.18 | TFLOPs: 357.85 | Global batch size: 96 | Global tokens/sec: 463660.90 | Global MFU (%): 36.18 | Global TFLOPs: 4294.16 | 
2025-05-19 09:14:06,793 - root - INFO - Step: 160 | Loss: 6.96 | Tokens per second: 38784.87 | Training tokens per second (%): 3.03 | MFU (%): 36.32 | TFLOPs: 359.20 | Global batch size: 96 | Global tokens/sec: 465418.41 | Global MFU (%): 36.32 | Global TFLOPs: 4310.44 | 
2025-05-19 09:14:10,794 - root - INFO - Step: 170 | Loss: 6.91 | Tokens per second: 40960.04 | Training tokens per second (%): 2.71 | MFU (%): 38.36 | TFLOPs: 379.35 | Global batch size: 96 | Global tokens/sec: 491520.51 | Global MFU (%): 38.36 | Global TFLOPs: 4552.18 | 
2025-05-19 09:14:15,016 - root - INFO - Step: 180 | Loss: 6.88 | Tokens per second: 38808.82 | Training tokens per second (%): 3.18 | MFU (%): 36.34 | TFLOPs: 359.43 | Global batch size: 96 | Global tokens/sec: 465705.79 | Global MFU (%): 36.34 | Global TFLOPs: 4313.10 | 
2025-05-19 09:14:19,249 - root - INFO - Step: 190 | Loss: 6.89 | Tokens per second: 38710.63 | Training tokens per second (%): 3.06 | MFU (%): 36.25 | TFLOPs: 358.52 | Global batch size: 96 | Global tokens/sec: 464527.50 | Global MFU (%): 36.25 | Global TFLOPs: 4302.19 | 
2025-05-19 09:14:23,469 - root - INFO - Step: 200 | Loss: 6.82 | Tokens per second: 38837.89 | Training tokens per second (%): 2.95 | MFU (%): 36.37 | TFLOPs: 359.69 | Global batch size: 96 | Global tokens/sec: 466054.73 | Global MFU (%): 36.37 | Global TFLOPs: 4316.33 | 
2025-05-19 09:14:27,675 - root - INFO - Step: 210 | Loss: 6.79 | Tokens per second: 38957.48 | Training tokens per second (%): 3.19 | MFU (%): 36.48 | TFLOPs: 360.80 | Global batch size: 96 | Global tokens/sec: 467489.76 | Global MFU (%): 36.48 | Global TFLOPs: 4329.62 | 
2025-05-19 09:14:31,908 - root - INFO - Step: 220 | Loss: 6.73 | Tokens per second: 38713.19 | Training tokens per second (%): 2.92 | MFU (%): 36.25 | TFLOPs: 358.54 | Global batch size: 96 | Global tokens/sec: 464558.24 | Global MFU (%): 36.25 | Global TFLOPs: 4302.47 | 
2025-05-19 09:14:36,131 - root - INFO - Step: 230 | Loss: 6.71 | Tokens per second: 38802.45 | Training tokens per second (%): 2.95 | MFU (%): 36.34 | TFLOPs: 359.37 | Global batch size: 96 | Global tokens/sec: 465629.36 | Global MFU (%): 36.34 | Global TFLOPs: 4312.39 | 
2025-05-19 09:14:40,373 - root - INFO - Step: 240 | Loss: 6.61 | Tokens per second: 38633.03 | Training tokens per second (%): 3.01 | MFU (%): 36.18 | TFLOPs: 357.80 | Global batch size: 96 | Global tokens/sec: 463596.36 | Global MFU (%): 36.18 | Global TFLOPs: 4293.56 | 
2025-05-19 09:14:44,597 - root - INFO - Step: 250 | Loss: 6.55 | Tokens per second: 38799.44 | Training tokens per second (%): 2.90 | MFU (%): 36.33 | TFLOPs: 359.34 | Global batch size: 96 | Global tokens/sec: 465593.28 | Global MFU (%): 36.33 | Global TFLOPs: 4312.06 | 
2025-05-19 09:14:48,590 - root - INFO - Step: 260 | Loss: 6.53 | Tokens per second: 41038.28 | Training tokens per second (%): 3.16 | MFU (%): 38.43 | TFLOPs: 380.07 | Global batch size: 96 | Global tokens/sec: 492459.32 | Global MFU (%): 38.43 | Global TFLOPs: 4560.88 | 
2025-05-19 09:14:52,815 - root - INFO - Step: 270 | Loss: 6.53 | Tokens per second: 38787.63 | Training tokens per second (%): 3.02 | MFU (%): 36.32 | TFLOPs: 359.23 | Global batch size: 96 | Global tokens/sec: 465451.51 | Global MFU (%): 36.32 | Global TFLOPs: 4310.75 | 
2025-05-19 09:14:57,050 - root - INFO - Step: 280 | Loss: 6.49 | Tokens per second: 38693.24 | Training tokens per second (%): 2.72 | MFU (%): 36.23 | TFLOPs: 358.35 | Global batch size: 96 | Global tokens/sec: 464318.91 | Global MFU (%): 36.23 | Global TFLOPs: 4300.26 | 
2025-05-19 09:15:01,279 - root - INFO - Step: 290 | Loss: 6.44 | Tokens per second: 38743.86 | Training tokens per second (%): 2.91 | MFU (%): 36.28 | TFLOPs: 358.82 | Global batch size: 96 | Global tokens/sec: 464926.37 | Global MFU (%): 36.28 | Global TFLOPs: 4305.88 | 
2025-05-19 09:15:05,511 - root - INFO - Step: 300 | Loss: 6.43 | Tokens per second: 38727.91 | Training tokens per second (%): 3.43 | MFU (%): 36.27 | TFLOPs: 358.68 | Global batch size: 96 | Global tokens/sec: 464734.88 | Global MFU (%): 36.27 | Global TFLOPs: 4304.11 | 
2025-05-19 09:15:09,734 - root - INFO - Step: 310 | Loss: 6.40 | Tokens per second: 38804.91 | Training tokens per second (%): 3.26 | MFU (%): 36.34 | TFLOPs: 359.39 | Global batch size: 96 | Global tokens/sec: 465658.87 | Global MFU (%): 36.34 | Global TFLOPs: 4312.67 | 
2025-05-19 09:15:13,958 - root - INFO - Step: 320 | Loss: 6.31 | Tokens per second: 38790.72 | Training tokens per second (%): 2.98 | MFU (%): 36.33 | TFLOPs: 359.26 | Global batch size: 96 | Global tokens/sec: 465488.67 | Global MFU (%): 36.33 | Global TFLOPs: 4311.09 | 
2025-05-19 09:15:18,189 - root - INFO - Step: 330 | Loss: 6.35 | Tokens per second: 38736.36 | Training tokens per second (%): 2.72 | MFU (%): 36.27 | TFLOPs: 358.75 | Global batch size: 96 | Global tokens/sec: 464836.31 | Global MFU (%): 36.27 | Global TFLOPs: 4305.05 | 
2025-05-19 09:15:22,177 - root - INFO - Step: 340 | Loss: 6.28 | Tokens per second: 41086.57 | Training tokens per second (%): 3.16 | MFU (%): 38.48 | TFLOPs: 380.52 | Global batch size: 96 | Global tokens/sec: 493038.87 | Global MFU (%): 38.48 | Global TFLOPs: 4566.24 | 
2025-05-19 09:15:26,419 - root - INFO - Step: 350 | Loss: 6.27 | Tokens per second: 38634.35 | Training tokens per second (%): 2.75 | MFU (%): 36.18 | TFLOPs: 357.81 | Global batch size: 96 | Global tokens/sec: 463612.26 | Global MFU (%): 36.18 | Global TFLOPs: 4293.71 | 
2025-05-19 09:15:30,645 - root - INFO - Step: 360 | Loss: 6.24 | Tokens per second: 38771.02 | Training tokens per second (%): 2.86 | MFU (%): 36.31 | TFLOPs: 359.07 | Global batch size: 96 | Global tokens/sec: 465252.18 | Global MFU (%): 36.31 | Global TFLOPs: 4308.90 | 
2025-05-19 09:15:34,871 - root - INFO - Step: 370 | Loss: 6.17 | Tokens per second: 38775.55 | Training tokens per second (%): 3.51 | MFU (%): 36.31 | TFLOPs: 359.12 | Global batch size: 96 | Global tokens/sec: 465306.60 | Global MFU (%): 36.31 | Global TFLOPs: 4309.40 | 
2025-05-19 09:15:39,076 - root - INFO - Step: 380 | Loss: 6.25 | Tokens per second: 38975.43 | Training tokens per second (%): 3.52 | MFU (%): 36.50 | TFLOPs: 360.97 | Global batch size: 96 | Global tokens/sec: 467705.14 | Global MFU (%): 36.50 | Global TFLOPs: 4331.62 | 
2025-05-19 09:15:43,305 - root - INFO - Step: 390 | Loss: 6.10 | Tokens per second: 38742.81 | Training tokens per second (%): 3.22 | MFU (%): 36.28 | TFLOPs: 358.81 | Global batch size: 96 | Global tokens/sec: 464913.71 | Global MFU (%): 36.28 | Global TFLOPs: 4305.77 | 
2025-05-19 09:15:47,521 - root - INFO - Step: 400 | Loss: 6.21 | Tokens per second: 38867.49 | Training tokens per second (%): 3.39 | MFU (%): 36.40 | TFLOPs: 359.97 | Global batch size: 96 | Global tokens/sec: 466409.90 | Global MFU (%): 36.40 | Global TFLOPs: 4319.62 | 
2025-05-19 09:15:51,737 - root - INFO - Step: 410 | Loss: 6.22 | Tokens per second: 38872.64 | Training tokens per second (%): 2.86 | MFU (%): 36.40 | TFLOPs: 360.02 | Global batch size: 96 | Global tokens/sec: 466471.65 | Global MFU (%): 36.40 | Global TFLOPs: 4320.19 | 
2025-05-19 09:15:55,748 - root - INFO - Step: 420 | Loss: 6.05 | Tokens per second: 40852.78 | Training tokens per second (%): 3.42 | MFU (%): 38.26 | TFLOPs: 378.36 | Global batch size: 96 | Global tokens/sec: 490233.37 | Global MFU (%): 38.26 | Global TFLOPs: 4540.26 | 
2025-05-19 09:15:59,972 - root - INFO - Step: 430 | Loss: 6.08 | Tokens per second: 38798.34 | Training tokens per second (%): 2.94 | MFU (%): 36.33 | TFLOPs: 359.33 | Global batch size: 96 | Global tokens/sec: 465580.04 | Global MFU (%): 36.33 | Global TFLOPs: 4311.94 | 
2025-05-19 09:16:04,200 - root - INFO - Step: 440 | Loss: 6.05 | Tokens per second: 38760.41 | Training tokens per second (%): 3.35 | MFU (%): 36.30 | TFLOPs: 358.98 | Global batch size: 96 | Global tokens/sec: 465124.92 | Global MFU (%): 36.30 | Global TFLOPs: 4307.72 | 
2025-05-19 09:16:08,433 - root - INFO - Step: 450 | Loss: 6.08 | Tokens per second: 38713.08 | Training tokens per second (%): 3.16 | MFU (%): 36.25 | TFLOPs: 358.54 | Global batch size: 96 | Global tokens/sec: 464556.97 | Global MFU (%): 36.25 | Global TFLOPs: 4302.46 | 
2025-05-19 09:16:12,662 - root - INFO - Step: 460 | Loss: 5.99 | Tokens per second: 38748.98 | Training tokens per second (%): 3.32 | MFU (%): 36.29 | TFLOPs: 358.87 | Global batch size: 96 | Global tokens/sec: 464987.79 | Global MFU (%): 36.29 | Global TFLOPs: 4306.45 | 
2025-05-19 09:16:16,870 - root - INFO - Step: 470 | Loss: 5.97 | Tokens per second: 38941.38 | Training tokens per second (%): 3.33 | MFU (%): 36.47 | TFLOPs: 360.65 | Global batch size: 96 | Global tokens/sec: 467296.55 | Global MFU (%): 36.47 | Global TFLOPs: 4327.83 | 
2025-05-19 09:16:21,085 - root - INFO - Step: 480 | Loss: 6.03 | Tokens per second: 38877.83 | Training tokens per second (%): 3.50 | MFU (%): 36.41 | TFLOPs: 360.06 | Global batch size: 96 | Global tokens/sec: 466533.99 | Global MFU (%): 36.41 | Global TFLOPs: 4320.77 | 
2025-05-19 09:16:25,315 - root - INFO - Step: 490 | Loss: 6.00 | Tokens per second: 38738.81 | Training tokens per second (%): 2.87 | MFU (%): 36.28 | TFLOPs: 358.78 | Global batch size: 96 | Global tokens/sec: 464865.77 | Global MFU (%): 36.28 | Global TFLOPs: 4305.32 | 
2025-05-19 09:16:29,531 - root - INFO - Step: 500 | Loss: 6.06 | Tokens per second: 38867.88 | Training tokens per second (%): 2.90 | MFU (%): 36.40 | TFLOPs: 359.97 | Global batch size: 96 | Global tokens/sec: 466414.58 | Global MFU (%): 36.40 | Global TFLOPs: 4319.67 | 
2025-05-19 09:16:33,540 - root - INFO - Step: 510 | Loss: 5.89 | Tokens per second: 40876.92 | Training tokens per second (%): 2.95 | MFU (%): 38.28 | TFLOPs: 378.58 | Global batch size: 96 | Global tokens/sec: 490522.99 | Global MFU (%): 38.28 | Global TFLOPs: 4542.94 | 
2025-05-19 09:16:37,762 - root - INFO - Step: 520 | Loss: 5.90 | Tokens per second: 38807.21 | Training tokens per second (%): 2.88 | MFU (%): 36.34 | TFLOPs: 359.41 | Global batch size: 96 | Global tokens/sec: 465686.58 | Global MFU (%): 36.34 | Global TFLOPs: 4312.92 | 
2025-05-19 09:16:41,986 - root - INFO - Step: 530 | Loss: 5.80 | Tokens per second: 38799.97 | Training tokens per second (%): 3.49 | MFU (%): 36.33 | TFLOPs: 359.34 | Global batch size: 96 | Global tokens/sec: 465599.64 | Global MFU (%): 36.33 | Global TFLOPs: 4312.12 | 
2025-05-19 09:16:46,188 - root - INFO - Step: 540 | Loss: 5.89 | Tokens per second: 38992.89 | Training tokens per second (%): 3.49 | MFU (%): 36.51 | TFLOPs: 361.13 | Global batch size: 96 | Global tokens/sec: 467914.68 | Global MFU (%): 36.51 | Global TFLOPs: 4333.56 | 
2025-05-19 09:16:50,388 - root - INFO - Step: 550 | Loss: 5.82 | Tokens per second: 39018.89 | Training tokens per second (%): 3.07 | MFU (%): 36.54 | TFLOPs: 361.37 | Global batch size: 96 | Global tokens/sec: 468226.68 | Global MFU (%): 36.54 | Global TFLOPs: 4336.45 | 
2025-05-19 09:16:54,600 - root - INFO - Step: 560 | Loss: 5.79 | Tokens per second: 38906.51 | Training tokens per second (%): 3.55 | MFU (%): 36.43 | TFLOPs: 360.33 | Global batch size: 96 | Global tokens/sec: 466878.15 | Global MFU (%): 36.43 | Global TFLOPs: 4323.96 | 
2025-05-19 09:16:58,835 - root - INFO - Step: 570 | Loss: 5.83 | Tokens per second: 38691.85 | Training tokens per second (%): 3.34 | MFU (%): 36.23 | TFLOPs: 358.34 | Global batch size: 96 | Global tokens/sec: 464302.19 | Global MFU (%): 36.23 | Global TFLOPs: 4300.10 | 
2025-05-19 09:17:03,064 - root - INFO - Step: 580 | Loss: 5.74 | Tokens per second: 38752.66 | Training tokens per second (%): 3.39 | MFU (%): 36.29 | TFLOPs: 358.90 | Global batch size: 96 | Global tokens/sec: 465031.86 | Global MFU (%): 36.29 | Global TFLOPs: 4306.86 | 
2025-05-19 09:17:07,047 - root - INFO - Step: 590 | Loss: 5.79 | Tokens per second: 41135.33 | Training tokens per second (%): 3.28 | MFU (%): 38.52 | TFLOPs: 380.97 | Global batch size: 96 | Global tokens/sec: 493623.98 | Global MFU (%): 38.52 | Global TFLOPs: 4571.66 | 
2025-05-19 09:17:11,282 - root - INFO - Step: 600 | Loss: 5.78 | Tokens per second: 38700.69 | Training tokens per second (%): 2.96 | MFU (%): 36.24 | TFLOPs: 358.42 | Global batch size: 96 | Global tokens/sec: 464408.27 | Global MFU (%): 36.24 | Global TFLOPs: 4301.08 | 
2025-05-19 09:17:15,513 - root - INFO - Step: 610 | Loss: 5.83 | Tokens per second: 38728.77 | Training tokens per second (%): 2.97 | MFU (%): 36.27 | TFLOPs: 358.68 | Global batch size: 96 | Global tokens/sec: 464745.26 | Global MFU (%): 36.27 | Global TFLOPs: 4304.21 | 
2025-05-19 09:17:19,744 - root - INFO - Step: 620 | Loss: 5.67 | Tokens per second: 38730.41 | Training tokens per second (%): 3.02 | MFU (%): 36.27 | TFLOPs: 358.70 | Global batch size: 96 | Global tokens/sec: 464764.97 | Global MFU (%): 36.27 | Global TFLOPs: 4304.39 | 
2025-05-19 09:17:23,973 - root - INFO - Step: 630 | Loss: 5.74 | Tokens per second: 38748.89 | Training tokens per second (%): 2.98 | MFU (%): 36.29 | TFLOPs: 358.87 | Global batch size: 96 | Global tokens/sec: 464986.69 | Global MFU (%): 36.29 | Global TFLOPs: 4306.44 | 
2025-05-19 09:17:28,195 - root - INFO - Step: 640 | Loss: 5.71 | Tokens per second: 38812.83 | Training tokens per second (%): 3.12 | MFU (%): 36.35 | TFLOPs: 359.46 | Global batch size: 96 | Global tokens/sec: 465753.91 | Global MFU (%): 36.35 | Global TFLOPs: 4313.55 | 
2025-05-19 09:17:32,437 - root - INFO - Step: 650 | Loss: 5.73 | Tokens per second: 38634.06 | Training tokens per second (%): 2.90 | MFU (%): 36.18 | TFLOPs: 357.81 | Global batch size: 96 | Global tokens/sec: 463608.74 | Global MFU (%): 36.18 | Global TFLOPs: 4293.68 | 
2025-05-19 09:17:36,661 - root - INFO - Step: 660 | Loss: 5.72 | Tokens per second: 38795.24 | Training tokens per second (%): 3.03 | MFU (%): 36.33 | TFLOPs: 359.30 | Global batch size: 96 | Global tokens/sec: 465542.93 | Global MFU (%): 36.33 | Global TFLOPs: 4311.59 | 
2025-05-19 09:17:40,873 - root - INFO - Step: 670 | Loss: 5.66 | Tokens per second: 38903.43 | Training tokens per second (%): 3.42 | MFU (%): 36.43 | TFLOPs: 360.30 | Global batch size: 96 | Global tokens/sec: 466841.13 | Global MFU (%): 36.43 | Global TFLOPs: 4323.62 | 
2025-05-19 09:17:44,875 - root - INFO - Step: 680 | Loss: 5.62 | Tokens per second: 40944.01 | Training tokens per second (%): 2.96 | MFU (%): 38.34 | TFLOPs: 379.20 | Global batch size: 96 | Global tokens/sec: 491328.06 | Global MFU (%): 38.34 | Global TFLOPs: 4550.40 | 
2025-05-19 09:17:49,112 - root - INFO - Step: 690 | Loss: 5.63 | Tokens per second: 38679.19 | Training tokens per second (%): 3.25 | MFU (%): 36.22 | TFLOPs: 358.22 | Global batch size: 96 | Global tokens/sec: 464150.32 | Global MFU (%): 36.22 | Global TFLOPs: 4298.70 | 
2025-05-19 09:17:53,330 - root - INFO - Step: 700 | Loss: 5.57 | Tokens per second: 38850.23 | Training tokens per second (%): 3.37 | MFU (%): 36.38 | TFLOPs: 359.81 | Global batch size: 96 | Global tokens/sec: 466202.82 | Global MFU (%): 36.38 | Global TFLOPs: 4317.70 | 
2025-05-19 09:17:57,533 - root - INFO - Step: 710 | Loss: 5.55 | Tokens per second: 38988.30 | Training tokens per second (%): 2.99 | MFU (%): 36.51 | TFLOPs: 361.09 | Global batch size: 96 | Global tokens/sec: 467859.58 | Global MFU (%): 36.51 | Global TFLOPs: 4333.05 | 
2025-05-19 09:18:01,763 - root - INFO - Step: 720 | Loss: 5.55 | Tokens per second: 38735.93 | Training tokens per second (%): 3.19 | MFU (%): 36.27 | TFLOPs: 358.75 | Global batch size: 96 | Global tokens/sec: 464831.11 | Global MFU (%): 36.27 | Global TFLOPs: 4305.00 | 
2025-05-19 09:18:05,972 - root - INFO - Step: 730 | Loss: 5.54 | Tokens per second: 38935.32 | Training tokens per second (%): 3.34 | MFU (%): 36.46 | TFLOPs: 360.60 | Global batch size: 96 | Global tokens/sec: 467223.86 | Global MFU (%): 36.46 | Global TFLOPs: 4327.16 | 
2025-05-19 09:18:10,190 - root - INFO - Step: 740 | Loss: 5.69 | Tokens per second: 38849.39 | Training tokens per second (%): 2.91 | MFU (%): 36.38 | TFLOPs: 359.80 | Global batch size: 96 | Global tokens/sec: 466192.65 | Global MFU (%): 36.38 | Global TFLOPs: 4317.61 | 
2025-05-19 09:18:14,390 - root - INFO - Step: 750 | Loss: 5.59 | Tokens per second: 39020.55 | Training tokens per second (%): 3.23 | MFU (%): 36.54 | TFLOPs: 361.39 | Global batch size: 96 | Global tokens/sec: 468246.61 | Global MFU (%): 36.54 | Global TFLOPs: 4336.63 | 
2025-05-19 09:18:18,367 - root - INFO - Step: 760 | Loss: 5.54 | Tokens per second: 41201.43 | Training tokens per second (%): 2.97 | MFU (%): 38.58 | TFLOPs: 381.58 | Global batch size: 96 | Global tokens/sec: 494417.17 | Global MFU (%): 38.58 | Global TFLOPs: 4579.01 | 
2025-05-19 09:18:22,583 - root - INFO - Step: 770 | Loss: 5.46 | Tokens per second: 38863.87 | Training tokens per second (%): 2.92 | MFU (%): 36.39 | TFLOPs: 359.93 | Global batch size: 96 | Global tokens/sec: 466366.40 | Global MFU (%): 36.39 | Global TFLOPs: 4319.22 | 
2025-05-19 09:18:26,810 - root - INFO - Step: 780 | Loss: 5.57 | Tokens per second: 38774.99 | Training tokens per second (%): 3.39 | MFU (%): 36.31 | TFLOPs: 359.11 | Global batch size: 96 | Global tokens/sec: 465299.83 | Global MFU (%): 36.31 | Global TFLOPs: 4309.34 | 
2025-05-19 09:18:31,032 - root - INFO - Step: 790 | Loss: 5.55 | Tokens per second: 38808.07 | Training tokens per second (%): 2.81 | MFU (%): 36.34 | TFLOPs: 359.42 | Global batch size: 96 | Global tokens/sec: 465696.86 | Global MFU (%): 36.34 | Global TFLOPs: 4313.02 | 
2025-05-19 09:18:35,272 - root - INFO - Step: 800 | Loss: 5.44 | Tokens per second: 38652.77 | Training tokens per second (%): 3.47 | MFU (%): 36.20 | TFLOPs: 357.98 | Global batch size: 96 | Global tokens/sec: 463833.28 | Global MFU (%): 36.20 | Global TFLOPs: 4295.76 | 
2025-05-19 09:18:39,515 - root - INFO - Step: 810 | Loss: 5.57 | Tokens per second: 38619.70 | Training tokens per second (%): 3.09 | MFU (%): 36.17 | TFLOPs: 357.67 | Global batch size: 96 | Global tokens/sec: 463436.42 | Global MFU (%): 36.17 | Global TFLOPs: 4292.08 | 
2025-05-19 09:18:43,730 - root - INFO - Step: 820 | Loss: 5.60 | Tokens per second: 38876.41 | Training tokens per second (%): 3.09 | MFU (%): 36.41 | TFLOPs: 360.05 | Global batch size: 96 | Global tokens/sec: 466516.88 | Global MFU (%): 36.41 | Global TFLOPs: 4320.61 | 
2025-05-19 09:18:47,948 - root - INFO - Step: 830 | Loss: 5.50 | Tokens per second: 38845.68 | Training tokens per second (%): 3.20 | MFU (%): 36.38 | TFLOPs: 359.77 | Global batch size: 96 | Global tokens/sec: 466148.19 | Global MFU (%): 36.38 | Global TFLOPs: 4317.20 | 
2025-05-19 09:18:51,943 - root - INFO - Step: 840 | Loss: 5.56 | Tokens per second: 41020.04 | Training tokens per second (%): 3.20 | MFU (%): 38.41 | TFLOPs: 379.90 | Global batch size: 96 | Global tokens/sec: 492240.50 | Global MFU (%): 38.41 | Global TFLOPs: 4558.85 | 
2025-05-19 09:18:56,166 - root - INFO - Step: 850 | Loss: 5.45 | Tokens per second: 38808.12 | Training tokens per second (%): 3.49 | MFU (%): 36.34 | TFLOPs: 359.42 | Global batch size: 96 | Global tokens/sec: 465697.49 | Global MFU (%): 36.34 | Global TFLOPs: 4313.02 | 
2025-05-19 09:19:00,391 - root - INFO - Step: 860 | Loss: 5.38 | Tokens per second: 38788.01 | Training tokens per second (%): 3.51 | MFU (%): 36.32 | TFLOPs: 359.23 | Global batch size: 96 | Global tokens/sec: 465456.08 | Global MFU (%): 36.32 | Global TFLOPs: 4310.79 | 
2025-05-19 09:19:04,629 - root - INFO - Step: 870 | Loss: 5.42 | Tokens per second: 38658.66 | Training tokens per second (%): 3.34 | MFU (%): 36.20 | TFLOPs: 358.03 | Global batch size: 96 | Global tokens/sec: 463903.91 | Global MFU (%): 36.20 | Global TFLOPs: 4296.41 | 
2025-05-19 09:19:08,864 - root - INFO - Step: 880 | Loss: 5.39 | Tokens per second: 38702.35 | Training tokens per second (%): 3.67 | MFU (%): 36.24 | TFLOPs: 358.44 | Global batch size: 96 | Global tokens/sec: 464428.21 | Global MFU (%): 36.24 | Global TFLOPs: 4301.27 | 
2025-05-19 09:19:13,093 - root - INFO - Step: 890 | Loss: 5.48 | Tokens per second: 38743.63 | Training tokens per second (%): 3.72 | MFU (%): 36.28 | TFLOPs: 358.82 | Global batch size: 96 | Global tokens/sec: 464923.58 | Global MFU (%): 36.28 | Global TFLOPs: 4305.86 | 
2025-05-19 09:19:17,315 - root - INFO - Step: 900 | Loss: 5.39 | Tokens per second: 38817.03 | Training tokens per second (%): 2.91 | MFU (%): 36.35 | TFLOPs: 359.50 | Global batch size: 96 | Global tokens/sec: 465804.34 | Global MFU (%): 36.35 | Global TFLOPs: 4314.01 | 
2025-05-19 09:19:21,535 - root - INFO - Step: 910 | Loss: 5.40 | Tokens per second: 38827.69 | Training tokens per second (%): 3.23 | MFU (%): 36.36 | TFLOPs: 359.60 | Global batch size: 96 | Global tokens/sec: 465932.27 | Global MFU (%): 36.36 | Global TFLOPs: 4315.20 | 
2025-05-19 09:19:25,769 - root - INFO - Step: 920 | Loss: 5.50 | Tokens per second: 38701.85 | Training tokens per second (%): 2.96 | MFU (%): 36.24 | TFLOPs: 358.43 | Global batch size: 96 | Global tokens/sec: 464422.20 | Global MFU (%): 36.24 | Global TFLOPs: 4301.21 | 
2025-05-19 09:19:29,773 - root - INFO - Step: 930 | Loss: 5.34 | Tokens per second: 40933.19 | Training tokens per second (%): 3.16 | MFU (%): 38.33 | TFLOPs: 379.10 | Global batch size: 96 | Global tokens/sec: 491198.28 | Global MFU (%): 38.33 | Global TFLOPs: 4549.20 | 
2025-05-19 09:19:33,983 - root - INFO - Step: 940 | Loss: 5.35 | Tokens per second: 38916.81 | Training tokens per second (%): 2.77 | MFU (%): 36.44 | TFLOPs: 360.43 | Global batch size: 96 | Global tokens/sec: 467001.75 | Global MFU (%): 36.44 | Global TFLOPs: 4325.10 | 
2025-05-19 09:19:38,185 - root - INFO - Step: 950 | Loss: 5.38 | Tokens per second: 39004.91 | Training tokens per second (%): 2.67 | MFU (%): 36.53 | TFLOPs: 361.24 | Global batch size: 96 | Global tokens/sec: 468058.96 | Global MFU (%): 36.53 | Global TFLOPs: 4334.89 | 
2025-05-19 09:19:42,408 - root - INFO - Step: 960 | Loss: 5.41 | Tokens per second: 38801.87 | Training tokens per second (%): 3.14 | MFU (%): 36.34 | TFLOPs: 359.36 | Global batch size: 96 | Global tokens/sec: 465622.46 | Global MFU (%): 36.34 | Global TFLOPs: 4312.33 | 
2025-05-19 09:19:46,639 - root - INFO - Step: 970 | Loss: 5.41 | Tokens per second: 38726.11 | Training tokens per second (%): 3.25 | MFU (%): 36.26 | TFLOPs: 358.66 | Global batch size: 96 | Global tokens/sec: 464713.32 | Global MFU (%): 36.26 | Global TFLOPs: 4303.91 | 
2025-05-19 09:19:50,884 - root - INFO - Step: 980 | Loss: 5.36 | Tokens per second: 38610.17 | Training tokens per second (%): 3.34 | MFU (%): 36.16 | TFLOPs: 357.59 | Global batch size: 96 | Global tokens/sec: 463322.09 | Global MFU (%): 36.16 | Global TFLOPs: 4291.02 | 
2025-05-19 09:19:55,105 - root - INFO - Step: 990 | Loss: 5.33 | Tokens per second: 38822.71 | Training tokens per second (%): 3.42 | MFU (%): 36.36 | TFLOPs: 359.55 | Global batch size: 96 | Global tokens/sec: 465872.55 | Global MFU (%): 36.36 | Global TFLOPs: 4314.65 | 
2025-05-19 09:19:59,316 - root - INFO - Step: 1000 | Loss: 5.36 | Tokens per second: 38912.27 | Training tokens per second (%): 3.37 | MFU (%): 36.44 | TFLOPs: 360.38 | Global batch size: 96 | Global tokens/sec: 466947.26 | Global MFU (%): 36.44 | Global TFLOPs: 4324.60 | 
2025-05-19 09:19:59,316 - root - INFO - Training completed
[sbatch-master] task finished
