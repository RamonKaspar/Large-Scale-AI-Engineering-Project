[sbatch-master] running on nid006571
[sbatch-master] SLURM_NODELIST: nid[006571,006587]
[sbatch-master] SLURM_NNODES: 2
[sbatch-master] SLURM_NODEID: 0
[Master] Master node: nid006571
[Master] World size: 8
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
[srun] rank=0 host=nid006571 noderank=0 localrank=0
[srun] rank=1 host=nid006587 noderank=1 localrank=0
W0519 10:09:51.809000 170217 torch/distributed/run.py:792] 
W0519 10:09:51.809000 170217 torch/distributed/run.py:792] *****************************************
W0519 10:09:51.809000 170217 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0519 10:09:51.809000 170217 torch/distributed/run.py:792] *****************************************
W0519 10:09:53.006000 106292 torch/distributed/run.py:792] 
W0519 10:09:53.006000 106292 torch/distributed/run.py:792] *****************************************
W0519 10:09:53.006000 106292 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0519 10:09:53.006000 106292 torch/distributed/run.py:792] *****************************************
2025-05-19 10:10:05,014 - root - INFO - [Distributed Init] Rank 4 initialized on node 1 on GPU 0.
2025-05-19 10:10:05,099 - root - INFO - [Distributed Init] Rank 0 initialized on node 0 on GPU 0.
[rank4]:[W519 10:10:05.246716089 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 10:10:05,510 - root - INFO - [Distributed Init] Rank 5 initialized on node 1 on GPU 1.
[rank5]:[W519 10:10:05.302657058 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 5]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 10:10:05,681 - root - INFO - [Distributed Init] Rank 7 initialized on node 1 on GPU 3.
2025-05-19 10:10:05,681 - root - INFO - [Distributed Init] Rank 6 initialized on node 1 on GPU 2.
[rank6]:[W519 10:10:05.473708666 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 6]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W519 10:10:05.473709306 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 7]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank0]:[W519 10:10:05.221413226 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 10:10:05,952 - root - INFO - [Distributed Init] Rank 2 initialized on node 0 on GPU 2.
2025-05-19 10:10:05,953 - root - INFO - [Distributed Init] Rank 1 initialized on node 0 on GPU 1.
[rank2]:[W519 10:10:05.365278647 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W519 10:10:05.365355669 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 10:10:05,962 - root - INFO - [Distributed Init] Rank 3 initialized on node 0 on GPU 3.
[rank3]:[W519 10:10:05.372113621 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 10:10:10,888 - root - INFO - [Rank 0] All ranks ready!
2025-05-19 10:10:10,888 - root - INFO - Distributed training enabled: 8 processes
2025-05-19 10:10:10,888 - root - INFO - Master process: 0 on cuda:0
2025-05-19 10:10:10,888 - root - INFO - Experiment args: Namespace(dataset='/capstor/scratch/cscs/kasparr/project/train_data.parquet', dataset_type='padding-free', pretokenized=False, tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=10, profile=False, profile_step_start=10, profile_step_end=10, grad_max_norm=1, model_dtype='bf16', compile=False, distributed=True, backend='nccl', find_unused_parameters=False)
2025-05-19 10:10:10,888 - root - INFO - Setting up Tokenizer...
2025-05-19 10:10:11,431 - root - INFO - Setting up DataLoaders...
2025-05-19 10:10:11,431 - root - INFO - Using padding-free IterableParquetDataset with on-the-fly tokenization
DDP sharding: rank 3/8DDP sharding: rank 0/8

Rank 3/8 processing documents 294714 to 392951Rank 0/8 processing documents 0 to 98237

DDP sharding: rank 1/8
Rank 1/8 processing documents 98238 to 196475
2025-05-19 10:10:18,728 - root - INFO - Setting up Model...
DDP sharding: rank 2/8
Rank 2/8 processing documents 196476 to 294713
DDP sharding: rank 6/8DDP sharding: rank 7/8DDP sharding: rank 4/8DDP sharding: rank 5/8



Rank 7/8 processing documents 687666 to 785905Rank 5/8 processing documents 491190 to 589427Rank 6/8 processing documents 589428 to 687665Rank 4/8 processing documents 392952 to 491189



2025-05-19 10:10:28,252 - root - INFO - Model wrapped with DistributedDataParallel
2025-05-19 10:10:28,253 - root - INFO - Global batch size: 64 (local: 8 Ã— 8 processes)
2025-05-19 10:10:28,254 - root - INFO - Starting training!
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
2025-05-19 10:10:29,815 - root - INFO - Step: 1 | Loss: 11.95 | Tokens per second: 10501.23 | Training tokens per second (%): 12.47 | MFU (%): 9.83 | TFLOPs: 97.26 | Global batch size: 64 | Global tokens/sec: 84009.82 | Global MFU (%): 9.83 | Global TFLOPs: 778.05 | 
2025-05-19 10:10:34,171 - root - INFO - Step: 10 | Loss: 11.91 | Tokens per second: 33857.70 | Training tokens per second (%): 12.48 | MFU (%): 31.71 | TFLOPs: 313.57 | Global batch size: 64 | Global tokens/sec: 270861.61 | Global MFU (%): 31.71 | Global TFLOPs: 2508.57 | 
2025-05-19 10:10:38,847 - root - INFO - Step: 20 | Loss: 11.66 | Tokens per second: 35039.37 | Training tokens per second (%): 12.47 | MFU (%): 32.81 | TFLOPs: 324.51 | Global batch size: 64 | Global tokens/sec: 280314.99 | Global MFU (%): 32.81 | Global TFLOPs: 2596.12 | 
2025-05-19 10:10:43,360 - root - INFO - Step: 30 | Loss: 11.06 | Tokens per second: 36310.52 | Training tokens per second (%): 12.48 | MFU (%): 34.00 | TFLOPs: 336.29 | Global batch size: 64 | Global tokens/sec: 290484.12 | Global MFU (%): 34.00 | Global TFLOPs: 2690.30 | 
2025-05-19 10:10:48,090 - root - INFO - Step: 40 | Loss: 9.99 | Tokens per second: 34647.24 | Training tokens per second (%): 12.47 | MFU (%): 32.45 | TFLOPs: 320.88 | Global batch size: 64 | Global tokens/sec: 277177.95 | Global MFU (%): 32.45 | Global TFLOPs: 2567.06 | 
2025-05-19 10:10:52,587 - root - INFO - Step: 50 | Loss: 9.33 | Tokens per second: 36436.04 | Training tokens per second (%): 12.48 | MFU (%): 34.12 | TFLOPs: 337.45 | Global batch size: 64 | Global tokens/sec: 291488.29 | Global MFU (%): 34.12 | Global TFLOPs: 2699.60 | 
2025-05-19 10:10:57,147 - root - INFO - Step: 60 | Loss: 8.82 | Tokens per second: 35931.81 | Training tokens per second (%): 12.47 | MFU (%): 33.65 | TFLOPs: 332.78 | Global batch size: 64 | Global tokens/sec: 287454.52 | Global MFU (%): 33.65 | Global TFLOPs: 2662.24 | 
2025-05-19 10:11:01,659 - root - INFO - Step: 70 | Loss: 8.24 | Tokens per second: 36318.41 | Training tokens per second (%): 12.47 | MFU (%): 34.01 | TFLOPs: 336.36 | Global batch size: 64 | Global tokens/sec: 290547.32 | Global MFU (%): 34.01 | Global TFLOPs: 2690.88 | 
2025-05-19 10:11:06,240 - root - INFO - Step: 80 | Loss: 7.82 | Tokens per second: 35773.18 | Training tokens per second (%): 12.48 | MFU (%): 33.50 | TFLOPs: 331.31 | Global batch size: 64 | Global tokens/sec: 286185.46 | Global MFU (%): 33.50 | Global TFLOPs: 2650.49 | 
2025-05-19 10:11:10,754 - root - INFO - Step: 90 | Loss: 7.51 | Tokens per second: 36302.06 | Training tokens per second (%): 12.47 | MFU (%): 33.99 | TFLOPs: 336.21 | Global batch size: 64 | Global tokens/sec: 290416.51 | Global MFU (%): 33.99 | Global TFLOPs: 2689.67 | 
2025-05-19 10:11:15,433 - root - INFO - Step: 100 | Loss: 7.32 | Tokens per second: 35025.21 | Training tokens per second (%): 12.48 | MFU (%): 32.80 | TFLOPs: 324.38 | Global batch size: 64 | Global tokens/sec: 280201.68 | Global MFU (%): 32.80 | Global TFLOPs: 2595.07 | 
2025-05-19 10:11:20,010 - root - INFO - Step: 110 | Loss: 7.25 | Tokens per second: 35797.94 | Training tokens per second (%): 12.48 | MFU (%): 33.52 | TFLOPs: 331.54 | Global batch size: 64 | Global tokens/sec: 286383.56 | Global MFU (%): 33.52 | Global TFLOPs: 2652.32 | 
2025-05-19 10:11:24,529 - root - INFO - Step: 120 | Loss: 7.24 | Tokens per second: 36260.03 | Training tokens per second (%): 12.48 | MFU (%): 33.96 | TFLOPs: 335.82 | Global batch size: 64 | Global tokens/sec: 290080.27 | Global MFU (%): 33.96 | Global TFLOPs: 2686.56 | 
2025-05-19 10:11:29,028 - root - INFO - Step: 130 | Loss: 7.15 | Tokens per second: 36425.56 | Training tokens per second (%): 12.47 | MFU (%): 34.11 | TFLOPs: 337.35 | Global batch size: 64 | Global tokens/sec: 291404.47 | Global MFU (%): 34.11 | Global TFLOPs: 2698.82 | 
2025-05-19 10:11:33,706 - root - INFO - Step: 140 | Loss: 7.07 | Tokens per second: 35026.92 | Training tokens per second (%): 12.49 | MFU (%): 32.80 | TFLOPs: 324.40 | Global batch size: 64 | Global tokens/sec: 280215.33 | Global MFU (%): 32.80 | Global TFLOPs: 2595.19 | 
2025-05-19 10:11:38,206 - root - INFO - Step: 150 | Loss: 7.02 | Tokens per second: 36416.87 | Training tokens per second (%): 12.47 | MFU (%): 34.10 | TFLOPs: 337.27 | Global batch size: 64 | Global tokens/sec: 291334.95 | Global MFU (%): 34.10 | Global TFLOPs: 2698.18 | 
2025-05-19 10:11:42,694 - root - INFO - Step: 160 | Loss: 6.90 | Tokens per second: 36508.35 | Training tokens per second (%): 12.47 | MFU (%): 34.19 | TFLOPs: 338.12 | Global batch size: 64 | Global tokens/sec: 292066.78 | Global MFU (%): 34.19 | Global TFLOPs: 2704.96 | 
2025-05-19 10:11:47,251 - root - INFO - Step: 170 | Loss: 6.92 | Tokens per second: 35962.52 | Training tokens per second (%): 12.47 | MFU (%): 33.68 | TFLOPs: 333.06 | Global batch size: 64 | Global tokens/sec: 287700.19 | Global MFU (%): 33.68 | Global TFLOPs: 2664.51 | 
2025-05-19 10:11:51,817 - root - INFO - Step: 180 | Loss: 6.77 | Tokens per second: 35889.53 | Training tokens per second (%): 12.48 | MFU (%): 33.61 | TFLOPs: 332.39 | Global batch size: 64 | Global tokens/sec: 287116.26 | Global MFU (%): 33.61 | Global TFLOPs: 2659.11 | 
2025-05-19 10:11:56,388 - root - INFO - Step: 190 | Loss: 6.76 | Tokens per second: 35847.93 | Training tokens per second (%): 12.47 | MFU (%): 33.57 | TFLOPs: 332.00 | Global batch size: 64 | Global tokens/sec: 286783.40 | Global MFU (%): 33.57 | Global TFLOPs: 2656.02 | 
2025-05-19 10:12:00,962 - root - INFO - Step: 200 | Loss: 6.74 | Tokens per second: 35822.41 | Training tokens per second (%): 12.48 | MFU (%): 33.55 | TFLOPs: 331.77 | Global batch size: 64 | Global tokens/sec: 286579.24 | Global MFU (%): 33.55 | Global TFLOPs: 2654.13 | 
2025-05-19 10:12:05,519 - root - INFO - Step: 210 | Loss: 6.60 | Tokens per second: 35957.54 | Training tokens per second (%): 12.47 | MFU (%): 33.67 | TFLOPs: 333.02 | Global batch size: 64 | Global tokens/sec: 287660.36 | Global MFU (%): 33.67 | Global TFLOPs: 2664.15 | 
2025-05-19 10:12:10,017 - root - INFO - Step: 220 | Loss: 6.77 | Tokens per second: 36436.06 | Training tokens per second (%): 12.47 | MFU (%): 34.12 | TFLOPs: 337.45 | Global batch size: 64 | Global tokens/sec: 291488.45 | Global MFU (%): 34.12 | Global TFLOPs: 2699.60 | 
2025-05-19 10:12:14,649 - root - INFO - Step: 230 | Loss: 6.50 | Tokens per second: 35372.89 | Training tokens per second (%): 12.48 | MFU (%): 33.12 | TFLOPs: 327.60 | Global batch size: 64 | Global tokens/sec: 282983.15 | Global MFU (%): 33.12 | Global TFLOPs: 2620.83 | 
2025-05-19 10:12:19,216 - root - INFO - Step: 240 | Loss: 6.47 | Tokens per second: 35881.87 | Training tokens per second (%): 12.47 | MFU (%): 33.60 | TFLOPs: 332.32 | Global batch size: 64 | Global tokens/sec: 287054.94 | Global MFU (%): 33.60 | Global TFLOPs: 2658.54 | 
2025-05-19 10:12:23,820 - root - INFO - Step: 250 | Loss: 6.53 | Tokens per second: 35587.26 | Training tokens per second (%): 12.47 | MFU (%): 33.33 | TFLOPs: 329.59 | Global batch size: 64 | Global tokens/sec: 284698.06 | Global MFU (%): 33.33 | Global TFLOPs: 2636.71 | 
2025-05-19 10:12:28,335 - root - INFO - Step: 260 | Loss: 6.42 | Tokens per second: 36294.31 | Training tokens per second (%): 12.48 | MFU (%): 33.99 | TFLOPs: 336.14 | Global batch size: 64 | Global tokens/sec: 290354.52 | Global MFU (%): 33.99 | Global TFLOPs: 2689.10 | 
2025-05-19 10:12:32,977 - root - INFO - Step: 270 | Loss: 6.45 | Tokens per second: 35302.75 | Training tokens per second (%): 12.48 | MFU (%): 33.06 | TFLOPs: 326.95 | Global batch size: 64 | Global tokens/sec: 282422.04 | Global MFU (%): 33.06 | Global TFLOPs: 2615.63 | 
2025-05-19 10:12:37,577 - root - INFO - Step: 280 | Loss: 6.43 | Tokens per second: 35626.08 | Training tokens per second (%): 12.47 | MFU (%): 33.36 | TFLOPs: 329.95 | Global batch size: 64 | Global tokens/sec: 285008.67 | Global MFU (%): 33.36 | Global TFLOPs: 2639.59 | 
2025-05-19 10:12:42,131 - root - INFO - Step: 290 | Loss: 6.36 | Tokens per second: 35980.43 | Training tokens per second (%): 12.47 | MFU (%): 33.69 | TFLOPs: 333.23 | Global batch size: 64 | Global tokens/sec: 287843.48 | Global MFU (%): 33.69 | Global TFLOPs: 2665.84 | 
2025-05-19 10:12:46,663 - root - INFO - Step: 300 | Loss: 6.40 | Tokens per second: 36153.71 | Training tokens per second (%): 12.48 | MFU (%): 33.86 | TFLOPs: 334.83 | Global batch size: 64 | Global tokens/sec: 289229.65 | Global MFU (%): 33.86 | Global TFLOPs: 2678.68 | 
2025-05-19 10:12:51,242 - root - INFO - Step: 310 | Loss: 6.26 | Tokens per second: 35790.25 | Training tokens per second (%): 12.47 | MFU (%): 33.52 | TFLOPs: 331.47 | Global batch size: 64 | Global tokens/sec: 286322.00 | Global MFU (%): 33.52 | Global TFLOPs: 2651.75 | 
2025-05-19 10:12:55,799 - root - INFO - Step: 320 | Loss: 6.30 | Tokens per second: 35957.22 | Training tokens per second (%): 12.48 | MFU (%): 33.67 | TFLOPs: 333.02 | Global batch size: 64 | Global tokens/sec: 287657.78 | Global MFU (%): 33.67 | Global TFLOPs: 2664.12 | 
2025-05-19 10:13:00,342 - root - INFO - Step: 330 | Loss: 6.26 | Tokens per second: 36070.47 | Training tokens per second (%): 12.47 | MFU (%): 33.78 | TFLOPs: 334.06 | Global batch size: 64 | Global tokens/sec: 288563.75 | Global MFU (%): 33.78 | Global TFLOPs: 2672.51 | 
2025-05-19 10:13:04,916 - root - INFO - Step: 340 | Loss: 6.41 | Tokens per second: 35823.74 | Training tokens per second (%): 12.47 | MFU (%): 33.55 | TFLOPs: 331.78 | Global batch size: 64 | Global tokens/sec: 286589.96 | Global MFU (%): 33.55 | Global TFLOPs: 2654.23 | 
2025-05-19 10:13:09,452 - root - INFO - Step: 350 | Loss: 6.25 | Tokens per second: 36129.50 | Training tokens per second (%): 12.48 | MFU (%): 33.83 | TFLOPs: 334.61 | Global batch size: 64 | Global tokens/sec: 289036.04 | Global MFU (%): 33.83 | Global TFLOPs: 2676.89 | 
2025-05-19 10:13:13,948 - root - INFO - Step: 360 | Loss: 6.17 | Tokens per second: 36449.12 | Training tokens per second (%): 12.48 | MFU (%): 34.13 | TFLOPs: 337.57 | Global batch size: 64 | Global tokens/sec: 291592.93 | Global MFU (%): 34.13 | Global TFLOPs: 2700.57 | 
2025-05-19 10:13:18,591 - root - INFO - Step: 370 | Loss: 6.14 | Tokens per second: 35290.93 | Training tokens per second (%): 12.48 | MFU (%): 33.05 | TFLOPs: 326.84 | Global batch size: 64 | Global tokens/sec: 282327.41 | Global MFU (%): 33.05 | Global TFLOPs: 2614.76 | 
2025-05-19 10:13:23,094 - root - INFO - Step: 380 | Loss: 6.16 | Tokens per second: 36390.98 | Training tokens per second (%): 12.47 | MFU (%): 34.08 | TFLOPs: 337.03 | Global batch size: 64 | Global tokens/sec: 291127.83 | Global MFU (%): 34.08 | Global TFLOPs: 2696.26 | 
2025-05-19 10:13:27,659 - root - INFO - Step: 390 | Loss: 6.05 | Tokens per second: 35893.53 | Training tokens per second (%): 12.47 | MFU (%): 33.61 | TFLOPs: 332.43 | Global batch size: 64 | Global tokens/sec: 287148.21 | Global MFU (%): 33.61 | Global TFLOPs: 2659.40 | 
2025-05-19 10:13:32,183 - root - INFO - Step: 400 | Loss: 6.09 | Tokens per second: 36222.45 | Training tokens per second (%): 12.48 | MFU (%): 33.92 | TFLOPs: 335.47 | Global batch size: 64 | Global tokens/sec: 289779.60 | Global MFU (%): 33.92 | Global TFLOPs: 2683.77 | 
2025-05-19 10:13:36,770 - root - INFO - Step: 410 | Loss: 6.09 | Tokens per second: 35719.21 | Training tokens per second (%): 12.48 | MFU (%): 33.45 | TFLOPs: 330.81 | Global batch size: 64 | Global tokens/sec: 285753.67 | Global MFU (%): 33.45 | Global TFLOPs: 2646.49 | 
2025-05-19 10:13:41,254 - root - INFO - Step: 420 | Loss: 6.10 | Tokens per second: 36548.19 | Training tokens per second (%): 12.47 | MFU (%): 34.23 | TFLOPs: 338.49 | Global batch size: 64 | Global tokens/sec: 292385.49 | Global MFU (%): 34.23 | Global TFLOPs: 2707.91 | 
2025-05-19 10:13:45,811 - root - INFO - Step: 430 | Loss: 6.11 | Tokens per second: 35956.60 | Training tokens per second (%): 12.47 | MFU (%): 33.67 | TFLOPs: 333.01 | Global batch size: 64 | Global tokens/sec: 287652.76 | Global MFU (%): 33.67 | Global TFLOPs: 2664.08 | 
2025-05-19 10:13:50,432 - root - INFO - Step: 440 | Loss: 6.20 | Tokens per second: 35463.55 | Training tokens per second (%): 12.48 | MFU (%): 33.21 | TFLOPs: 328.44 | Global batch size: 64 | Global tokens/sec: 283708.37 | Global MFU (%): 33.21 | Global TFLOPs: 2627.54 | 
2025-05-19 10:13:54,961 - root - INFO - Step: 450 | Loss: 6.11 | Tokens per second: 36179.05 | Training tokens per second (%): 12.48 | MFU (%): 33.88 | TFLOPs: 335.07 | Global batch size: 64 | Global tokens/sec: 289432.44 | Global MFU (%): 33.88 | Global TFLOPs: 2680.56 | 
2025-05-19 10:13:59,487 - root - INFO - Step: 460 | Loss: 6.20 | Tokens per second: 36203.34 | Training tokens per second (%): 12.47 | MFU (%): 33.90 | TFLOPs: 335.29 | Global batch size: 64 | Global tokens/sec: 289626.71 | Global MFU (%): 33.90 | Global TFLOPs: 2682.36 | 
2025-05-19 10:14:04,005 - root - INFO - Step: 470 | Loss: 5.99 | Tokens per second: 36270.41 | Training tokens per second (%): 12.48 | MFU (%): 33.97 | TFLOPs: 335.92 | Global batch size: 64 | Global tokens/sec: 290163.30 | Global MFU (%): 33.97 | Global TFLOPs: 2687.33 | 
2025-05-19 10:14:08,564 - root - INFO - Step: 480 | Loss: 5.97 | Tokens per second: 35943.37 | Training tokens per second (%): 12.47 | MFU (%): 33.66 | TFLOPs: 332.89 | Global batch size: 64 | Global tokens/sec: 287546.93 | Global MFU (%): 33.66 | Global TFLOPs: 2663.10 | 
2025-05-19 10:14:13,110 - root - INFO - Step: 490 | Loss: 5.96 | Tokens per second: 36049.65 | Training tokens per second (%): 12.48 | MFU (%): 33.76 | TFLOPs: 333.87 | Global batch size: 64 | Global tokens/sec: 288397.20 | Global MFU (%): 33.76 | Global TFLOPs: 2670.97 | 
2025-05-19 10:14:17,770 - root - INFO - Step: 500 | Loss: 5.96 | Tokens per second: 35165.66 | Training tokens per second (%): 12.48 | MFU (%): 32.93 | TFLOPs: 325.68 | Global batch size: 64 | Global tokens/sec: 281325.30 | Global MFU (%): 32.93 | Global TFLOPs: 2605.47 | 
2025-05-19 10:14:22,297 - root - INFO - Step: 510 | Loss: 5.94 | Tokens per second: 36196.96 | Training tokens per second (%): 12.47 | MFU (%): 33.90 | TFLOPs: 335.24 | Global batch size: 64 | Global tokens/sec: 289575.69 | Global MFU (%): 33.90 | Global TFLOPs: 2681.88 | 
2025-05-19 10:14:26,798 - root - INFO - Step: 520 | Loss: 5.85 | Tokens per second: 36406.78 | Training tokens per second (%): 12.48 | MFU (%): 34.09 | TFLOPs: 337.18 | Global batch size: 64 | Global tokens/sec: 291254.24 | Global MFU (%): 34.09 | Global TFLOPs: 2697.43 | 
2025-05-19 10:14:31,334 - root - INFO - Step: 530 | Loss: 5.89 | Tokens per second: 36121.94 | Training tokens per second (%): 12.48 | MFU (%): 33.83 | TFLOPs: 334.54 | Global batch size: 64 | Global tokens/sec: 288975.54 | Global MFU (%): 33.83 | Global TFLOPs: 2676.33 | 
2025-05-19 10:14:35,836 - root - INFO - Step: 540 | Loss: 5.89 | Tokens per second: 36400.66 | Training tokens per second (%): 12.48 | MFU (%): 34.09 | TFLOPs: 337.12 | Global batch size: 64 | Global tokens/sec: 291205.30 | Global MFU (%): 34.09 | Global TFLOPs: 2696.98 | 
2025-05-19 10:14:40,360 - root - INFO - Step: 550 | Loss: 5.90 | Tokens per second: 36215.95 | Training tokens per second (%): 12.47 | MFU (%): 33.91 | TFLOPs: 335.41 | Global batch size: 64 | Global tokens/sec: 289727.59 | Global MFU (%): 33.91 | Global TFLOPs: 2683.29 | 
2025-05-19 10:14:44,878 - root - INFO - Step: 560 | Loss: 5.89 | Tokens per second: 36272.17 | Training tokens per second (%): 12.47 | MFU (%): 33.97 | TFLOPs: 335.93 | Global batch size: 64 | Global tokens/sec: 290177.35 | Global MFU (%): 33.97 | Global TFLOPs: 2687.46 | 
2025-05-19 10:14:49,401 - root - INFO - Step: 570 | Loss: 5.81 | Tokens per second: 36229.76 | Training tokens per second (%): 12.48 | MFU (%): 33.93 | TFLOPs: 335.54 | Global batch size: 64 | Global tokens/sec: 289838.09 | Global MFU (%): 33.93 | Global TFLOPs: 2684.31 | 
2025-05-19 10:14:53,925 - root - INFO - Step: 580 | Loss: 5.78 | Tokens per second: 36219.98 | Training tokens per second (%): 12.48 | MFU (%): 33.92 | TFLOPs: 335.45 | Global batch size: 64 | Global tokens/sec: 289759.83 | Global MFU (%): 33.92 | Global TFLOPs: 2683.59 | 
2025-05-19 10:14:58,459 - root - INFO - Step: 590 | Loss: 5.94 | Tokens per second: 36146.37 | Training tokens per second (%): 12.47 | MFU (%): 33.85 | TFLOPs: 334.77 | Global batch size: 64 | Global tokens/sec: 289170.94 | Global MFU (%): 33.85 | Global TFLOPs: 2678.14 | 
2025-05-19 10:15:02,969 - root - INFO - Step: 600 | Loss: 5.82 | Tokens per second: 36330.91 | Training tokens per second (%): 12.47 | MFU (%): 34.02 | TFLOPs: 336.48 | Global batch size: 64 | Global tokens/sec: 290647.28 | Global MFU (%): 34.02 | Global TFLOPs: 2691.81 | 
2025-05-19 10:15:07,496 - root - INFO - Step: 610 | Loss: 5.77 | Tokens per second: 36195.47 | Training tokens per second (%): 12.48 | MFU (%): 33.90 | TFLOPs: 335.22 | Global batch size: 64 | Global tokens/sec: 289563.76 | Global MFU (%): 33.90 | Global TFLOPs: 2681.77 | 
2025-05-19 10:15:12,019 - root - INFO - Step: 620 | Loss: 5.74 | Tokens per second: 36229.71 | Training tokens per second (%): 12.48 | MFU (%): 33.93 | TFLOPs: 335.54 | Global batch size: 64 | Global tokens/sec: 289837.71 | Global MFU (%): 33.93 | Global TFLOPs: 2684.31 | 
2025-05-19 10:15:16,614 - root - INFO - Step: 630 | Loss: 5.84 | Tokens per second: 35660.89 | Training tokens per second (%): 12.47 | MFU (%): 33.39 | TFLOPs: 330.27 | Global batch size: 64 | Global tokens/sec: 285287.12 | Global MFU (%): 33.39 | Global TFLOPs: 2642.17 | 
2025-05-19 10:15:21,220 - root - INFO - Step: 640 | Loss: 5.86 | Tokens per second: 35580.19 | Training tokens per second (%): 12.48 | MFU (%): 33.32 | TFLOPs: 329.52 | Global batch size: 64 | Global tokens/sec: 284641.53 | Global MFU (%): 33.32 | Global TFLOPs: 2636.19 | 
2025-05-19 10:15:25,760 - root - INFO - Step: 650 | Loss: 5.80 | Tokens per second: 36090.47 | Training tokens per second (%): 12.48 | MFU (%): 33.80 | TFLOPs: 334.25 | Global batch size: 64 | Global tokens/sec: 288723.75 | Global MFU (%): 33.80 | Global TFLOPs: 2673.99 | 
2025-05-19 10:15:30,263 - root - INFO - Step: 660 | Loss: 5.81 | Tokens per second: 36394.91 | Training tokens per second (%): 12.48 | MFU (%): 34.08 | TFLOPs: 337.07 | Global batch size: 64 | Global tokens/sec: 291159.25 | Global MFU (%): 34.08 | Global TFLOPs: 2696.55 | 
2025-05-19 10:15:34,855 - root - INFO - Step: 670 | Loss: 5.75 | Tokens per second: 35678.98 | Training tokens per second (%): 12.47 | MFU (%): 33.41 | TFLOPs: 330.44 | Global batch size: 64 | Global tokens/sec: 285431.88 | Global MFU (%): 33.41 | Global TFLOPs: 2643.51 | 
2025-05-19 10:15:39,450 - root - INFO - Step: 680 | Loss: 5.84 | Tokens per second: 35668.11 | Training tokens per second (%): 12.47 | MFU (%): 33.40 | TFLOPs: 330.34 | Global batch size: 64 | Global tokens/sec: 285344.91 | Global MFU (%): 33.40 | Global TFLOPs: 2642.70 | 
2025-05-19 10:15:44,179 - root - INFO - Step: 690 | Loss: 5.71 | Tokens per second: 34646.82 | Training tokens per second (%): 12.48 | MFU (%): 32.44 | TFLOPs: 320.88 | Global batch size: 64 | Global tokens/sec: 277174.59 | Global MFU (%): 32.44 | Global TFLOPs: 2567.03 | 
2025-05-19 10:15:48,687 - root - INFO - Step: 700 | Loss: 5.80 | Tokens per second: 36351.20 | Training tokens per second (%): 12.48 | MFU (%): 34.04 | TFLOPs: 336.66 | Global batch size: 64 | Global tokens/sec: 290809.61 | Global MFU (%): 34.04 | Global TFLOPs: 2693.31 | 
2025-05-19 10:15:53,229 - root - INFO - Step: 710 | Loss: 5.75 | Tokens per second: 36075.61 | Training tokens per second (%): 12.47 | MFU (%): 33.78 | TFLOPs: 334.11 | Global batch size: 64 | Global tokens/sec: 288604.90 | Global MFU (%): 33.78 | Global TFLOPs: 2672.89 | 
2025-05-19 10:15:57,725 - root - INFO - Step: 720 | Loss: 5.66 | Tokens per second: 36451.51 | Training tokens per second (%): 12.48 | MFU (%): 34.13 | TFLOPs: 337.59 | Global batch size: 64 | Global tokens/sec: 291612.06 | Global MFU (%): 34.13 | Global TFLOPs: 2700.74 | 
2025-05-19 10:16:02,354 - root - INFO - Step: 730 | Loss: 5.68 | Tokens per second: 35397.46 | Training tokens per second (%): 12.47 | MFU (%): 33.15 | TFLOPs: 327.83 | Global batch size: 64 | Global tokens/sec: 283179.71 | Global MFU (%): 33.15 | Global TFLOPs: 2622.65 | 
2025-05-19 10:16:06,910 - root - INFO - Step: 740 | Loss: 5.66 | Tokens per second: 35962.96 | Training tokens per second (%): 12.48 | MFU (%): 33.68 | TFLOPs: 333.07 | Global batch size: 64 | Global tokens/sec: 287703.69 | Global MFU (%): 33.68 | Global TFLOPs: 2664.55 | 
2025-05-19 10:16:11,502 - root - INFO - Step: 750 | Loss: 5.63 | Tokens per second: 35692.13 | Training tokens per second (%): 12.48 | MFU (%): 33.42 | TFLOPs: 330.56 | Global batch size: 64 | Global tokens/sec: 285537.05 | Global MFU (%): 33.42 | Global TFLOPs: 2644.48 | 
2025-05-19 10:16:16,099 - root - INFO - Step: 760 | Loss: 5.70 | Tokens per second: 35646.31 | Training tokens per second (%): 12.48 | MFU (%): 33.38 | TFLOPs: 330.14 | Global batch size: 64 | Global tokens/sec: 285170.46 | Global MFU (%): 33.38 | Global TFLOPs: 2641.09 | 
2025-05-19 10:16:20,610 - root - INFO - Step: 770 | Loss: 5.60 | Tokens per second: 36320.70 | Training tokens per second (%): 12.47 | MFU (%): 34.01 | TFLOPs: 336.38 | Global batch size: 64 | Global tokens/sec: 290565.58 | Global MFU (%): 34.01 | Global TFLOPs: 2691.05 | 
2025-05-19 10:16:25,113 - root - INFO - Step: 780 | Loss: 5.67 | Tokens per second: 36394.14 | Training tokens per second (%): 12.47 | MFU (%): 34.08 | TFLOPs: 337.06 | Global batch size: 64 | Global tokens/sec: 291153.16 | Global MFU (%): 34.08 | Global TFLOPs: 2696.49 | 
2025-05-19 10:16:29,614 - root - INFO - Step: 790 | Loss: 5.59 | Tokens per second: 36404.65 | Training tokens per second (%): 12.47 | MFU (%): 34.09 | TFLOPs: 337.16 | Global batch size: 64 | Global tokens/sec: 291237.22 | Global MFU (%): 34.09 | Global TFLOPs: 2697.27 | 
2025-05-19 10:16:34,138 - root - INFO - Step: 800 | Loss: 5.71 | Tokens per second: 36222.05 | Training tokens per second (%): 12.48 | MFU (%): 33.92 | TFLOPs: 335.47 | Global batch size: 64 | Global tokens/sec: 289776.43 | Global MFU (%): 33.92 | Global TFLOPs: 2683.74 | 
2025-05-19 10:16:38,685 - root - INFO - Step: 810 | Loss: 5.56 | Tokens per second: 36036.70 | Training tokens per second (%): 12.48 | MFU (%): 33.75 | TFLOPs: 333.75 | Global batch size: 64 | Global tokens/sec: 288293.58 | Global MFU (%): 33.75 | Global TFLOPs: 2670.01 | 
2025-05-19 10:16:43,205 - root - INFO - Step: 820 | Loss: 5.60 | Tokens per second: 36250.78 | Training tokens per second (%): 12.47 | MFU (%): 33.95 | TFLOPs: 335.73 | Global batch size: 64 | Global tokens/sec: 290006.22 | Global MFU (%): 33.95 | Global TFLOPs: 2685.87 | 
2025-05-19 10:16:47,709 - root - INFO - Step: 830 | Loss: 5.63 | Tokens per second: 36382.08 | Training tokens per second (%): 12.48 | MFU (%): 34.07 | TFLOPs: 336.95 | Global batch size: 64 | Global tokens/sec: 291056.68 | Global MFU (%): 34.07 | Global TFLOPs: 2695.60 | 
2025-05-19 10:16:52,257 - root - INFO - Step: 840 | Loss: 5.78 | Tokens per second: 36036.97 | Training tokens per second (%): 12.48 | MFU (%): 33.75 | TFLOPs: 333.75 | Global batch size: 64 | Global tokens/sec: 288295.79 | Global MFU (%): 33.75 | Global TFLOPs: 2670.03 | 
2025-05-19 10:16:56,833 - root - INFO - Step: 850 | Loss: 5.61 | Tokens per second: 35804.52 | Training tokens per second (%): 12.48 | MFU (%): 33.53 | TFLOPs: 331.60 | Global batch size: 64 | Global tokens/sec: 286436.18 | Global MFU (%): 33.53 | Global TFLOPs: 2652.81 | 
2025-05-19 10:17:01,386 - root - INFO - Step: 860 | Loss: 5.62 | Tokens per second: 35995.79 | Training tokens per second (%): 12.48 | MFU (%): 33.71 | TFLOPs: 333.37 | Global batch size: 64 | Global tokens/sec: 287966.32 | Global MFU (%): 33.71 | Global TFLOPs: 2666.98 | 
2025-05-19 10:17:05,972 - root - INFO - Step: 870 | Loss: 5.50 | Tokens per second: 35726.43 | Training tokens per second (%): 12.47 | MFU (%): 33.46 | TFLOPs: 330.88 | Global batch size: 64 | Global tokens/sec: 285811.42 | Global MFU (%): 33.46 | Global TFLOPs: 2647.02 | 
2025-05-19 10:17:10,484 - root - INFO - Step: 880 | Loss: 5.58 | Tokens per second: 36319.10 | Training tokens per second (%): 12.47 | MFU (%): 34.01 | TFLOPs: 336.37 | Global batch size: 64 | Global tokens/sec: 290552.79 | Global MFU (%): 34.01 | Global TFLOPs: 2690.93 | 
2025-05-19 10:17:15,058 - root - INFO - Step: 890 | Loss: 5.60 | Tokens per second: 35827.68 | Training tokens per second (%): 12.48 | MFU (%): 33.55 | TFLOPs: 331.82 | Global batch size: 64 | Global tokens/sec: 286621.44 | Global MFU (%): 33.55 | Global TFLOPs: 2654.52 | 
2025-05-19 10:17:19,563 - root - INFO - Step: 900 | Loss: 5.46 | Tokens per second: 36368.73 | Training tokens per second (%): 12.47 | MFU (%): 34.06 | TFLOPs: 336.83 | Global batch size: 64 | Global tokens/sec: 290949.82 | Global MFU (%): 34.06 | Global TFLOPs: 2694.61 | 
2025-05-19 10:17:24,133 - root - INFO - Step: 910 | Loss: 5.48 | Tokens per second: 35855.54 | Training tokens per second (%): 12.48 | MFU (%): 33.58 | TFLOPs: 332.07 | Global batch size: 64 | Global tokens/sec: 286844.31 | Global MFU (%): 33.58 | Global TFLOPs: 2656.59 | 
2025-05-19 10:17:28,686 - root - INFO - Step: 920 | Loss: 5.64 | Tokens per second: 35996.18 | Training tokens per second (%): 12.47 | MFU (%): 33.71 | TFLOPs: 333.38 | Global batch size: 64 | Global tokens/sec: 287969.44 | Global MFU (%): 33.71 | Global TFLOPs: 2667.01 | 
2025-05-19 10:17:33,263 - root - INFO - Step: 930 | Loss: 5.48 | Tokens per second: 35799.42 | Training tokens per second (%): 12.47 | MFU (%): 33.52 | TFLOPs: 331.55 | Global batch size: 64 | Global tokens/sec: 286395.39 | Global MFU (%): 33.52 | Global TFLOPs: 2652.43 | 
2025-05-19 10:17:37,910 - root - INFO - Step: 940 | Loss: 5.64 | Tokens per second: 35259.76 | Training tokens per second (%): 12.48 | MFU (%): 33.02 | TFLOPs: 326.56 | Global batch size: 64 | Global tokens/sec: 282078.05 | Global MFU (%): 33.02 | Global TFLOPs: 2612.45 | 
2025-05-19 10:17:42,409 - root - INFO - Step: 950 | Loss: 5.66 | Tokens per second: 36423.75 | Training tokens per second (%): 12.48 | MFU (%): 34.11 | TFLOPs: 337.34 | Global batch size: 64 | Global tokens/sec: 291389.98 | Global MFU (%): 34.11 | Global TFLOPs: 2698.69 | 
2025-05-19 10:17:47,079 - root - INFO - Step: 960 | Loss: 5.61 | Tokens per second: 35089.74 | Training tokens per second (%): 12.49 | MFU (%): 32.86 | TFLOPs: 324.98 | Global batch size: 64 | Global tokens/sec: 280717.95 | Global MFU (%): 32.86 | Global TFLOPs: 2599.85 | 
2025-05-19 10:17:51,619 - root - INFO - Step: 970 | Loss: 5.56 | Tokens per second: 36098.11 | Training tokens per second (%): 12.47 | MFU (%): 33.80 | TFLOPs: 334.32 | Global batch size: 64 | Global tokens/sec: 288784.92 | Global MFU (%): 33.80 | Global TFLOPs: 2674.56 | 
2025-05-19 10:17:56,170 - root - INFO - Step: 980 | Loss: 5.57 | Tokens per second: 36004.11 | Training tokens per second (%): 12.47 | MFU (%): 33.72 | TFLOPs: 333.45 | Global batch size: 64 | Global tokens/sec: 288032.86 | Global MFU (%): 33.72 | Global TFLOPs: 2667.60 | 
2025-05-19 10:18:00,717 - root - INFO - Step: 990 | Loss: 5.50 | Tokens per second: 36036.77 | Training tokens per second (%): 12.48 | MFU (%): 33.75 | TFLOPs: 333.75 | Global batch size: 64 | Global tokens/sec: 288294.13 | Global MFU (%): 33.75 | Global TFLOPs: 2670.02 | 
2025-05-19 10:18:05,248 - root - INFO - Step: 1000 | Loss: 5.58 | Tokens per second: 36166.42 | Training tokens per second (%): 12.48 | MFU (%): 33.87 | TFLOPs: 334.95 | Global batch size: 64 | Global tokens/sec: 289331.37 | Global MFU (%): 33.87 | Global TFLOPs: 2679.62 | 
2025-05-19 10:18:05,248 - root - INFO - Training completed
[sbatch-master] task finished
