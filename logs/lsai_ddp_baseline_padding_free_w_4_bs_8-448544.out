[sbatch-master] running on nid006641
[sbatch-master] SLURM_NODELIST: nid006641
[sbatch-master] SLURM_NNODES: 1
[sbatch-master] SLURM_NODEID: 0
[Master] Master node: nid006641
[Master] World size: 4
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
[srun] rank=0 host=nid006641 noderank=0 localrank=0
W0519 10:01:14.036000 245091 torch/distributed/run.py:792] 
W0519 10:01:14.036000 245091 torch/distributed/run.py:792] *****************************************
W0519 10:01:14.036000 245091 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0519 10:01:14.036000 245091 torch/distributed/run.py:792] *****************************************
2025-05-19 10:01:29,220 - root - INFO - [Distributed Init] Rank 0 initialized on node 0 on GPU 0.
[rank0]:[W519 10:01:29.863812766 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 10:01:29,865 - root - INFO - [Distributed Init] Rank 1 initialized on node 0 on GPU 1.
[rank1]:[W519 10:01:29.952224405 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 10:01:29,945 - root - INFO - [Distributed Init] Rank 3 initialized on node 0 on GPU 3.
[rank3]:[W519 10:01:29.034331188 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 10:01:29,955 - root - INFO - [Distributed Init] Rank 2 initialized on node 0 on GPU 2.
[rank2]:[W519 10:01:29.042172188 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 10:01:38,124 - root - INFO - [Rank 0] All ranks ready!
2025-05-19 10:01:38,125 - root - INFO - Distributed training enabled: 4 processes
2025-05-19 10:01:38,125 - root - INFO - Master process: 0 on cuda:0
2025-05-19 10:01:38,125 - root - INFO - Experiment args: Namespace(dataset='/capstor/scratch/cscs/kasparr/project/train_data.parquet', dataset_type='padding-free', pretokenized=False, tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=10, profile=False, profile_step_start=10, profile_step_end=10, grad_max_norm=1, model_dtype='bf16', compile=False, distributed=True, backend='nccl', find_unused_parameters=False)
2025-05-19 10:01:38,125 - root - INFO - Setting up Tokenizer...
2025-05-19 10:01:38,677 - root - INFO - Setting up DataLoaders...
2025-05-19 10:01:38,677 - root - INFO - Using padding-free IterableParquetDataset with on-the-fly tokenization
DDP sharding: rank 2/4DDP sharding: rank 0/4DDP sharding: rank 1/4


Rank 0/4 processing documents 0 to 196475Rank 2/4 processing documents 392952 to 589427

Rank 1/4 processing documents 196476 to 392951
2025-05-19 10:01:45,135 - root - INFO - Setting up Model...
DDP sharding: rank 3/4
Rank 3/4 processing documents 589428 to 785905
2025-05-19 10:01:56,369 - root - INFO - Model wrapped with DistributedDataParallel
2025-05-19 10:01:56,371 - root - INFO - Global batch size: 32 (local: 8 Ã— 4 processes)
2025-05-19 10:01:56,371 - root - INFO - Starting training!
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
2025-05-19 10:01:57,946 - root - INFO - Step: 1 | Loss: 11.93 | Tokens per second: 10408.58 | Training tokens per second (%): 24.95 | MFU (%): 9.75 | TFLOPs: 96.40 | Global batch size: 32 | Global tokens/sec: 41634.32 | Global MFU (%): 9.75 | Global TFLOPs: 385.59 | 
2025-05-19 10:02:01,928 - root - INFO - Step: 10 | Loss: 11.91 | Tokens per second: 37038.19 | Training tokens per second (%): 24.96 | MFU (%): 34.68 | TFLOPs: 343.03 | Global batch size: 32 | Global tokens/sec: 148152.77 | Global MFU (%): 34.68 | Global TFLOPs: 1372.11 | 
2025-05-19 10:02:06,241 - root - INFO - Step: 20 | Loss: 11.70 | Tokens per second: 37996.37 | Training tokens per second (%): 24.95 | MFU (%): 35.58 | TFLOPs: 351.90 | Global batch size: 32 | Global tokens/sec: 151985.49 | Global MFU (%): 35.58 | Global TFLOPs: 1407.60 | 
2025-05-19 10:02:10,549 - root - INFO - Step: 30 | Loss: 11.13 | Tokens per second: 38029.37 | Training tokens per second (%): 24.95 | MFU (%): 35.61 | TFLOPs: 352.21 | Global batch size: 32 | Global tokens/sec: 152117.50 | Global MFU (%): 35.61 | Global TFLOPs: 1408.83 | 
2025-05-19 10:02:14,853 - root - INFO - Step: 40 | Loss: 10.04 | Tokens per second: 38081.34 | Training tokens per second (%): 24.95 | MFU (%): 35.66 | TFLOPs: 352.69 | Global batch size: 32 | Global tokens/sec: 152325.36 | Global MFU (%): 35.66 | Global TFLOPs: 1410.75 | 
2025-05-19 10:02:19,126 - root - INFO - Step: 50 | Loss: 9.39 | Tokens per second: 38347.27 | Training tokens per second (%): 24.95 | MFU (%): 35.91 | TFLOPs: 355.15 | Global batch size: 32 | Global tokens/sec: 153389.06 | Global MFU (%): 35.91 | Global TFLOPs: 1420.60 | 
2025-05-19 10:02:23,407 - root - INFO - Step: 60 | Loss: 8.92 | Tokens per second: 38273.97 | Training tokens per second (%): 24.95 | MFU (%): 35.84 | TFLOPs: 354.47 | Global batch size: 32 | Global tokens/sec: 153095.88 | Global MFU (%): 35.84 | Global TFLOPs: 1417.89 | 
2025-05-19 10:02:27,726 - root - INFO - Step: 70 | Loss: 8.32 | Tokens per second: 37942.92 | Training tokens per second (%): 24.95 | MFU (%): 35.53 | TFLOPs: 351.41 | Global batch size: 32 | Global tokens/sec: 151771.68 | Global MFU (%): 35.53 | Global TFLOPs: 1405.62 | 
2025-05-19 10:02:32,062 - root - INFO - Step: 80 | Loss: 7.94 | Tokens per second: 37791.03 | Training tokens per second (%): 24.95 | MFU (%): 35.39 | TFLOPs: 350.00 | Global batch size: 32 | Global tokens/sec: 151164.11 | Global MFU (%): 35.39 | Global TFLOPs: 1400.00 | 
2025-05-19 10:02:36,363 - root - INFO - Step: 90 | Loss: 7.67 | Tokens per second: 38103.33 | Training tokens per second (%): 24.95 | MFU (%): 35.68 | TFLOPs: 352.89 | Global batch size: 32 | Global tokens/sec: 152413.31 | Global MFU (%): 35.68 | Global TFLOPs: 1411.57 | 
2025-05-19 10:02:40,752 - root - INFO - Step: 100 | Loss: 7.39 | Tokens per second: 37331.13 | Training tokens per second (%): 24.96 | MFU (%): 34.96 | TFLOPs: 345.74 | Global batch size: 32 | Global tokens/sec: 149324.51 | Global MFU (%): 34.96 | Global TFLOPs: 1382.96 | 
2025-05-19 10:02:45,030 - root - INFO - Step: 110 | Loss: 7.33 | Tokens per second: 38308.20 | Training tokens per second (%): 24.95 | MFU (%): 35.87 | TFLOPs: 354.79 | Global batch size: 32 | Global tokens/sec: 153232.80 | Global MFU (%): 35.87 | Global TFLOPs: 1419.15 | 
2025-05-19 10:02:49,363 - root - INFO - Step: 120 | Loss: 7.30 | Tokens per second: 37817.08 | Training tokens per second (%): 24.96 | MFU (%): 35.41 | TFLOPs: 350.24 | Global batch size: 32 | Global tokens/sec: 151268.32 | Global MFU (%): 35.41 | Global TFLOPs: 1400.96 | 
2025-05-19 10:02:53,657 - root - INFO - Step: 130 | Loss: 7.22 | Tokens per second: 38164.76 | Training tokens per second (%): 24.95 | MFU (%): 35.74 | TFLOPs: 353.46 | Global batch size: 32 | Global tokens/sec: 152659.02 | Global MFU (%): 35.74 | Global TFLOPs: 1413.84 | 
2025-05-19 10:02:58,152 - root - INFO - Step: 140 | Loss: 7.11 | Tokens per second: 36453.49 | Training tokens per second (%): 24.99 | MFU (%): 34.14 | TFLOPs: 337.61 | Global batch size: 32 | Global tokens/sec: 145813.97 | Global MFU (%): 34.14 | Global TFLOPs: 1350.45 | 
2025-05-19 10:03:02,455 - root - INFO - Step: 150 | Loss: 7.08 | Tokens per second: 38084.87 | Training tokens per second (%): 24.94 | MFU (%): 35.66 | TFLOPs: 352.72 | Global batch size: 32 | Global tokens/sec: 152339.47 | Global MFU (%): 35.66 | Global TFLOPs: 1410.88 | 
2025-05-19 10:03:06,730 - root - INFO - Step: 160 | Loss: 6.94 | Tokens per second: 38330.33 | Training tokens per second (%): 24.95 | MFU (%): 35.89 | TFLOPs: 354.99 | Global batch size: 32 | Global tokens/sec: 153321.33 | Global MFU (%): 35.89 | Global TFLOPs: 1419.97 | 
2025-05-19 10:03:11,076 - root - INFO - Step: 170 | Loss: 7.07 | Tokens per second: 37703.37 | Training tokens per second (%): 24.94 | MFU (%): 35.31 | TFLOPs: 349.19 | Global batch size: 32 | Global tokens/sec: 150813.47 | Global MFU (%): 35.31 | Global TFLOPs: 1396.75 | 
2025-05-19 10:03:15,385 - root - INFO - Step: 180 | Loss: 6.84 | Tokens per second: 38023.90 | Training tokens per second (%): 24.96 | MFU (%): 35.61 | TFLOPs: 352.16 | Global batch size: 32 | Global tokens/sec: 152095.60 | Global MFU (%): 35.61 | Global TFLOPs: 1408.62 | 
2025-05-19 10:03:19,724 - root - INFO - Step: 190 | Loss: 6.95 | Tokens per second: 37766.18 | Training tokens per second (%): 24.95 | MFU (%): 35.37 | TFLOPs: 349.77 | Global batch size: 32 | Global tokens/sec: 151064.73 | Global MFU (%): 35.37 | Global TFLOPs: 1399.08 | 
2025-05-19 10:03:24,071 - root - INFO - Step: 200 | Loss: 6.89 | Tokens per second: 37696.78 | Training tokens per second (%): 24.95 | MFU (%): 35.30 | TFLOPs: 349.13 | Global batch size: 32 | Global tokens/sec: 150787.12 | Global MFU (%): 35.30 | Global TFLOPs: 1396.50 | 
2025-05-19 10:03:28,377 - root - INFO - Step: 210 | Loss: 6.73 | Tokens per second: 38060.85 | Training tokens per second (%): 24.94 | MFU (%): 35.64 | TFLOPs: 352.50 | Global batch size: 32 | Global tokens/sec: 152243.39 | Global MFU (%): 35.64 | Global TFLOPs: 1409.99 | 
2025-05-19 10:03:32,662 - root - INFO - Step: 220 | Loss: 6.82 | Tokens per second: 38237.77 | Training tokens per second (%): 24.94 | MFU (%): 35.81 | TFLOPs: 354.14 | Global batch size: 32 | Global tokens/sec: 152951.07 | Global MFU (%): 35.81 | Global TFLOPs: 1416.55 | 
2025-05-19 10:03:37,100 - root - INFO - Step: 230 | Loss: 6.66 | Tokens per second: 36927.17 | Training tokens per second (%): 24.97 | MFU (%): 34.58 | TFLOPs: 342.00 | Global batch size: 32 | Global tokens/sec: 147708.67 | Global MFU (%): 34.58 | Global TFLOPs: 1367.99 | 
2025-05-19 10:03:41,461 - root - INFO - Step: 240 | Loss: 6.61 | Tokens per second: 37570.34 | Training tokens per second (%): 24.94 | MFU (%): 35.18 | TFLOPs: 347.96 | Global batch size: 32 | Global tokens/sec: 150281.36 | Global MFU (%): 35.18 | Global TFLOPs: 1391.82 | 
2025-05-19 10:03:45,812 - root - INFO - Step: 250 | Loss: 6.71 | Tokens per second: 37663.94 | Training tokens per second (%): 24.95 | MFU (%): 35.27 | TFLOPs: 348.82 | Global batch size: 32 | Global tokens/sec: 150655.76 | Global MFU (%): 35.27 | Global TFLOPs: 1395.29 | 
2025-05-19 10:03:50,117 - root - INFO - Step: 260 | Loss: 6.51 | Tokens per second: 38064.06 | Training tokens per second (%): 24.95 | MFU (%): 35.64 | TFLOPs: 352.53 | Global batch size: 32 | Global tokens/sec: 152256.24 | Global MFU (%): 35.64 | Global TFLOPs: 1410.11 | 
2025-05-19 10:03:54,573 - root - INFO - Step: 270 | Loss: 6.59 | Tokens per second: 36773.08 | Training tokens per second (%): 24.96 | MFU (%): 34.44 | TFLOPs: 340.57 | Global batch size: 32 | Global tokens/sec: 147092.32 | Global MFU (%): 34.44 | Global TFLOPs: 1362.29 | 
2025-05-19 10:03:58,865 - root - INFO - Step: 280 | Loss: 6.51 | Tokens per second: 38175.70 | Training tokens per second (%): 24.95 | MFU (%): 35.75 | TFLOPs: 353.56 | Global batch size: 32 | Global tokens/sec: 152702.81 | Global MFU (%): 35.75 | Global TFLOPs: 1414.25 | 
2025-05-19 10:04:03,165 - root - INFO - Step: 290 | Loss: 6.56 | Tokens per second: 38105.72 | Training tokens per second (%): 24.94 | MFU (%): 35.68 | TFLOPs: 352.91 | Global batch size: 32 | Global tokens/sec: 152422.90 | Global MFU (%): 35.68 | Global TFLOPs: 1411.65 | 
2025-05-19 10:04:07,473 - root - INFO - Step: 300 | Loss: 6.57 | Tokens per second: 38039.74 | Training tokens per second (%): 24.96 | MFU (%): 35.62 | TFLOPs: 352.30 | Global batch size: 32 | Global tokens/sec: 152158.97 | Global MFU (%): 35.62 | Global TFLOPs: 1409.21 | 
2025-05-19 10:04:11,822 - root - INFO - Step: 310 | Loss: 6.38 | Tokens per second: 37683.66 | Training tokens per second (%): 24.95 | MFU (%): 35.29 | TFLOPs: 349.00 | Global batch size: 32 | Global tokens/sec: 150734.62 | Global MFU (%): 35.29 | Global TFLOPs: 1396.02 | 
2025-05-19 10:04:16,180 - root - INFO - Step: 320 | Loss: 6.30 | Tokens per second: 37600.22 | Training tokens per second (%): 24.96 | MFU (%): 35.21 | TFLOPs: 348.23 | Global batch size: 32 | Global tokens/sec: 150400.89 | Global MFU (%): 35.21 | Global TFLOPs: 1392.93 | 
2025-05-19 10:04:20,520 - root - INFO - Step: 330 | Loss: 6.40 | Tokens per second: 37752.53 | Training tokens per second (%): 24.94 | MFU (%): 35.35 | TFLOPs: 349.64 | Global batch size: 32 | Global tokens/sec: 151010.13 | Global MFU (%): 35.35 | Global TFLOPs: 1398.57 | 
2025-05-19 10:04:24,864 - root - INFO - Step: 340 | Loss: 6.37 | Tokens per second: 37727.17 | Training tokens per second (%): 24.95 | MFU (%): 35.33 | TFLOPs: 349.41 | Global batch size: 32 | Global tokens/sec: 150908.69 | Global MFU (%): 35.33 | Global TFLOPs: 1397.63 | 
2025-05-19 10:04:29,188 - root - INFO - Step: 350 | Loss: 6.42 | Tokens per second: 37897.13 | Training tokens per second (%): 24.95 | MFU (%): 35.49 | TFLOPs: 350.98 | Global batch size: 32 | Global tokens/sec: 151588.53 | Global MFU (%): 35.49 | Global TFLOPs: 1403.93 | 
2025-05-19 10:04:33,494 - root - INFO - Step: 360 | Loss: 6.34 | Tokens per second: 38053.54 | Training tokens per second (%): 24.96 | MFU (%): 35.63 | TFLOPs: 352.43 | Global batch size: 32 | Global tokens/sec: 152214.14 | Global MFU (%): 35.63 | Global TFLOPs: 1409.72 | 
2025-05-19 10:04:37,886 - root - INFO - Step: 370 | Loss: 6.31 | Tokens per second: 37304.69 | Training tokens per second (%): 24.97 | MFU (%): 34.93 | TFLOPs: 345.49 | Global batch size: 32 | Global tokens/sec: 149218.77 | Global MFU (%): 34.93 | Global TFLOPs: 1381.98 | 
2025-05-19 10:04:42,180 - root - INFO - Step: 380 | Loss: 6.31 | Tokens per second: 38168.41 | Training tokens per second (%): 24.94 | MFU (%): 35.74 | TFLOPs: 353.49 | Global batch size: 32 | Global tokens/sec: 152673.65 | Global MFU (%): 35.74 | Global TFLOPs: 1413.98 | 
2025-05-19 10:04:46,562 - root - INFO - Step: 390 | Loss: 6.01 | Tokens per second: 37394.60 | Training tokens per second (%): 24.95 | MFU (%): 35.02 | TFLOPs: 346.33 | Global batch size: 32 | Global tokens/sec: 149578.40 | Global MFU (%): 35.02 | Global TFLOPs: 1385.31 | 
2025-05-19 10:04:50,883 - root - INFO - Step: 400 | Loss: 6.28 | Tokens per second: 37916.70 | Training tokens per second (%): 24.96 | MFU (%): 35.51 | TFLOPs: 351.16 | Global batch size: 32 | Global tokens/sec: 151666.82 | Global MFU (%): 35.51 | Global TFLOPs: 1404.65 | 
2025-05-19 10:04:55,274 - root - INFO - Step: 410 | Loss: 6.27 | Tokens per second: 37316.53 | Training tokens per second (%): 24.95 | MFU (%): 34.94 | TFLOPs: 345.60 | Global batch size: 32 | Global tokens/sec: 149266.11 | Global MFU (%): 34.94 | Global TFLOPs: 1382.42 | 
2025-05-19 10:04:59,577 - root - INFO - Step: 420 | Loss: 6.34 | Tokens per second: 38083.35 | Training tokens per second (%): 24.95 | MFU (%): 35.66 | TFLOPs: 352.71 | Global batch size: 32 | Global tokens/sec: 152333.41 | Global MFU (%): 35.66 | Global TFLOPs: 1410.82 | 
2025-05-19 10:05:03,905 - root - INFO - Step: 430 | Loss: 6.23 | Tokens per second: 37865.29 | Training tokens per second (%): 24.94 | MFU (%): 35.46 | TFLOPs: 350.69 | Global batch size: 32 | Global tokens/sec: 151461.15 | Global MFU (%): 35.46 | Global TFLOPs: 1402.75 | 
2025-05-19 10:05:08,281 - root - INFO - Step: 440 | Loss: 6.46 | Tokens per second: 37441.36 | Training tokens per second (%): 24.96 | MFU (%): 35.06 | TFLOPs: 346.76 | Global batch size: 32 | Global tokens/sec: 149765.46 | Global MFU (%): 35.06 | Global TFLOPs: 1387.04 | 
2025-05-19 10:05:12,601 - root - INFO - Step: 450 | Loss: 6.26 | Tokens per second: 37935.60 | Training tokens per second (%): 24.96 | MFU (%): 35.52 | TFLOPs: 351.34 | Global batch size: 32 | Global tokens/sec: 151742.38 | Global MFU (%): 35.52 | Global TFLOPs: 1405.35 | 
2025-05-19 10:05:16,918 - root - INFO - Step: 460 | Loss: 6.31 | Tokens per second: 37959.29 | Training tokens per second (%): 24.95 | MFU (%): 35.55 | TFLOPs: 351.56 | Global batch size: 32 | Global tokens/sec: 151837.18 | Global MFU (%): 35.55 | Global TFLOPs: 1406.23 | 
2025-05-19 10:05:21,218 - root - INFO - Step: 470 | Loss: 6.15 | Tokens per second: 38108.11 | Training tokens per second (%): 24.96 | MFU (%): 35.69 | TFLOPs: 352.94 | Global batch size: 32 | Global tokens/sec: 152432.46 | Global MFU (%): 35.69 | Global TFLOPs: 1411.74 | 
2025-05-19 10:05:25,529 - root - INFO - Step: 480 | Loss: 6.18 | Tokens per second: 38011.17 | Training tokens per second (%): 24.95 | MFU (%): 35.60 | TFLOPs: 352.04 | Global batch size: 32 | Global tokens/sec: 152044.67 | Global MFU (%): 35.60 | Global TFLOPs: 1408.15 | 
2025-05-19 10:05:29,841 - root - INFO - Step: 490 | Loss: 6.07 | Tokens per second: 38004.89 | Training tokens per second (%): 24.95 | MFU (%): 35.59 | TFLOPs: 351.98 | Global batch size: 32 | Global tokens/sec: 152019.55 | Global MFU (%): 35.59 | Global TFLOPs: 1407.92 | 
2025-05-19 10:05:34,272 - root - INFO - Step: 500 | Loss: 6.12 | Tokens per second: 36980.97 | Training tokens per second (%): 24.95 | MFU (%): 34.63 | TFLOPs: 342.50 | Global batch size: 32 | Global tokens/sec: 147923.87 | Global MFU (%): 34.63 | Global TFLOPs: 1369.99 | 
2025-05-19 10:05:38,563 - root - INFO - Step: 510 | Loss: 6.18 | Tokens per second: 38187.67 | Training tokens per second (%): 24.95 | MFU (%): 35.76 | TFLOPs: 353.67 | Global batch size: 32 | Global tokens/sec: 152750.68 | Global MFU (%): 35.76 | Global TFLOPs: 1414.69 | 
2025-05-19 10:05:42,858 - root - INFO - Step: 520 | Loss: 6.00 | Tokens per second: 38149.09 | Training tokens per second (%): 24.96 | MFU (%): 35.72 | TFLOPs: 353.32 | Global batch size: 32 | Global tokens/sec: 152596.38 | Global MFU (%): 35.72 | Global TFLOPs: 1413.26 | 
2025-05-19 10:05:47,185 - root - INFO - Step: 530 | Loss: 6.01 | Tokens per second: 37870.89 | Training tokens per second (%): 24.95 | MFU (%): 35.46 | TFLOPs: 350.74 | Global batch size: 32 | Global tokens/sec: 151483.57 | Global MFU (%): 35.46 | Global TFLOPs: 1402.95 | 
2025-05-19 10:05:51,464 - root - INFO - Step: 540 | Loss: 6.03 | Tokens per second: 38296.93 | Training tokens per second (%): 24.97 | MFU (%): 35.86 | TFLOPs: 354.68 | Global batch size: 32 | Global tokens/sec: 153187.73 | Global MFU (%): 35.86 | Global TFLOPs: 1418.74 | 
2025-05-19 10:05:55,760 - root - INFO - Step: 550 | Loss: 6.11 | Tokens per second: 38139.87 | Training tokens per second (%): 24.95 | MFU (%): 35.72 | TFLOPs: 353.23 | Global batch size: 32 | Global tokens/sec: 152559.47 | Global MFU (%): 35.72 | Global TFLOPs: 1412.92 | 
2025-05-19 10:06:00,060 - root - INFO - Step: 560 | Loss: 6.05 | Tokens per second: 38112.35 | Training tokens per second (%): 24.95 | MFU (%): 35.69 | TFLOPs: 352.97 | Global batch size: 32 | Global tokens/sec: 152449.40 | Global MFU (%): 35.69 | Global TFLOPs: 1411.90 | 
2025-05-19 10:06:04,393 - root - INFO - Step: 570 | Loss: 5.95 | Tokens per second: 37818.58 | Training tokens per second (%): 24.97 | MFU (%): 35.41 | TFLOPs: 350.25 | Global batch size: 32 | Global tokens/sec: 151274.33 | Global MFU (%): 35.41 | Global TFLOPs: 1401.02 | 
2025-05-19 10:06:08,695 - root - INFO - Step: 580 | Loss: 5.95 | Tokens per second: 38086.57 | Training tokens per second (%): 24.96 | MFU (%): 35.67 | TFLOPs: 352.74 | Global batch size: 32 | Global tokens/sec: 152346.29 | Global MFU (%): 35.67 | Global TFLOPs: 1410.94 | 
2025-05-19 10:06:13,048 - root - INFO - Step: 590 | Loss: 6.10 | Tokens per second: 37650.36 | Training tokens per second (%): 24.94 | MFU (%): 35.26 | TFLOPs: 348.70 | Global batch size: 32 | Global tokens/sec: 150601.44 | Global MFU (%): 35.26 | Global TFLOPs: 1394.78 | 
2025-05-19 10:06:17,333 - root - INFO - Step: 600 | Loss: 6.00 | Tokens per second: 38240.44 | Training tokens per second (%): 24.95 | MFU (%): 35.81 | TFLOPs: 354.16 | Global batch size: 32 | Global tokens/sec: 152961.76 | Global MFU (%): 35.81 | Global TFLOPs: 1416.64 | 
2025-05-19 10:06:21,637 - root - INFO - Step: 610 | Loss: 5.78 | Tokens per second: 38068.41 | Training tokens per second (%): 24.95 | MFU (%): 35.65 | TFLOPs: 352.57 | Global batch size: 32 | Global tokens/sec: 152273.64 | Global MFU (%): 35.65 | Global TFLOPs: 1410.27 | 
2025-05-19 10:06:25,955 - root - INFO - Step: 620 | Loss: 6.02 | Tokens per second: 37951.83 | Training tokens per second (%): 24.96 | MFU (%): 35.54 | TFLOPs: 351.49 | Global batch size: 32 | Global tokens/sec: 151807.32 | Global MFU (%): 35.54 | Global TFLOPs: 1405.95 | 
2025-05-19 10:06:30,265 - root - INFO - Step: 630 | Loss: 5.95 | Tokens per second: 38018.64 | Training tokens per second (%): 24.94 | MFU (%): 35.60 | TFLOPs: 352.11 | Global batch size: 32 | Global tokens/sec: 152074.56 | Global MFU (%): 35.60 | Global TFLOPs: 1408.43 | 
2025-05-19 10:06:34,637 - root - INFO - Step: 640 | Loss: 6.09 | Tokens per second: 37481.21 | Training tokens per second (%): 24.96 | MFU (%): 35.10 | TFLOPs: 347.13 | Global batch size: 32 | Global tokens/sec: 149924.84 | Global MFU (%): 35.10 | Global TFLOPs: 1388.52 | 
2025-05-19 10:06:38,944 - root - INFO - Step: 650 | Loss: 5.85 | Tokens per second: 38044.03 | Training tokens per second (%): 24.95 | MFU (%): 35.63 | TFLOPs: 352.34 | Global batch size: 32 | Global tokens/sec: 152176.11 | Global MFU (%): 35.63 | Global TFLOPs: 1409.37 | 
2025-05-19 10:06:43,244 - root - INFO - Step: 660 | Loss: 5.94 | Tokens per second: 38111.31 | Training tokens per second (%): 24.96 | MFU (%): 35.69 | TFLOPs: 352.97 | Global batch size: 32 | Global tokens/sec: 152445.23 | Global MFU (%): 35.69 | Global TFLOPs: 1411.86 | 
2025-05-19 10:06:47,580 - root - INFO - Step: 670 | Loss: 5.89 | Tokens per second: 37795.19 | Training tokens per second (%): 24.95 | MFU (%): 35.39 | TFLOPs: 350.04 | Global batch size: 32 | Global tokens/sec: 151180.76 | Global MFU (%): 35.39 | Global TFLOPs: 1400.15 | 
2025-05-19 10:06:51,873 - root - INFO - Step: 680 | Loss: 6.02 | Tokens per second: 38172.64 | Training tokens per second (%): 24.95 | MFU (%): 35.75 | TFLOPs: 353.53 | Global batch size: 32 | Global tokens/sec: 152690.55 | Global MFU (%): 35.75 | Global TFLOPs: 1414.13 | 
2025-05-19 10:06:56,241 - root - INFO - Step: 690 | Loss: 5.86 | Tokens per second: 37513.03 | Training tokens per second (%): 24.96 | MFU (%): 35.13 | TFLOPs: 347.42 | Global batch size: 32 | Global tokens/sec: 150052.11 | Global MFU (%): 35.13 | Global TFLOPs: 1389.70 | 
2025-05-19 10:07:00,531 - root - INFO - Step: 700 | Loss: 5.94 | Tokens per second: 38193.59 | Training tokens per second (%): 24.96 | MFU (%): 35.77 | TFLOPs: 353.73 | Global batch size: 32 | Global tokens/sec: 152774.34 | Global MFU (%): 35.77 | Global TFLOPs: 1414.91 | 
2025-05-19 10:07:04,843 - root - INFO - Step: 710 | Loss: 5.92 | Tokens per second: 38001.92 | Training tokens per second (%): 24.95 | MFU (%): 35.59 | TFLOPs: 351.95 | Global batch size: 32 | Global tokens/sec: 152007.67 | Global MFU (%): 35.59 | Global TFLOPs: 1407.81 | 
2025-05-19 10:07:09,130 - root - INFO - Step: 720 | Loss: 5.82 | Tokens per second: 38223.30 | Training tokens per second (%): 24.96 | MFU (%): 35.79 | TFLOPs: 354.00 | Global batch size: 32 | Global tokens/sec: 152893.19 | Global MFU (%): 35.79 | Global TFLOPs: 1416.01 | 
2025-05-19 10:07:13,498 - root - INFO - Step: 730 | Loss: 5.91 | Tokens per second: 37522.25 | Training tokens per second (%): 24.95 | MFU (%): 35.14 | TFLOPs: 347.51 | Global batch size: 32 | Global tokens/sec: 150089.00 | Global MFU (%): 35.14 | Global TFLOPs: 1390.04 | 
2025-05-19 10:07:17,852 - root - INFO - Step: 740 | Loss: 5.79 | Tokens per second: 37630.56 | Training tokens per second (%): 24.96 | MFU (%): 35.24 | TFLOPs: 348.51 | Global batch size: 32 | Global tokens/sec: 150522.24 | Global MFU (%): 35.24 | Global TFLOPs: 1394.05 | 
2025-05-19 10:07:22,226 - root - INFO - Step: 750 | Loss: 5.81 | Tokens per second: 37462.02 | Training tokens per second (%): 24.96 | MFU (%): 35.08 | TFLOPs: 346.95 | Global batch size: 32 | Global tokens/sec: 149848.06 | Global MFU (%): 35.08 | Global TFLOPs: 1387.81 | 
2025-05-19 10:07:26,549 - root - INFO - Step: 760 | Loss: 5.88 | Tokens per second: 37908.63 | Training tokens per second (%): 24.97 | MFU (%): 35.50 | TFLOPs: 351.09 | Global batch size: 32 | Global tokens/sec: 151634.50 | Global MFU (%): 35.50 | Global TFLOPs: 1404.35 | 
2025-05-19 10:07:30,849 - root - INFO - Step: 770 | Loss: 5.81 | Tokens per second: 38107.42 | Training tokens per second (%): 24.95 | MFU (%): 35.69 | TFLOPs: 352.93 | Global batch size: 32 | Global tokens/sec: 152429.67 | Global MFU (%): 35.69 | Global TFLOPs: 1411.72 | 
2025-05-19 10:07:35,160 - root - INFO - Step: 780 | Loss: 5.78 | Tokens per second: 38013.77 | Training tokens per second (%): 24.95 | MFU (%): 35.60 | TFLOPs: 352.06 | Global batch size: 32 | Global tokens/sec: 152055.10 | Global MFU (%): 35.60 | Global TFLOPs: 1408.25 | 
2025-05-19 10:07:39,455 - root - INFO - Step: 790 | Loss: 5.65 | Tokens per second: 38151.15 | Training tokens per second (%): 24.95 | MFU (%): 35.73 | TFLOPs: 353.33 | Global batch size: 32 | Global tokens/sec: 152604.60 | Global MFU (%): 35.73 | Global TFLOPs: 1413.34 | 
2025-05-19 10:07:43,754 - root - INFO - Step: 800 | Loss: 5.91 | Tokens per second: 38116.23 | Training tokens per second (%): 24.96 | MFU (%): 35.69 | TFLOPs: 353.01 | Global batch size: 32 | Global tokens/sec: 152464.93 | Global MFU (%): 35.69 | Global TFLOPs: 1412.04 | 
2025-05-19 10:07:48,094 - root - INFO - Step: 810 | Loss: 5.78 | Tokens per second: 37756.37 | Training tokens per second (%): 24.96 | MFU (%): 35.36 | TFLOPs: 349.68 | Global batch size: 32 | Global tokens/sec: 151025.49 | Global MFU (%): 35.36 | Global TFLOPs: 1398.71 | 
2025-05-19 10:07:52,405 - root - INFO - Step: 820 | Loss: 5.76 | Tokens per second: 38010.79 | Training tokens per second (%): 24.95 | MFU (%): 35.59 | TFLOPs: 352.03 | Global batch size: 32 | Global tokens/sec: 152043.16 | Global MFU (%): 35.59 | Global TFLOPs: 1408.14 | 
2025-05-19 10:07:56,699 - root - INFO - Step: 830 | Loss: 5.80 | Tokens per second: 38169.24 | Training tokens per second (%): 24.96 | MFU (%): 35.74 | TFLOPs: 353.50 | Global batch size: 32 | Global tokens/sec: 152676.95 | Global MFU (%): 35.74 | Global TFLOPs: 1414.01 | 
2025-05-19 10:08:01,010 - root - INFO - Step: 840 | Loss: 5.86 | Tokens per second: 38006.21 | Training tokens per second (%): 24.96 | MFU (%): 35.59 | TFLOPs: 351.99 | Global batch size: 32 | Global tokens/sec: 152024.82 | Global MFU (%): 35.59 | Global TFLOPs: 1407.97 | 
2025-05-19 10:08:05,358 - root - INFO - Step: 850 | Loss: 5.77 | Tokens per second: 37689.54 | Training tokens per second (%): 24.96 | MFU (%): 35.29 | TFLOPs: 349.06 | Global batch size: 32 | Global tokens/sec: 150758.17 | Global MFU (%): 35.29 | Global TFLOPs: 1396.24 | 
2025-05-19 10:08:09,706 - root - INFO - Step: 860 | Loss: 5.82 | Tokens per second: 37688.22 | Training tokens per second (%): 24.96 | MFU (%): 35.29 | TFLOPs: 349.05 | Global batch size: 32 | Global tokens/sec: 150752.86 | Global MFU (%): 35.29 | Global TFLOPs: 1396.19 | 
2025-05-19 10:08:14,026 - root - INFO - Step: 870 | Loss: 5.68 | Tokens per second: 37928.67 | Training tokens per second (%): 24.95 | MFU (%): 35.52 | TFLOPs: 351.27 | Global batch size: 32 | Global tokens/sec: 151714.67 | Global MFU (%): 35.52 | Global TFLOPs: 1405.09 | 
2025-05-19 10:08:18,349 - root - INFO - Step: 880 | Loss: 5.77 | Tokens per second: 37906.10 | Training tokens per second (%): 24.94 | MFU (%): 35.50 | TFLOPs: 351.06 | Global batch size: 32 | Global tokens/sec: 151624.38 | Global MFU (%): 35.50 | Global TFLOPs: 1404.26 | 
2025-05-19 10:08:22,658 - root - INFO - Step: 890 | Loss: 5.72 | Tokens per second: 38030.04 | Training tokens per second (%): 24.95 | MFU (%): 35.61 | TFLOPs: 352.21 | Global batch size: 32 | Global tokens/sec: 152120.16 | Global MFU (%): 35.61 | Global TFLOPs: 1408.85 | 
2025-05-19 10:08:26,959 - root - INFO - Step: 900 | Loss: 5.62 | Tokens per second: 38103.64 | Training tokens per second (%): 24.94 | MFU (%): 35.68 | TFLOPs: 352.89 | Global batch size: 32 | Global tokens/sec: 152414.56 | Global MFU (%): 35.68 | Global TFLOPs: 1411.58 | 
2025-05-19 10:08:31,265 - root - INFO - Step: 910 | Loss: 5.64 | Tokens per second: 38050.47 | Training tokens per second (%): 24.95 | MFU (%): 35.63 | TFLOPs: 352.40 | Global batch size: 32 | Global tokens/sec: 152201.89 | Global MFU (%): 35.63 | Global TFLOPs: 1409.61 | 
2025-05-19 10:08:35,623 - root - INFO - Step: 920 | Loss: 5.82 | Tokens per second: 37602.10 | Training tokens per second (%): 24.94 | MFU (%): 35.21 | TFLOPs: 348.25 | Global batch size: 32 | Global tokens/sec: 150408.42 | Global MFU (%): 35.21 | Global TFLOPs: 1393.00 | 
2025-05-19 10:08:39,957 - root - INFO - Step: 930 | Loss: 5.61 | Tokens per second: 37807.98 | Training tokens per second (%): 24.94 | MFU (%): 35.41 | TFLOPs: 350.16 | Global batch size: 32 | Global tokens/sec: 151231.93 | Global MFU (%): 35.41 | Global TFLOPs: 1400.62 | 
2025-05-19 10:08:44,265 - root - INFO - Step: 940 | Loss: 5.70 | Tokens per second: 38037.97 | Training tokens per second (%): 24.96 | MFU (%): 35.62 | TFLOPs: 352.29 | Global batch size: 32 | Global tokens/sec: 152151.90 | Global MFU (%): 35.62 | Global TFLOPs: 1409.14 | 
2025-05-19 10:08:48,573 - root - INFO - Step: 950 | Loss: 5.75 | Tokens per second: 38036.40 | Training tokens per second (%): 24.96 | MFU (%): 35.62 | TFLOPs: 352.27 | Global batch size: 32 | Global tokens/sec: 152145.59 | Global MFU (%): 35.62 | Global TFLOPs: 1409.09 | 
2025-05-19 10:08:53,047 - root - INFO - Step: 960 | Loss: 5.77 | Tokens per second: 36628.41 | Training tokens per second (%): 24.98 | MFU (%): 34.30 | TFLOPs: 339.23 | Global batch size: 32 | Global tokens/sec: 146513.62 | Global MFU (%): 34.30 | Global TFLOPs: 1356.93 | 
2025-05-19 10:08:57,336 - root - INFO - Step: 970 | Loss: 5.65 | Tokens per second: 38209.31 | Training tokens per second (%): 24.94 | MFU (%): 35.78 | TFLOPs: 353.87 | Global batch size: 32 | Global tokens/sec: 152837.23 | Global MFU (%): 35.78 | Global TFLOPs: 1415.49 | 
2025-05-19 10:09:01,657 - root - INFO - Step: 980 | Loss: 5.68 | Tokens per second: 37926.18 | Training tokens per second (%): 24.94 | MFU (%): 35.52 | TFLOPs: 351.25 | Global batch size: 32 | Global tokens/sec: 151704.73 | Global MFU (%): 35.52 | Global TFLOPs: 1405.00 | 
2025-05-19 10:09:05,954 - root - INFO - Step: 990 | Loss: 5.67 | Tokens per second: 38129.97 | Training tokens per second (%): 24.96 | MFU (%): 35.71 | TFLOPs: 353.14 | Global batch size: 32 | Global tokens/sec: 152519.89 | Global MFU (%): 35.71 | Global TFLOPs: 1412.55 | 
2025-05-19 10:09:10,272 - root - INFO - Step: 1000 | Loss: 5.70 | Tokens per second: 37947.66 | Training tokens per second (%): 24.96 | MFU (%): 35.54 | TFLOPs: 351.45 | Global batch size: 32 | Global tokens/sec: 151790.66 | Global MFU (%): 35.54 | Global TFLOPs: 1405.80 | 
2025-05-19 10:09:10,272 - root - INFO - Training completed
[sbatch-master] task finished
