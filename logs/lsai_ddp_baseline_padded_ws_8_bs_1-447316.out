[sbatch-master] running on nid006646
[sbatch-master] SLURM_NODELIST: nid[006646,006673]
[sbatch-master] SLURM_NNODES: 2
[sbatch-master] SLURM_NODEID: 0
[Master] Master node: nid006646
[Master] World size: 8
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
[srun] rank=0 host=nid006646 noderank=0 localrank=0
W0518 20:45:43.658000 154257 torch/distributed/run.py:792] 
W0518 20:45:43.658000 154257 torch/distributed/run.py:792] *****************************************
W0518 20:45:43.658000 154257 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0518 20:45:43.658000 154257 torch/distributed/run.py:792] *****************************************
[srun] rank=1 host=nid006673 noderank=1 localrank=0
W0518 20:45:47.853000 65829 torch/distributed/run.py:792] 
W0518 20:45:47.853000 65829 torch/distributed/run.py:792] *****************************************
W0518 20:45:47.853000 65829 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0518 20:45:47.853000 65829 torch/distributed/run.py:792] *****************************************
2025-05-18 20:46:01,661 - root - INFO - [Distributed Init] Rank 0 initialized on node 0 on GPU 0.
2025-05-18 20:46:01,701 - root - INFO - [Distributed Init] Rank 4 initialized on node 1 on GPU 0.
[rank0]:[W518 20:46:02.224088158 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W518 20:46:02.594146502 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 20:46:02,174 - root - INFO - [Distributed Init] Rank 1 initialized on node 0 on GPU 1.
2025-05-18 20:46:02,174 - root - INFO - [Distributed Init] Rank 3 initialized on node 0 on GPU 3.
[rank1]:[W518 20:46:02.307459351 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W518 20:46:02.307472759 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 20:46:02,228 - root - INFO - [Distributed Init] Rank 2 initialized on node 0 on GPU 2.
[rank2]:[W518 20:46:02.362237471 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 20:46:02,309 - root - INFO - [Distributed Init] Rank 7 initialized on node 1 on GPU 3.
2025-05-18 20:46:02,309 - root - INFO - [Distributed Init] Rank 6 initialized on node 1 on GPU 2.
2025-05-18 20:46:02,309 - root - INFO - [Distributed Init] Rank 5 initialized on node 1 on GPU 1.
[rank5]:[W518 20:46:02.734314754 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 5]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W518 20:46:02.734314818 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 7]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W518 20:46:02.734907951 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 6]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 20:46:07,193 - root - INFO - [Rank 0] All ranks ready!
2025-05-18 20:46:07,193 - root - INFO - Distributed training enabled: 8 processes
2025-05-18 20:46:07,193 - root - INFO - Master process: 0 on cuda:0
2025-05-18 20:46:07,193 - root - INFO - Experiment args: Namespace(dataset='/capstor/scratch/cscs/kasparr/project/train_data.parquet', dataset_type='padded', pretokenized=False, tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=1, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=10, profile=False, profile_step_start=10, profile_step_end=10, grad_max_norm=1, model_dtype='bf16', compile=False, distributed=True, backend='nccl', find_unused_parameters=False)
2025-05-18 20:46:07,193 - root - INFO - Setting up Tokenizer...
2025-05-18 20:46:12,674 - root - INFO - Setting up DataLoaders...
2025-05-18 20:46:12,674 - root - INFO - Using padded ParquetDataset with on-the-fly tokenization
2025-05-18 20:46:17,976 - root - INFO - Setting up Model...
2025-05-18 20:46:54,199 - root - INFO - Model wrapped with DistributedDataParallel
2025-05-18 20:46:54,201 - root - INFO - Global batch size: 8 (local: 1 Ã— 8 processes)
2025-05-18 20:46:54,201 - root - INFO - Starting training!
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
2025-05-18 20:46:55,803 - root - INFO - Step: 1 | Loss: 11.93 | Tokens per second: 1278.84 | Training tokens per second (%): 6.13 | MFU (%): 6.66 | TFLOPs: 65.91 | Global batch size: 8 | Global tokens/sec: 10230.76 | Global MFU (%): 6.66 | Global TFLOPs: 527.31 | 
2025-05-18 20:47:00,444 - root - INFO - Step: 10 | Loss: 11.75 | Tokens per second: 3972.15 | Training tokens per second (%): 4.78 | MFU (%): 20.70 | TFLOPs: 204.73 | Global batch size: 8 | Global tokens/sec: 31777.22 | Global MFU (%): 20.70 | Global TFLOPs: 1637.84 | 
2025-05-18 20:47:05,730 - root - INFO - Step: 20 | Loss: 10.55 | Tokens per second: 3875.04 | Training tokens per second (%): 7.45 | MFU (%): 20.19 | TFLOPs: 199.72 | Global batch size: 8 | Global tokens/sec: 31000.30 | Global MFU (%): 20.19 | Global TFLOPs: 1597.79 | 
2025-05-18 20:47:10,892 - root - INFO - Step: 30 | Loss: 9.68 | Tokens per second: 3968.57 | Training tokens per second (%): 5.80 | MFU (%): 20.68 | TFLOPs: 204.54 | Global batch size: 8 | Global tokens/sec: 31748.54 | Global MFU (%): 20.68 | Global TFLOPs: 1636.36 | 
2025-05-18 20:47:16,061 - root - INFO - Step: 40 | Loss: 9.15 | Tokens per second: 3962.68 | Training tokens per second (%): 4.14 | MFU (%): 20.65 | TFLOPs: 204.24 | Global batch size: 8 | Global tokens/sec: 31701.42 | Global MFU (%): 20.65 | Global TFLOPs: 1633.93 | 
2025-05-18 20:47:21,212 - root - INFO - Step: 50 | Loss: 8.54 | Tokens per second: 3976.37 | Training tokens per second (%): 4.12 | MFU (%): 20.72 | TFLOPs: 204.95 | Global batch size: 8 | Global tokens/sec: 31810.97 | Global MFU (%): 20.72 | Global TFLOPs: 1639.58 | 
2025-05-18 20:47:26,397 - root - INFO - Step: 60 | Loss: 8.14 | Tokens per second: 3950.70 | Training tokens per second (%): 6.00 | MFU (%): 20.59 | TFLOPs: 203.62 | Global batch size: 8 | Global tokens/sec: 31605.61 | Global MFU (%): 20.59 | Global TFLOPs: 1628.99 | 
2025-05-18 20:47:31,577 - root - INFO - Step: 70 | Loss: 7.89 | Tokens per second: 3954.13 | Training tokens per second (%): 2.99 | MFU (%): 20.61 | TFLOPs: 203.80 | Global batch size: 8 | Global tokens/sec: 31633.00 | Global MFU (%): 20.61 | Global TFLOPs: 1630.40 | 
2025-05-18 20:47:36,722 - root - INFO - Step: 80 | Loss: 7.30 | Tokens per second: 3981.30 | Training tokens per second (%): 6.46 | MFU (%): 20.75 | TFLOPs: 205.20 | Global batch size: 8 | Global tokens/sec: 31850.42 | Global MFU (%): 20.75 | Global TFLOPs: 1641.61 | 
2025-05-18 20:47:41,877 - root - INFO - Step: 90 | Loss: 7.29 | Tokens per second: 3973.78 | Training tokens per second (%): 3.93 | MFU (%): 20.71 | TFLOPs: 204.81 | Global batch size: 8 | Global tokens/sec: 31790.22 | Global MFU (%): 20.71 | Global TFLOPs: 1638.51 | 
2025-05-18 20:47:47,065 - root - INFO - Step: 100 | Loss: 7.27 | Tokens per second: 3948.05 | Training tokens per second (%): 5.35 | MFU (%): 20.58 | TFLOPs: 203.49 | Global batch size: 8 | Global tokens/sec: 31584.37 | Global MFU (%): 20.58 | Global TFLOPs: 1627.90 | 
2025-05-18 20:47:52,231 - root - INFO - Step: 110 | Loss: 7.22 | Tokens per second: 3965.05 | Training tokens per second (%): 6.78 | MFU (%): 20.66 | TFLOPs: 204.36 | Global batch size: 8 | Global tokens/sec: 31720.40 | Global MFU (%): 20.66 | Global TFLOPs: 1634.91 | 
2025-05-18 20:47:57,392 - root - INFO - Step: 120 | Loss: 7.04 | Tokens per second: 3968.96 | Training tokens per second (%): 3.38 | MFU (%): 20.68 | TFLOPs: 204.57 | Global batch size: 8 | Global tokens/sec: 31751.69 | Global MFU (%): 20.68 | Global TFLOPs: 1636.52 | 
2025-05-18 20:48:02,552 - root - INFO - Step: 130 | Loss: 6.90 | Tokens per second: 3969.77 | Training tokens per second (%): 2.56 | MFU (%): 20.69 | TFLOPs: 204.61 | Global batch size: 8 | Global tokens/sec: 31758.14 | Global MFU (%): 20.69 | Global TFLOPs: 1636.85 | 
2025-05-18 20:48:07,721 - root - INFO - Step: 140 | Loss: 6.85 | Tokens per second: 3962.41 | Training tokens per second (%): 3.35 | MFU (%): 20.65 | TFLOPs: 204.23 | Global batch size: 8 | Global tokens/sec: 31699.30 | Global MFU (%): 20.65 | Global TFLOPs: 1633.82 | 
2025-05-18 20:48:12,913 - root - INFO - Step: 150 | Loss: 6.64 | Tokens per second: 3944.99 | Training tokens per second (%): 4.52 | MFU (%): 20.56 | TFLOPs: 203.33 | Global batch size: 8 | Global tokens/sec: 31559.93 | Global MFU (%): 20.56 | Global TFLOPs: 1626.64 | 
2025-05-18 20:48:18,100 - root - INFO - Step: 160 | Loss: 6.86 | Tokens per second: 3949.17 | Training tokens per second (%): 4.35 | MFU (%): 20.58 | TFLOPs: 203.55 | Global batch size: 8 | Global tokens/sec: 31593.38 | Global MFU (%): 20.58 | Global TFLOPs: 1628.36 | 
2025-05-18 20:48:23,258 - root - INFO - Step: 170 | Loss: 6.91 | Tokens per second: 3971.34 | Training tokens per second (%): 4.50 | MFU (%): 20.70 | TFLOPs: 204.69 | Global batch size: 8 | Global tokens/sec: 31770.69 | Global MFU (%): 20.70 | Global TFLOPs: 1637.50 | 
2025-05-18 20:48:28,432 - root - INFO - Step: 180 | Loss: 6.74 | Tokens per second: 3958.74 | Training tokens per second (%): 3.78 | MFU (%): 20.63 | TFLOPs: 204.04 | Global batch size: 8 | Global tokens/sec: 31669.91 | Global MFU (%): 20.63 | Global TFLOPs: 1632.31 | 
2025-05-18 20:48:33,597 - root - INFO - Step: 190 | Loss: 6.34 | Tokens per second: 3965.66 | Training tokens per second (%): 3.78 | MFU (%): 20.67 | TFLOPs: 204.39 | Global batch size: 8 | Global tokens/sec: 31725.26 | Global MFU (%): 20.67 | Global TFLOPs: 1635.16 | 
2025-05-18 20:48:38,748 - root - INFO - Step: 200 | Loss: 6.75 | Tokens per second: 3976.79 | Training tokens per second (%): 4.19 | MFU (%): 20.72 | TFLOPs: 204.97 | Global batch size: 8 | Global tokens/sec: 31814.34 | Global MFU (%): 20.72 | Global TFLOPs: 1639.75 | 
2025-05-18 20:48:43,916 - root - INFO - Step: 210 | Loss: 6.55 | Tokens per second: 3963.16 | Training tokens per second (%): 4.25 | MFU (%): 20.65 | TFLOPs: 204.27 | Global batch size: 8 | Global tokens/sec: 31705.31 | Global MFU (%): 20.65 | Global TFLOPs: 1634.13 | 
2025-05-18 20:48:49,060 - root - INFO - Step: 220 | Loss: 6.42 | Tokens per second: 3982.31 | Training tokens per second (%): 2.73 | MFU (%): 20.75 | TFLOPs: 205.25 | Global batch size: 8 | Global tokens/sec: 31858.49 | Global MFU (%): 20.75 | Global TFLOPs: 1642.03 | 
2025-05-18 20:48:54,245 - root - INFO - Step: 230 | Loss: 6.51 | Tokens per second: 3950.30 | Training tokens per second (%): 4.90 | MFU (%): 20.59 | TFLOPs: 203.60 | Global batch size: 8 | Global tokens/sec: 31602.41 | Global MFU (%): 20.59 | Global TFLOPs: 1628.83 | 
2025-05-18 20:48:59,420 - root - INFO - Step: 240 | Loss: 6.35 | Tokens per second: 3957.94 | Training tokens per second (%): 3.43 | MFU (%): 20.63 | TFLOPs: 204.00 | Global batch size: 8 | Global tokens/sec: 31663.49 | Global MFU (%): 20.63 | Global TFLOPs: 1631.97 | 
2025-05-18 20:49:04,578 - root - INFO - Step: 250 | Loss: 6.43 | Tokens per second: 3971.37 | Training tokens per second (%): 7.18 | MFU (%): 20.70 | TFLOPs: 204.69 | Global batch size: 8 | Global tokens/sec: 31770.99 | Global MFU (%): 20.70 | Global TFLOPs: 1637.52 | 
2025-05-18 20:49:09,758 - root - INFO - Step: 260 | Loss: 5.84 | Tokens per second: 3953.89 | Training tokens per second (%): 7.71 | MFU (%): 20.61 | TFLOPs: 203.79 | Global batch size: 8 | Global tokens/sec: 31631.15 | Global MFU (%): 20.61 | Global TFLOPs: 1630.31 | 
2025-05-18 20:49:14,939 - root - INFO - Step: 270 | Loss: 5.57 | Tokens per second: 3953.77 | Training tokens per second (%): 5.68 | MFU (%): 20.60 | TFLOPs: 203.78 | Global batch size: 8 | Global tokens/sec: 31630.17 | Global MFU (%): 20.60 | Global TFLOPs: 1630.26 | 
2025-05-18 20:49:20,119 - root - INFO - Step: 280 | Loss: 5.97 | Tokens per second: 3954.66 | Training tokens per second (%): 4.49 | MFU (%): 20.61 | TFLOPs: 203.83 | Global batch size: 8 | Global tokens/sec: 31637.27 | Global MFU (%): 20.61 | Global TFLOPs: 1630.62 | 
2025-05-18 20:49:25,304 - root - INFO - Step: 290 | Loss: 5.93 | Tokens per second: 3950.18 | Training tokens per second (%): 3.83 | MFU (%): 20.59 | TFLOPs: 203.60 | Global batch size: 8 | Global tokens/sec: 31601.42 | Global MFU (%): 20.59 | Global TFLOPs: 1628.78 | 
2025-05-18 20:49:30,460 - root - INFO - Step: 300 | Loss: 5.49 | Tokens per second: 3973.15 | Training tokens per second (%): 4.33 | MFU (%): 20.71 | TFLOPs: 204.78 | Global batch size: 8 | Global tokens/sec: 31785.19 | Global MFU (%): 20.71 | Global TFLOPs: 1638.25 | 
2025-05-18 20:49:35,620 - root - INFO - Step: 310 | Loss: 5.56 | Tokens per second: 3969.14 | Training tokens per second (%): 3.18 | MFU (%): 20.68 | TFLOPs: 204.57 | Global batch size: 8 | Global tokens/sec: 31753.10 | Global MFU (%): 20.68 | Global TFLOPs: 1636.59 | 
2025-05-18 20:49:40,777 - root - INFO - Step: 320 | Loss: 5.55 | Tokens per second: 3972.20 | Training tokens per second (%): 3.56 | MFU (%): 20.70 | TFLOPs: 204.73 | Global batch size: 8 | Global tokens/sec: 31777.63 | Global MFU (%): 20.70 | Global TFLOPs: 1637.86 | 
2025-05-18 20:49:45,962 - root - INFO - Step: 330 | Loss: 5.50 | Tokens per second: 3950.56 | Training tokens per second (%): 4.60 | MFU (%): 20.59 | TFLOPs: 203.62 | Global batch size: 8 | Global tokens/sec: 31604.47 | Global MFU (%): 20.59 | Global TFLOPs: 1628.93 | 
2025-05-18 20:49:51,119 - root - INFO - Step: 340 | Loss: 5.39 | Tokens per second: 3971.84 | Training tokens per second (%): 3.13 | MFU (%): 20.70 | TFLOPs: 204.71 | Global batch size: 8 | Global tokens/sec: 31774.75 | Global MFU (%): 20.70 | Global TFLOPs: 1637.71 | 
2025-05-18 20:49:56,268 - root - INFO - Step: 350 | Loss: 5.74 | Tokens per second: 3978.28 | Training tokens per second (%): 5.54 | MFU (%): 20.73 | TFLOPs: 205.05 | Global batch size: 8 | Global tokens/sec: 31826.26 | Global MFU (%): 20.73 | Global TFLOPs: 1640.36 | 
2025-05-18 20:50:01,430 - root - INFO - Step: 360 | Loss: 5.78 | Tokens per second: 3967.87 | Training tokens per second (%): 3.85 | MFU (%): 20.68 | TFLOPs: 204.51 | Global batch size: 8 | Global tokens/sec: 31742.96 | Global MFU (%): 20.68 | Global TFLOPs: 1636.07 | 
2025-05-18 20:50:06,616 - root - INFO - Step: 370 | Loss: 5.72 | Tokens per second: 3949.47 | Training tokens per second (%): 5.60 | MFU (%): 20.58 | TFLOPs: 203.56 | Global batch size: 8 | Global tokens/sec: 31595.73 | Global MFU (%): 20.58 | Global TFLOPs: 1628.48 | 
2025-05-18 20:50:11,769 - root - INFO - Step: 380 | Loss: 5.04 | Tokens per second: 3975.22 | Training tokens per second (%): 5.07 | MFU (%): 20.72 | TFLOPs: 204.89 | Global batch size: 8 | Global tokens/sec: 31801.73 | Global MFU (%): 20.72 | Global TFLOPs: 1639.10 | 
2025-05-18 20:50:16,941 - root - INFO - Step: 390 | Loss: 4.49 | Tokens per second: 3960.38 | Training tokens per second (%): 5.81 | MFU (%): 20.64 | TFLOPs: 204.12 | Global batch size: 8 | Global tokens/sec: 31683.07 | Global MFU (%): 20.64 | Global TFLOPs: 1632.98 | 
2025-05-18 20:50:22,122 - root - INFO - Step: 400 | Loss: 5.15 | Tokens per second: 3953.44 | Training tokens per second (%): 4.95 | MFU (%): 20.60 | TFLOPs: 203.77 | Global batch size: 8 | Global tokens/sec: 31627.56 | Global MFU (%): 20.60 | Global TFLOPs: 1630.12 | 
2025-05-18 20:50:27,270 - root - INFO - Step: 410 | Loss: 5.35 | Tokens per second: 3978.80 | Training tokens per second (%): 3.52 | MFU (%): 20.74 | TFLOPs: 205.07 | Global batch size: 8 | Global tokens/sec: 31830.40 | Global MFU (%): 20.74 | Global TFLOPs: 1640.58 | 
2025-05-18 20:50:32,441 - root - INFO - Step: 420 | Loss: 5.18 | Tokens per second: 3961.00 | Training tokens per second (%): 3.11 | MFU (%): 20.64 | TFLOPs: 204.15 | Global batch size: 8 | Global tokens/sec: 31687.96 | Global MFU (%): 20.64 | Global TFLOPs: 1633.24 | 
2025-05-18 20:50:37,606 - root - INFO - Step: 430 | Loss: 4.52 | Tokens per second: 3966.33 | Training tokens per second (%): 2.84 | MFU (%): 20.67 | TFLOPs: 204.43 | Global batch size: 8 | Global tokens/sec: 31730.65 | Global MFU (%): 20.67 | Global TFLOPs: 1635.44 | 
2025-05-18 20:50:42,784 - root - INFO - Step: 440 | Loss: 4.82 | Tokens per second: 3955.82 | Training tokens per second (%): 4.33 | MFU (%): 20.62 | TFLOPs: 203.89 | Global batch size: 8 | Global tokens/sec: 31646.52 | Global MFU (%): 20.62 | Global TFLOPs: 1631.10 | 
2025-05-18 20:50:47,953 - root - INFO - Step: 450 | Loss: 4.69 | Tokens per second: 3962.27 | Training tokens per second (%): 4.89 | MFU (%): 20.65 | TFLOPs: 204.22 | Global batch size: 8 | Global tokens/sec: 31698.13 | Global MFU (%): 20.65 | Global TFLOPs: 1633.76 | 
2025-05-18 20:50:53,149 - root - INFO - Step: 460 | Loss: 4.71 | Tokens per second: 3942.50 | Training tokens per second (%): 6.64 | MFU (%): 20.55 | TFLOPs: 203.20 | Global batch size: 8 | Global tokens/sec: 31539.97 | Global MFU (%): 20.55 | Global TFLOPs: 1625.61 | 
2025-05-18 20:50:58,336 - root - INFO - Step: 470 | Loss: 4.46 | Tokens per second: 3949.10 | Training tokens per second (%): 5.41 | MFU (%): 20.58 | TFLOPs: 203.54 | Global batch size: 8 | Global tokens/sec: 31592.83 | Global MFU (%): 20.58 | Global TFLOPs: 1628.33 | 
2025-05-18 20:51:03,515 - root - INFO - Step: 480 | Loss: 5.11 | Tokens per second: 3954.39 | Training tokens per second (%): 5.33 | MFU (%): 20.61 | TFLOPs: 203.81 | Global batch size: 8 | Global tokens/sec: 31635.12 | Global MFU (%): 20.61 | Global TFLOPs: 1630.51 | 
2025-05-18 20:51:08,699 - root - INFO - Step: 490 | Loss: 4.89 | Tokens per second: 3951.30 | Training tokens per second (%): 4.55 | MFU (%): 20.59 | TFLOPs: 203.65 | Global batch size: 8 | Global tokens/sec: 31610.42 | Global MFU (%): 20.59 | Global TFLOPs: 1629.24 | 
2025-05-18 20:51:13,864 - root - INFO - Step: 500 | Loss: 5.07 | Tokens per second: 3966.08 | Training tokens per second (%): 4.09 | MFU (%): 20.67 | TFLOPs: 204.42 | Global batch size: 8 | Global tokens/sec: 31728.67 | Global MFU (%): 20.67 | Global TFLOPs: 1635.33 | 
2025-05-18 20:51:19,006 - root - INFO - Step: 510 | Loss: 4.14 | Tokens per second: 3983.18 | Training tokens per second (%): 2.82 | MFU (%): 20.76 | TFLOPs: 205.30 | Global batch size: 8 | Global tokens/sec: 31865.42 | Global MFU (%): 20.76 | Global TFLOPs: 1642.38 | 
2025-05-18 20:51:24,192 - root - INFO - Step: 520 | Loss: 3.70 | Tokens per second: 3949.82 | Training tokens per second (%): 4.17 | MFU (%): 20.58 | TFLOPs: 203.58 | Global batch size: 8 | Global tokens/sec: 31598.55 | Global MFU (%): 20.58 | Global TFLOPs: 1628.63 | 
2025-05-18 20:51:29,381 - root - INFO - Step: 530 | Loss: 3.77 | Tokens per second: 3947.93 | Training tokens per second (%): 4.79 | MFU (%): 20.57 | TFLOPs: 203.48 | Global batch size: 8 | Global tokens/sec: 31583.44 | Global MFU (%): 20.57 | Global TFLOPs: 1627.85 | 
2025-05-18 20:51:34,542 - root - INFO - Step: 540 | Loss: 4.00 | Tokens per second: 3968.44 | Training tokens per second (%): 4.79 | MFU (%): 20.68 | TFLOPs: 204.54 | Global batch size: 8 | Global tokens/sec: 31747.50 | Global MFU (%): 20.68 | Global TFLOPs: 1636.30 | 
2025-05-18 20:51:39,725 - root - INFO - Step: 550 | Loss: 5.02 | Tokens per second: 3952.01 | Training tokens per second (%): 4.57 | MFU (%): 20.60 | TFLOPs: 203.69 | Global batch size: 8 | Global tokens/sec: 31616.09 | Global MFU (%): 20.60 | Global TFLOPs: 1629.53 | 
2025-05-18 20:51:44,874 - root - INFO - Step: 560 | Loss: 4.02 | Tokens per second: 3978.02 | Training tokens per second (%): 4.91 | MFU (%): 20.73 | TFLOPs: 205.03 | Global batch size: 8 | Global tokens/sec: 31824.19 | Global MFU (%): 20.73 | Global TFLOPs: 1640.26 | 
2025-05-18 20:51:50,048 - root - INFO - Step: 570 | Loss: 4.68 | Tokens per second: 3959.35 | Training tokens per second (%): 5.16 | MFU (%): 20.63 | TFLOPs: 204.07 | Global batch size: 8 | Global tokens/sec: 31674.77 | Global MFU (%): 20.63 | Global TFLOPs: 1632.56 | 
2025-05-18 20:51:55,235 - root - INFO - Step: 580 | Loss: 4.65 | Tokens per second: 3948.39 | Training tokens per second (%): 6.27 | MFU (%): 20.58 | TFLOPs: 203.50 | Global batch size: 8 | Global tokens/sec: 31587.15 | Global MFU (%): 20.58 | Global TFLOPs: 1628.04 | 
2025-05-18 20:52:00,419 - root - INFO - Step: 590 | Loss: 4.10 | Tokens per second: 3951.36 | Training tokens per second (%): 6.14 | MFU (%): 20.59 | TFLOPs: 203.66 | Global batch size: 8 | Global tokens/sec: 31610.86 | Global MFU (%): 20.59 | Global TFLOPs: 1629.26 | 
2025-05-18 20:52:05,570 - root - INFO - Step: 600 | Loss: 3.72 | Tokens per second: 3976.92 | Training tokens per second (%): 5.03 | MFU (%): 20.73 | TFLOPs: 204.97 | Global batch size: 8 | Global tokens/sec: 31815.32 | Global MFU (%): 20.73 | Global TFLOPs: 1639.80 | 
2025-05-18 20:52:10,756 - root - INFO - Step: 610 | Loss: 4.36 | Tokens per second: 3949.85 | Training tokens per second (%): 5.14 | MFU (%): 20.58 | TFLOPs: 203.58 | Global batch size: 8 | Global tokens/sec: 31598.81 | Global MFU (%): 20.58 | Global TFLOPs: 1628.64 | 
2025-05-18 20:52:15,902 - root - INFO - Step: 620 | Loss: 4.09 | Tokens per second: 3980.44 | Training tokens per second (%): 3.21 | MFU (%): 20.74 | TFLOPs: 205.16 | Global batch size: 8 | Global tokens/sec: 31843.50 | Global MFU (%): 20.74 | Global TFLOPs: 1641.25 | 
2025-05-18 20:52:21,096 - root - INFO - Step: 630 | Loss: 3.50 | Tokens per second: 3943.77 | Training tokens per second (%): 4.14 | MFU (%): 20.55 | TFLOPs: 203.27 | Global batch size: 8 | Global tokens/sec: 31550.19 | Global MFU (%): 20.55 | Global TFLOPs: 1626.13 | 
2025-05-18 20:52:26,269 - root - INFO - Step: 640 | Loss: 4.18 | Tokens per second: 3959.36 | Training tokens per second (%): 4.53 | MFU (%): 20.63 | TFLOPs: 204.07 | Global batch size: 8 | Global tokens/sec: 31674.85 | Global MFU (%): 20.63 | Global TFLOPs: 1632.56 | 
2025-05-18 20:52:31,418 - root - INFO - Step: 650 | Loss: 3.50 | Tokens per second: 3978.34 | Training tokens per second (%): 3.15 | MFU (%): 20.73 | TFLOPs: 205.05 | Global batch size: 8 | Global tokens/sec: 31826.75 | Global MFU (%): 20.73 | Global TFLOPs: 1640.39 | 
2025-05-18 20:52:36,616 - root - INFO - Step: 660 | Loss: 4.14 | Tokens per second: 3939.94 | Training tokens per second (%): 4.76 | MFU (%): 20.53 | TFLOPs: 203.07 | Global batch size: 8 | Global tokens/sec: 31519.52 | Global MFU (%): 20.53 | Global TFLOPs: 1624.55 | 
2025-05-18 20:52:41,789 - root - INFO - Step: 670 | Loss: 3.22 | Tokens per second: 3960.33 | Training tokens per second (%): 5.38 | MFU (%): 20.64 | TFLOPs: 204.12 | Global batch size: 8 | Global tokens/sec: 31682.64 | Global MFU (%): 20.64 | Global TFLOPs: 1632.96 | 
2025-05-18 20:52:46,977 - root - INFO - Step: 680 | Loss: 4.19 | Tokens per second: 3948.01 | Training tokens per second (%): 6.40 | MFU (%): 20.57 | TFLOPs: 203.49 | Global batch size: 8 | Global tokens/sec: 31584.09 | Global MFU (%): 20.57 | Global TFLOPs: 1627.88 | 
2025-05-18 20:52:52,153 - root - INFO - Step: 690 | Loss: 3.73 | Tokens per second: 3957.56 | Training tokens per second (%): 6.35 | MFU (%): 20.62 | TFLOPs: 203.98 | Global batch size: 8 | Global tokens/sec: 31660.47 | Global MFU (%): 20.62 | Global TFLOPs: 1631.82 | 
2025-05-18 20:52:57,333 - root - INFO - Step: 700 | Loss: 3.21 | Tokens per second: 3954.24 | Training tokens per second (%): 4.08 | MFU (%): 20.61 | TFLOPs: 203.81 | Global batch size: 8 | Global tokens/sec: 31633.88 | Global MFU (%): 20.61 | Global TFLOPs: 1630.45 | 
2025-05-18 20:53:02,508 - root - INFO - Step: 710 | Loss: 3.77 | Tokens per second: 3957.50 | Training tokens per second (%): 4.56 | MFU (%): 20.62 | TFLOPs: 203.97 | Global batch size: 8 | Global tokens/sec: 31660.01 | Global MFU (%): 20.62 | Global TFLOPs: 1631.80 | 
2025-05-18 20:53:07,701 - root - INFO - Step: 720 | Loss: 3.54 | Tokens per second: 3945.08 | Training tokens per second (%): 5.04 | MFU (%): 20.56 | TFLOPs: 203.33 | Global batch size: 8 | Global tokens/sec: 31560.64 | Global MFU (%): 20.56 | Global TFLOPs: 1626.67 | 
2025-05-18 20:53:12,845 - root - INFO - Step: 730 | Loss: 3.92 | Tokens per second: 3981.40 | Training tokens per second (%): 3.78 | MFU (%): 20.75 | TFLOPs: 205.21 | Global batch size: 8 | Global tokens/sec: 31851.19 | Global MFU (%): 20.75 | Global TFLOPs: 1641.65 | 
2025-05-18 20:53:18,017 - root - INFO - Step: 740 | Loss: 3.39 | Tokens per second: 3960.31 | Training tokens per second (%): 4.21 | MFU (%): 20.64 | TFLOPs: 204.12 | Global batch size: 8 | Global tokens/sec: 31682.44 | Global MFU (%): 20.64 | Global TFLOPs: 1632.95 | 
2025-05-18 20:53:23,205 - root - INFO - Step: 750 | Loss: 4.29 | Tokens per second: 3948.86 | Training tokens per second (%): 4.13 | MFU (%): 20.58 | TFLOPs: 203.53 | Global batch size: 8 | Global tokens/sec: 31590.84 | Global MFU (%): 20.58 | Global TFLOPs: 1628.23 | 
2025-05-18 20:53:28,408 - root - INFO - Step: 760 | Loss: 3.25 | Tokens per second: 3936.13 | Training tokens per second (%): 6.67 | MFU (%): 20.51 | TFLOPs: 202.87 | Global batch size: 8 | Global tokens/sec: 31489.01 | Global MFU (%): 20.51 | Global TFLOPs: 1622.98 | 
2025-05-18 20:53:33,575 - root - INFO - Step: 770 | Loss: 3.02 | Tokens per second: 3964.50 | Training tokens per second (%): 5.05 | MFU (%): 20.66 | TFLOPs: 204.33 | Global batch size: 8 | Global tokens/sec: 31715.96 | Global MFU (%): 20.66 | Global TFLOPs: 1634.68 | 
2025-05-18 20:53:38,749 - root - INFO - Step: 780 | Loss: 2.87 | Tokens per second: 3959.12 | Training tokens per second (%): 4.42 | MFU (%): 20.63 | TFLOPs: 204.06 | Global batch size: 8 | Global tokens/sec: 31672.92 | Global MFU (%): 20.63 | Global TFLOPs: 1632.46 | 
2025-05-18 20:53:43,913 - root - INFO - Step: 790 | Loss: 3.69 | Tokens per second: 3966.57 | Training tokens per second (%): 4.11 | MFU (%): 20.67 | TFLOPs: 204.44 | Global batch size: 8 | Global tokens/sec: 31732.59 | Global MFU (%): 20.67 | Global TFLOPs: 1635.54 | 
2025-05-18 20:53:49,081 - root - INFO - Step: 800 | Loss: 2.80 | Tokens per second: 3963.00 | Training tokens per second (%): 4.59 | MFU (%): 20.65 | TFLOPs: 204.26 | Global batch size: 8 | Global tokens/sec: 31703.99 | Global MFU (%): 20.65 | Global TFLOPs: 1634.06 | 
2025-05-18 20:53:54,276 - root - INFO - Step: 810 | Loss: 3.46 | Tokens per second: 3943.48 | Training tokens per second (%): 4.33 | MFU (%): 20.55 | TFLOPs: 203.25 | Global batch size: 8 | Global tokens/sec: 31547.82 | Global MFU (%): 20.55 | Global TFLOPs: 1626.01 | 
2025-05-18 20:53:59,462 - root - INFO - Step: 820 | Loss: 2.92 | Tokens per second: 3949.31 | Training tokens per second (%): 4.47 | MFU (%): 20.58 | TFLOPs: 203.55 | Global batch size: 8 | Global tokens/sec: 31594.47 | Global MFU (%): 20.58 | Global TFLOPs: 1628.42 | 
2025-05-18 20:54:04,637 - root - INFO - Step: 830 | Loss: 2.96 | Tokens per second: 3957.91 | Training tokens per second (%): 6.83 | MFU (%): 20.63 | TFLOPs: 204.00 | Global batch size: 8 | Global tokens/sec: 31663.24 | Global MFU (%): 20.63 | Global TFLOPs: 1631.96 | 
2025-05-18 20:54:09,841 - root - INFO - Step: 840 | Loss: 3.13 | Tokens per second: 3936.37 | Training tokens per second (%): 6.00 | MFU (%): 20.51 | TFLOPs: 202.89 | Global batch size: 8 | Global tokens/sec: 31490.96 | Global MFU (%): 20.51 | Global TFLOPs: 1623.08 | 
2025-05-18 20:54:15,019 - root - INFO - Step: 850 | Loss: 3.33 | Tokens per second: 3955.68 | Training tokens per second (%): 7.29 | MFU (%): 20.61 | TFLOPs: 203.88 | Global batch size: 8 | Global tokens/sec: 31645.46 | Global MFU (%): 20.61 | Global TFLOPs: 1631.05 | 
2025-05-18 20:54:20,194 - root - INFO - Step: 860 | Loss: 2.80 | Tokens per second: 3958.47 | Training tokens per second (%): 5.60 | MFU (%): 20.63 | TFLOPs: 204.02 | Global batch size: 8 | Global tokens/sec: 31667.75 | Global MFU (%): 20.63 | Global TFLOPs: 1632.19 | 
2025-05-18 20:54:25,369 - root - INFO - Step: 870 | Loss: 3.40 | Tokens per second: 3958.00 | Training tokens per second (%): 2.95 | MFU (%): 20.63 | TFLOPs: 204.00 | Global batch size: 8 | Global tokens/sec: 31663.99 | Global MFU (%): 20.63 | Global TFLOPs: 1632.00 | 
2025-05-18 20:54:30,536 - root - INFO - Step: 880 | Loss: 2.33 | Tokens per second: 3964.45 | Training tokens per second (%): 7.08 | MFU (%): 20.66 | TFLOPs: 204.33 | Global batch size: 8 | Global tokens/sec: 31715.62 | Global MFU (%): 20.66 | Global TFLOPs: 1634.66 | 
2025-05-18 20:54:35,749 - root - INFO - Step: 890 | Loss: 3.19 | Tokens per second: 3929.34 | Training tokens per second (%): 6.01 | MFU (%): 20.48 | TFLOPs: 202.52 | Global batch size: 8 | Global tokens/sec: 31434.75 | Global MFU (%): 20.48 | Global TFLOPs: 1620.19 | 
2025-05-18 20:54:40,937 - root - INFO - Step: 900 | Loss: 3.35 | Tokens per second: 3948.09 | Training tokens per second (%): 4.00 | MFU (%): 20.58 | TFLOPs: 203.49 | Global batch size: 8 | Global tokens/sec: 31584.73 | Global MFU (%): 20.58 | Global TFLOPs: 1627.92 | 
2025-05-18 20:54:46,100 - root - INFO - Step: 910 | Loss: 1.42 | Tokens per second: 3967.36 | Training tokens per second (%): 5.07 | MFU (%): 20.68 | TFLOPs: 204.48 | Global batch size: 8 | Global tokens/sec: 31738.85 | Global MFU (%): 20.68 | Global TFLOPs: 1635.86 | 
2025-05-18 20:54:51,279 - root - INFO - Step: 920 | Loss: 2.45 | Tokens per second: 3955.02 | Training tokens per second (%): 7.56 | MFU (%): 20.61 | TFLOPs: 203.85 | Global batch size: 8 | Global tokens/sec: 31640.15 | Global MFU (%): 20.61 | Global TFLOPs: 1630.77 | 
2025-05-18 20:54:56,439 - root - INFO - Step: 930 | Loss: 1.63 | Tokens per second: 3969.86 | Training tokens per second (%): 3.14 | MFU (%): 20.69 | TFLOPs: 204.61 | Global batch size: 8 | Global tokens/sec: 31758.84 | Global MFU (%): 20.69 | Global TFLOPs: 1636.89 | 
2025-05-18 20:55:01,586 - root - INFO - Step: 940 | Loss: 2.24 | Tokens per second: 3979.48 | Training tokens per second (%): 4.37 | MFU (%): 20.74 | TFLOPs: 205.11 | Global batch size: 8 | Global tokens/sec: 31835.88 | Global MFU (%): 20.74 | Global TFLOPs: 1640.86 | 
2025-05-18 20:55:06,739 - root - INFO - Step: 950 | Loss: 3.13 | Tokens per second: 3974.80 | Training tokens per second (%): 6.24 | MFU (%): 20.71 | TFLOPs: 204.87 | Global batch size: 8 | Global tokens/sec: 31798.43 | Global MFU (%): 20.71 | Global TFLOPs: 1638.93 | 
2025-05-18 20:55:11,910 - root - INFO - Step: 960 | Loss: 2.48 | Tokens per second: 3961.41 | Training tokens per second (%): 4.20 | MFU (%): 20.64 | TFLOPs: 204.18 | Global batch size: 8 | Global tokens/sec: 31691.31 | Global MFU (%): 20.64 | Global TFLOPs: 1633.41 | 
2025-05-18 20:55:17,108 - root - INFO - Step: 970 | Loss: 2.85 | Tokens per second: 3940.61 | Training tokens per second (%): 5.33 | MFU (%): 20.54 | TFLOPs: 203.10 | Global batch size: 8 | Global tokens/sec: 31524.91 | Global MFU (%): 20.54 | Global TFLOPs: 1624.83 | 
2025-05-18 20:55:22,280 - root - INFO - Step: 980 | Loss: 2.19 | Tokens per second: 3960.35 | Training tokens per second (%): 3.36 | MFU (%): 20.64 | TFLOPs: 204.12 | Global batch size: 8 | Global tokens/sec: 31682.78 | Global MFU (%): 20.64 | Global TFLOPs: 1632.97 | 
2025-05-18 20:55:27,465 - root - INFO - Step: 990 | Loss: 2.80 | Tokens per second: 3950.51 | Training tokens per second (%): 4.84 | MFU (%): 20.59 | TFLOPs: 203.61 | Global batch size: 8 | Global tokens/sec: 31604.12 | Global MFU (%): 20.59 | Global TFLOPs: 1628.91 | 
2025-05-18 20:55:32,655 - root - INFO - Step: 1000 | Loss: 2.08 | Tokens per second: 3946.66 | Training tokens per second (%): 7.12 | MFU (%): 20.57 | TFLOPs: 203.42 | Global batch size: 8 | Global tokens/sec: 31573.31 | Global MFU (%): 20.57 | Global TFLOPs: 1627.33 | 
2025-05-18 20:55:32,655 - root - INFO - Training completed
[sbatch-master] task finished
