[sbatch-master] running on nid006602
[sbatch-master] SLURM_NODELIST: nid006602
[sbatch-master] SLURM_NNODES: 1
[sbatch-master] SLURM_NODEID: 0
[Master] Master node: nid006602
[Master] World size: 4
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
[srun] rank=0 host=nid006602 noderank=0 localrank=0
W0519 08:54:46.899000 158904 torch/distributed/run.py:792] 
W0519 08:54:46.899000 158904 torch/distributed/run.py:792] *****************************************
W0519 08:54:46.899000 158904 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0519 08:54:46.899000 158904 torch/distributed/run.py:792] *****************************************
2025-05-19 08:54:59,878 - root - INFO - [Distributed Init] Rank 0 initialized on node 0 on GPU 0.
[rank0]:[W519 08:55:00.075556355 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 08:55:00,430 - root - INFO - [Distributed Init] Rank 3 initialized on node 0 on GPU 3.
[rank3]:[W519 08:55:00.182688365 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 08:55:00,460 - root - INFO - [Distributed Init] Rank 2 initialized on node 0 on GPU 2.
2025-05-19 08:55:00,460 - root - INFO - [Distributed Init] Rank 1 initialized on node 0 on GPU 1.
[rank2]:[W519 08:55:00.213723054 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W519 08:55:00.214038213 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 08:55:07,345 - root - INFO - [Rank 0] All ranks ready!
2025-05-19 08:55:07,345 - root - INFO - Distributed training enabled: 4 processes
2025-05-19 08:55:07,345 - root - INFO - Master process: 0 on cuda:0
2025-05-19 08:55:07,345 - root - INFO - Experiment args: Namespace(dataset='/capstor/scratch/cscs/kasparr/project/train_data_tokenized_padded_snappy.parquet', dataset_type='padded', pretokenized=True, tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=10, profile=False, profile_step_start=10, profile_step_end=10, grad_max_norm=1, model_dtype='bf16', compile=False, distributed=True, backend='nccl', find_unused_parameters=False)
2025-05-19 08:55:07,345 - root - INFO - Setting up Tokenizer...
2025-05-19 08:55:08,026 - root - INFO - Setting up DataLoaders...
2025-05-19 08:55:08,026 - root - INFO - Using pretokenized data: /capstor/scratch/cscs/kasparr/project/train_data_tokenized_padded_snappy.parquet
Loaded pretokenized dataset with 785906 samples
Loaded pretokenized dataset with 785906 samples
Loaded pretokenized dataset with 785906 samples
2025-05-19 08:55:39,351 - root - INFO - Setting up Model...
Loaded pretokenized dataset with 785906 samples
2025-05-19 08:55:48,598 - root - INFO - Model wrapped with DistributedDataParallel
2025-05-19 08:55:48,599 - root - INFO - Global batch size: 32 (local: 8 Ã— 4 processes)
2025-05-19 08:55:48,599 - root - INFO - Starting training!
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
2025-05-19 08:55:50,090 - root - INFO - Step: 1 | Loss: 11.95 | Tokens per second: 10995.32 | Training tokens per second (%): 4.86 | MFU (%): 10.30 | TFLOPs: 101.83 | Global batch size: 32 | Global tokens/sec: 43981.30 | Global MFU (%): 10.30 | Global TFLOPs: 407.33 | 
2025-05-19 08:55:53,753 - root - INFO - Step: 10 | Loss: 11.89 | Tokens per second: 40258.02 | Training tokens per second (%): 9.42 | MFU (%): 37.70 | TFLOPs: 372.85 | Global batch size: 32 | Global tokens/sec: 161032.09 | Global MFU (%): 37.70 | Global TFLOPs: 1491.39 | 
2025-05-19 08:55:57,735 - root - INFO - Step: 20 | Loss: 11.39 | Tokens per second: 41159.64 | Training tokens per second (%): 9.72 | MFU (%): 38.54 | TFLOPs: 381.20 | Global batch size: 32 | Global tokens/sec: 164638.58 | Global MFU (%): 38.54 | Global TFLOPs: 1524.79 | 
2025-05-19 08:56:01,713 - root - INFO - Step: 30 | Loss: 10.82 | Tokens per second: 41192.05 | Training tokens per second (%): 9.62 | MFU (%): 38.57 | TFLOPs: 381.50 | Global batch size: 32 | Global tokens/sec: 164768.21 | Global MFU (%): 38.57 | Global TFLOPs: 1525.99 | 
2025-05-19 08:56:05,691 - root - INFO - Step: 40 | Loss: 10.16 | Tokens per second: 41193.50 | Training tokens per second (%): 8.43 | MFU (%): 38.58 | TFLOPs: 381.51 | Global batch size: 32 | Global tokens/sec: 164774.00 | Global MFU (%): 38.58 | Global TFLOPs: 1526.04 | 
2025-05-19 08:56:09,674 - root - INFO - Step: 50 | Loss: 9.62 | Tokens per second: 41137.20 | Training tokens per second (%): 9.75 | MFU (%): 38.52 | TFLOPs: 380.99 | Global batch size: 32 | Global tokens/sec: 164548.79 | Global MFU (%): 38.52 | Global TFLOPs: 1523.96 | 
2025-05-19 08:56:13,647 - root - INFO - Step: 60 | Loss: 9.22 | Tokens per second: 41247.69 | Training tokens per second (%): 8.90 | MFU (%): 38.63 | TFLOPs: 382.01 | Global batch size: 32 | Global tokens/sec: 164990.75 | Global MFU (%): 38.63 | Global TFLOPs: 1528.05 | 
2025-05-19 08:56:17,616 - root - INFO - Step: 70 | Loss: 8.68 | Tokens per second: 41285.49 | Training tokens per second (%): 9.66 | MFU (%): 38.66 | TFLOPs: 382.36 | Global batch size: 32 | Global tokens/sec: 165141.95 | Global MFU (%): 38.66 | Global TFLOPs: 1529.45 | 
2025-05-19 08:56:21,589 - root - INFO - Step: 80 | Loss: 8.21 | Tokens per second: 41254.79 | Training tokens per second (%): 9.37 | MFU (%): 38.63 | TFLOPs: 382.08 | Global batch size: 32 | Global tokens/sec: 165019.16 | Global MFU (%): 38.63 | Global TFLOPs: 1528.31 | 
2025-05-19 08:56:25,562 - root - INFO - Step: 90 | Loss: 7.80 | Tokens per second: 41245.45 | Training tokens per second (%): 9.33 | MFU (%): 38.62 | TFLOPs: 381.99 | Global batch size: 32 | Global tokens/sec: 164981.79 | Global MFU (%): 38.62 | Global TFLOPs: 1527.97 | 
2025-05-19 08:56:29,536 - root - INFO - Step: 100 | Loss: 7.64 | Tokens per second: 41232.28 | Training tokens per second (%): 9.46 | MFU (%): 38.61 | TFLOPs: 381.87 | Global batch size: 32 | Global tokens/sec: 164929.10 | Global MFU (%): 38.61 | Global TFLOPs: 1527.48 | 
2025-05-19 08:56:33,496 - root - INFO - Step: 110 | Loss: 7.38 | Tokens per second: 41379.18 | Training tokens per second (%): 9.61 | MFU (%): 38.75 | TFLOPs: 383.23 | Global batch size: 32 | Global tokens/sec: 165516.74 | Global MFU (%): 38.75 | Global TFLOPs: 1532.92 | 
2025-05-19 08:56:37,498 - root - INFO - Step: 120 | Loss: 7.37 | Tokens per second: 40952.94 | Training tokens per second (%): 9.11 | MFU (%): 38.35 | TFLOPs: 379.28 | Global batch size: 32 | Global tokens/sec: 163811.77 | Global MFU (%): 38.35 | Global TFLOPs: 1517.13 | 
2025-05-19 08:56:41,468 - root - INFO - Step: 130 | Loss: 7.26 | Tokens per second: 41280.36 | Training tokens per second (%): 9.71 | MFU (%): 38.66 | TFLOPs: 382.32 | Global batch size: 32 | Global tokens/sec: 165121.43 | Global MFU (%): 38.66 | Global TFLOPs: 1529.26 | 
2025-05-19 08:56:45,437 - root - INFO - Step: 140 | Loss: 7.29 | Tokens per second: 41289.79 | Training tokens per second (%): 8.94 | MFU (%): 38.67 | TFLOPs: 382.40 | Global batch size: 32 | Global tokens/sec: 165159.15 | Global MFU (%): 38.67 | Global TFLOPs: 1529.61 | 
2025-05-19 08:56:49,410 - root - INFO - Step: 150 | Loss: 7.12 | Tokens per second: 41238.92 | Training tokens per second (%): 9.78 | MFU (%): 38.62 | TFLOPs: 381.93 | Global batch size: 32 | Global tokens/sec: 164955.68 | Global MFU (%): 38.62 | Global TFLOPs: 1527.73 | 
2025-05-19 08:56:53,374 - root - INFO - Step: 160 | Loss: 7.27 | Tokens per second: 41347.20 | Training tokens per second (%): 8.54 | MFU (%): 38.72 | TFLOPs: 382.93 | Global batch size: 32 | Global tokens/sec: 165388.81 | Global MFU (%): 38.72 | Global TFLOPs: 1531.74 | 
2025-05-19 08:56:57,349 - root - INFO - Step: 170 | Loss: 7.16 | Tokens per second: 41224.63 | Training tokens per second (%): 9.07 | MFU (%): 38.60 | TFLOPs: 381.80 | Global batch size: 32 | Global tokens/sec: 164898.53 | Global MFU (%): 38.60 | Global TFLOPs: 1527.20 | 
2025-05-19 08:57:01,318 - root - INFO - Step: 180 | Loss: 7.07 | Tokens per second: 41287.63 | Training tokens per second (%): 8.97 | MFU (%): 38.66 | TFLOPs: 382.38 | Global batch size: 32 | Global tokens/sec: 165150.52 | Global MFU (%): 38.66 | Global TFLOPs: 1529.53 | 
2025-05-19 08:57:05,296 - root - INFO - Step: 190 | Loss: 6.98 | Tokens per second: 41197.27 | Training tokens per second (%): 9.50 | MFU (%): 38.58 | TFLOPs: 381.55 | Global batch size: 32 | Global tokens/sec: 164789.10 | Global MFU (%): 38.58 | Global TFLOPs: 1526.18 | 
2025-05-19 08:57:09,260 - root - INFO - Step: 200 | Loss: 7.08 | Tokens per second: 41340.13 | Training tokens per second (%): 9.25 | MFU (%): 38.71 | TFLOPs: 382.87 | Global batch size: 32 | Global tokens/sec: 165360.52 | Global MFU (%): 38.71 | Global TFLOPs: 1531.47 | 
2025-05-19 08:57:13,232 - root - INFO - Step: 210 | Loss: 7.05 | Tokens per second: 41248.84 | Training tokens per second (%): 10.29 | MFU (%): 38.63 | TFLOPs: 382.02 | Global batch size: 32 | Global tokens/sec: 164995.34 | Global MFU (%): 38.63 | Global TFLOPs: 1528.09 | 
2025-05-19 08:57:17,211 - root - INFO - Step: 220 | Loss: 6.94 | Tokens per second: 41189.39 | Training tokens per second (%): 9.02 | MFU (%): 38.57 | TFLOPs: 381.47 | Global batch size: 32 | Global tokens/sec: 164757.58 | Global MFU (%): 38.57 | Global TFLOPs: 1525.89 | 
2025-05-19 08:57:21,185 - root - INFO - Step: 230 | Loss: 6.86 | Tokens per second: 41240.54 | Training tokens per second (%): 10.01 | MFU (%): 38.62 | TFLOPs: 381.95 | Global batch size: 32 | Global tokens/sec: 164962.17 | Global MFU (%): 38.62 | Global TFLOPs: 1527.79 | 
2025-05-19 08:57:25,158 - root - INFO - Step: 240 | Loss: 6.95 | Tokens per second: 41241.38 | Training tokens per second (%): 10.10 | MFU (%): 38.62 | TFLOPs: 381.95 | Global batch size: 32 | Global tokens/sec: 164965.51 | Global MFU (%): 38.62 | Global TFLOPs: 1527.82 | 
2025-05-19 08:57:29,131 - root - INFO - Step: 250 | Loss: 6.86 | Tokens per second: 41250.82 | Training tokens per second (%): 9.45 | MFU (%): 38.63 | TFLOPs: 382.04 | Global batch size: 32 | Global tokens/sec: 165003.27 | Global MFU (%): 38.63 | Global TFLOPs: 1528.17 | 
2025-05-19 08:57:33,108 - root - INFO - Step: 260 | Loss: 6.74 | Tokens per second: 41200.33 | Training tokens per second (%): 10.14 | MFU (%): 38.58 | TFLOPs: 381.57 | Global batch size: 32 | Global tokens/sec: 164801.32 | Global MFU (%): 38.58 | Global TFLOPs: 1526.30 | 
2025-05-19 08:57:37,078 - root - INFO - Step: 270 | Loss: 6.70 | Tokens per second: 41279.79 | Training tokens per second (%): 8.53 | MFU (%): 38.66 | TFLOPs: 382.31 | Global batch size: 32 | Global tokens/sec: 165119.17 | Global MFU (%): 38.66 | Global TFLOPs: 1529.24 | 
2025-05-19 08:57:41,052 - root - INFO - Step: 280 | Loss: 6.72 | Tokens per second: 41238.52 | Training tokens per second (%): 10.49 | MFU (%): 38.62 | TFLOPs: 381.93 | Global batch size: 32 | Global tokens/sec: 164954.09 | Global MFU (%): 38.62 | Global TFLOPs: 1527.71 | 
2025-05-19 08:57:45,031 - root - INFO - Step: 290 | Loss: 6.80 | Tokens per second: 41181.80 | Training tokens per second (%): 10.74 | MFU (%): 38.56 | TFLOPs: 381.40 | Global batch size: 32 | Global tokens/sec: 164727.20 | Global MFU (%): 38.56 | Global TFLOPs: 1525.61 | 
2025-05-19 08:57:49,011 - root - INFO - Step: 300 | Loss: 6.73 | Tokens per second: 41171.47 | Training tokens per second (%): 9.63 | MFU (%): 38.55 | TFLOPs: 381.31 | Global batch size: 32 | Global tokens/sec: 164685.90 | Global MFU (%): 38.55 | Global TFLOPs: 1525.23 | 
2025-05-19 08:57:52,993 - root - INFO - Step: 310 | Loss: 6.65 | Tokens per second: 41159.34 | Training tokens per second (%): 9.04 | MFU (%): 38.54 | TFLOPs: 381.19 | Global batch size: 32 | Global tokens/sec: 164637.37 | Global MFU (%): 38.54 | Global TFLOPs: 1524.78 | 
2025-05-19 08:57:56,966 - root - INFO - Step: 320 | Loss: 6.59 | Tokens per second: 41244.20 | Training tokens per second (%): 8.74 | MFU (%): 38.62 | TFLOPs: 381.98 | Global batch size: 32 | Global tokens/sec: 164976.81 | Global MFU (%): 38.62 | Global TFLOPs: 1527.92 | 
2025-05-19 08:58:00,939 - root - INFO - Step: 330 | Loss: 6.52 | Tokens per second: 41246.27 | Training tokens per second (%): 11.23 | MFU (%): 38.62 | TFLOPs: 382.00 | Global batch size: 32 | Global tokens/sec: 164985.08 | Global MFU (%): 38.62 | Global TFLOPs: 1528.00 | 
2025-05-19 08:58:04,908 - root - INFO - Step: 340 | Loss: 6.56 | Tokens per second: 41289.46 | Training tokens per second (%): 9.15 | MFU (%): 38.67 | TFLOPs: 382.40 | Global batch size: 32 | Global tokens/sec: 165157.83 | Global MFU (%): 38.67 | Global TFLOPs: 1529.60 | 
2025-05-19 08:58:08,881 - root - INFO - Step: 350 | Loss: 6.61 | Tokens per second: 41243.74 | Training tokens per second (%): 9.46 | MFU (%): 38.62 | TFLOPs: 381.98 | Global batch size: 32 | Global tokens/sec: 164974.97 | Global MFU (%): 38.62 | Global TFLOPs: 1527.90 | 
2025-05-19 08:58:12,857 - root - INFO - Step: 360 | Loss: 6.54 | Tokens per second: 41217.51 | Training tokens per second (%): 10.13 | MFU (%): 38.60 | TFLOPs: 381.73 | Global batch size: 32 | Global tokens/sec: 164870.04 | Global MFU (%): 38.60 | Global TFLOPs: 1526.93 | 
2025-05-19 08:58:16,837 - root - INFO - Step: 370 | Loss: 6.59 | Tokens per second: 41174.90 | Training tokens per second (%): 9.17 | MFU (%): 38.56 | TFLOPs: 381.34 | Global batch size: 32 | Global tokens/sec: 164699.58 | Global MFU (%): 38.56 | Global TFLOPs: 1525.35 | 
2025-05-19 08:58:20,809 - root - INFO - Step: 380 | Loss: 6.55 | Tokens per second: 41251.16 | Training tokens per second (%): 8.73 | MFU (%): 38.63 | TFLOPs: 382.04 | Global batch size: 32 | Global tokens/sec: 165004.66 | Global MFU (%): 38.63 | Global TFLOPs: 1528.18 | 
2025-05-19 08:58:24,787 - root - INFO - Step: 390 | Loss: 6.39 | Tokens per second: 41194.73 | Training tokens per second (%): 9.94 | MFU (%): 38.58 | TFLOPs: 381.52 | Global batch size: 32 | Global tokens/sec: 164778.91 | Global MFU (%): 38.58 | Global TFLOPs: 1526.09 | 
2025-05-19 08:58:28,757 - root - INFO - Step: 400 | Loss: 6.40 | Tokens per second: 41279.03 | Training tokens per second (%): 8.78 | MFU (%): 38.66 | TFLOPs: 382.30 | Global batch size: 32 | Global tokens/sec: 165116.13 | Global MFU (%): 38.66 | Global TFLOPs: 1529.21 | 
2025-05-19 08:58:32,726 - root - INFO - Step: 410 | Loss: 6.50 | Tokens per second: 41291.20 | Training tokens per second (%): 9.22 | MFU (%): 38.67 | TFLOPs: 382.42 | Global batch size: 32 | Global tokens/sec: 165164.79 | Global MFU (%): 38.67 | Global TFLOPs: 1529.66 | 
2025-05-19 08:58:36,700 - root - INFO - Step: 420 | Loss: 6.44 | Tokens per second: 41236.59 | Training tokens per second (%): 9.30 | MFU (%): 38.62 | TFLOPs: 381.91 | Global batch size: 32 | Global tokens/sec: 164946.37 | Global MFU (%): 38.62 | Global TFLOPs: 1527.64 | 
2025-05-19 08:58:40,680 - root - INFO - Step: 430 | Loss: 6.30 | Tokens per second: 41168.63 | Training tokens per second (%): 9.75 | MFU (%): 38.55 | TFLOPs: 381.28 | Global batch size: 32 | Global tokens/sec: 164674.53 | Global MFU (%): 38.55 | Global TFLOPs: 1525.12 | 
2025-05-19 08:58:44,658 - root - INFO - Step: 440 | Loss: 6.38 | Tokens per second: 41191.65 | Training tokens per second (%): 9.31 | MFU (%): 38.57 | TFLOPs: 381.49 | Global batch size: 32 | Global tokens/sec: 164766.58 | Global MFU (%): 38.57 | Global TFLOPs: 1525.97 | 
2025-05-19 08:58:48,641 - root - INFO - Step: 450 | Loss: 6.34 | Tokens per second: 41149.11 | Training tokens per second (%): 10.11 | MFU (%): 38.53 | TFLOPs: 381.10 | Global batch size: 32 | Global tokens/sec: 164596.43 | Global MFU (%): 38.53 | Global TFLOPs: 1524.40 | 
2025-05-19 08:58:52,617 - root - INFO - Step: 460 | Loss: 6.37 | Tokens per second: 41218.31 | Training tokens per second (%): 9.55 | MFU (%): 38.60 | TFLOPs: 381.74 | Global batch size: 32 | Global tokens/sec: 164873.26 | Global MFU (%): 38.60 | Global TFLOPs: 1526.96 | 
2025-05-19 08:58:56,598 - root - INFO - Step: 470 | Loss: 6.43 | Tokens per second: 41160.60 | Training tokens per second (%): 8.64 | MFU (%): 38.54 | TFLOPs: 381.21 | Global batch size: 32 | Global tokens/sec: 164642.40 | Global MFU (%): 38.54 | Global TFLOPs: 1524.82 | 
2025-05-19 08:59:00,583 - root - INFO - Step: 480 | Loss: 6.30 | Tokens per second: 41120.98 | Training tokens per second (%): 8.67 | MFU (%): 38.51 | TFLOPs: 380.84 | Global batch size: 32 | Global tokens/sec: 164483.91 | Global MFU (%): 38.51 | Global TFLOPs: 1523.36 | 
2025-05-19 08:59:04,548 - root - INFO - Step: 490 | Loss: 6.28 | Tokens per second: 41333.75 | Training tokens per second (%): 8.51 | MFU (%): 38.71 | TFLOPs: 382.81 | Global batch size: 32 | Global tokens/sec: 165334.99 | Global MFU (%): 38.71 | Global TFLOPs: 1531.24 | 
2025-05-19 08:59:08,523 - root - INFO - Step: 500 | Loss: 6.17 | Tokens per second: 41219.48 | Training tokens per second (%): 9.46 | MFU (%): 38.60 | TFLOPs: 381.75 | Global batch size: 32 | Global tokens/sec: 164877.94 | Global MFU (%): 38.60 | Global TFLOPs: 1527.01 | 
2025-05-19 08:59:12,502 - root - INFO - Step: 510 | Loss: 6.23 | Tokens per second: 41191.34 | Training tokens per second (%): 10.34 | MFU (%): 38.57 | TFLOPs: 381.49 | Global batch size: 32 | Global tokens/sec: 164765.34 | Global MFU (%): 38.57 | Global TFLOPs: 1525.96 | 
2025-05-19 08:59:16,476 - root - INFO - Step: 520 | Loss: 6.12 | Tokens per second: 41235.88 | Training tokens per second (%): 8.85 | MFU (%): 38.62 | TFLOPs: 381.90 | Global batch size: 32 | Global tokens/sec: 164943.53 | Global MFU (%): 38.62 | Global TFLOPs: 1527.61 | 
2025-05-19 08:59:20,451 - root - INFO - Step: 530 | Loss: 6.17 | Tokens per second: 41219.92 | Training tokens per second (%): 9.21 | MFU (%): 38.60 | TFLOPs: 381.76 | Global batch size: 32 | Global tokens/sec: 164879.68 | Global MFU (%): 38.60 | Global TFLOPs: 1527.02 | 
2025-05-19 08:59:24,433 - root - INFO - Step: 540 | Loss: 6.11 | Tokens per second: 41151.26 | Training tokens per second (%): 9.18 | MFU (%): 38.54 | TFLOPs: 381.12 | Global batch size: 32 | Global tokens/sec: 164605.05 | Global MFU (%): 38.54 | Global TFLOPs: 1524.48 | 
2025-05-19 08:59:28,412 - root - INFO - Step: 550 | Loss: 6.17 | Tokens per second: 41190.58 | Training tokens per second (%): 10.09 | MFU (%): 38.57 | TFLOPs: 381.48 | Global batch size: 32 | Global tokens/sec: 164762.33 | Global MFU (%): 38.57 | Global TFLOPs: 1525.93 | 
2025-05-19 08:59:32,396 - root - INFO - Step: 560 | Loss: 6.12 | Tokens per second: 41129.79 | Training tokens per second (%): 10.98 | MFU (%): 38.52 | TFLOPs: 380.92 | Global batch size: 32 | Global tokens/sec: 164519.16 | Global MFU (%): 38.52 | Global TFLOPs: 1523.68 | 
2025-05-19 08:59:36,376 - root - INFO - Step: 570 | Loss: 6.09 | Tokens per second: 41174.29 | Training tokens per second (%): 9.71 | MFU (%): 38.56 | TFLOPs: 381.33 | Global batch size: 32 | Global tokens/sec: 164697.16 | Global MFU (%): 38.56 | Global TFLOPs: 1525.33 | 
2025-05-19 08:59:40,348 - root - INFO - Step: 580 | Loss: 6.30 | Tokens per second: 41253.64 | Training tokens per second (%): 9.37 | MFU (%): 38.63 | TFLOPs: 382.07 | Global batch size: 32 | Global tokens/sec: 165014.56 | Global MFU (%): 38.63 | Global TFLOPs: 1528.27 | 
2025-05-19 08:59:44,318 - root - INFO - Step: 590 | Loss: 6.17 | Tokens per second: 41281.30 | Training tokens per second (%): 9.13 | MFU (%): 38.66 | TFLOPs: 382.32 | Global batch size: 32 | Global tokens/sec: 165125.19 | Global MFU (%): 38.66 | Global TFLOPs: 1529.30 | 
2025-05-19 08:59:48,302 - root - INFO - Step: 600 | Loss: 6.05 | Tokens per second: 41131.21 | Training tokens per second (%): 9.05 | MFU (%): 38.52 | TFLOPs: 380.93 | Global batch size: 32 | Global tokens/sec: 164524.85 | Global MFU (%): 38.52 | Global TFLOPs: 1523.74 | 
2025-05-19 08:59:52,272 - root - INFO - Step: 610 | Loss: 6.03 | Tokens per second: 41279.59 | Training tokens per second (%): 9.84 | MFU (%): 38.66 | TFLOPs: 382.31 | Global batch size: 32 | Global tokens/sec: 165118.37 | Global MFU (%): 38.66 | Global TFLOPs: 1529.23 | 
2025-05-19 08:59:56,249 - root - INFO - Step: 620 | Loss: 6.00 | Tokens per second: 41203.42 | Training tokens per second (%): 8.73 | MFU (%): 38.58 | TFLOPs: 381.60 | Global batch size: 32 | Global tokens/sec: 164813.68 | Global MFU (%): 38.58 | Global TFLOPs: 1526.41 | 
2025-05-19 09:00:00,229 - root - INFO - Step: 630 | Loss: 6.03 | Tokens per second: 41180.48 | Training tokens per second (%): 9.15 | MFU (%): 38.56 | TFLOPs: 381.39 | Global batch size: 32 | Global tokens/sec: 164721.92 | Global MFU (%): 38.56 | Global TFLOPs: 1525.56 | 
2025-05-19 09:00:04,201 - root - INFO - Step: 640 | Loss: 5.99 | Tokens per second: 41247.58 | Training tokens per second (%): 7.75 | MFU (%): 38.63 | TFLOPs: 382.01 | Global batch size: 32 | Global tokens/sec: 164990.34 | Global MFU (%): 38.63 | Global TFLOPs: 1528.05 | 
2025-05-19 09:00:08,166 - root - INFO - Step: 650 | Loss: 6.06 | Tokens per second: 41330.08 | Training tokens per second (%): 8.67 | MFU (%): 38.70 | TFLOPs: 382.78 | Global batch size: 32 | Global tokens/sec: 165320.30 | Global MFU (%): 38.70 | Global TFLOPs: 1531.10 | 
2025-05-19 09:00:12,140 - root - INFO - Step: 660 | Loss: 5.97 | Tokens per second: 41240.36 | Training tokens per second (%): 9.67 | MFU (%): 38.62 | TFLOPs: 381.94 | Global batch size: 32 | Global tokens/sec: 164961.42 | Global MFU (%): 38.62 | Global TFLOPs: 1527.78 | 
2025-05-19 09:00:16,119 - root - INFO - Step: 670 | Loss: 5.99 | Tokens per second: 41183.48 | Training tokens per second (%): 10.11 | MFU (%): 38.57 | TFLOPs: 381.42 | Global batch size: 32 | Global tokens/sec: 164733.90 | Global MFU (%): 38.57 | Global TFLOPs: 1525.67 | 
2025-05-19 09:00:20,100 - root - INFO - Step: 680 | Loss: 6.12 | Tokens per second: 41164.27 | Training tokens per second (%): 9.51 | MFU (%): 38.55 | TFLOPs: 381.24 | Global batch size: 32 | Global tokens/sec: 164657.09 | Global MFU (%): 38.55 | Global TFLOPs: 1524.96 | 
2025-05-19 09:00:24,074 - root - INFO - Step: 690 | Loss: 5.80 | Tokens per second: 41239.71 | Training tokens per second (%): 9.69 | MFU (%): 38.62 | TFLOPs: 381.94 | Global batch size: 32 | Global tokens/sec: 164958.83 | Global MFU (%): 38.62 | Global TFLOPs: 1527.75 | 
2025-05-19 09:00:28,043 - root - INFO - Step: 700 | Loss: 6.11 | Tokens per second: 41280.78 | Training tokens per second (%): 10.62 | MFU (%): 38.66 | TFLOPs: 382.32 | Global batch size: 32 | Global tokens/sec: 165123.13 | Global MFU (%): 38.66 | Global TFLOPs: 1529.28 | 
2025-05-19 09:00:32,019 - root - INFO - Step: 710 | Loss: 5.96 | Tokens per second: 41219.62 | Training tokens per second (%): 9.01 | MFU (%): 38.60 | TFLOPs: 381.75 | Global batch size: 32 | Global tokens/sec: 164878.47 | Global MFU (%): 38.60 | Global TFLOPs: 1527.01 | 
2025-05-19 09:00:35,998 - root - INFO - Step: 720 | Loss: 5.88 | Tokens per second: 41184.82 | Training tokens per second (%): 9.54 | MFU (%): 38.57 | TFLOPs: 381.43 | Global batch size: 32 | Global tokens/sec: 164739.27 | Global MFU (%): 38.57 | Global TFLOPs: 1525.72 | 
2025-05-19 09:00:39,969 - root - INFO - Step: 730 | Loss: 5.81 | Tokens per second: 41272.87 | Training tokens per second (%): 9.92 | MFU (%): 38.65 | TFLOPs: 382.25 | Global batch size: 32 | Global tokens/sec: 165091.47 | Global MFU (%): 38.65 | Global TFLOPs: 1528.98 | 
2025-05-19 09:00:43,947 - root - INFO - Step: 740 | Loss: 5.98 | Tokens per second: 41192.22 | Training tokens per second (%): 9.54 | MFU (%): 38.57 | TFLOPs: 381.50 | Global batch size: 32 | Global tokens/sec: 164768.88 | Global MFU (%): 38.57 | Global TFLOPs: 1526.00 | 
2025-05-19 09:00:47,924 - root - INFO - Step: 750 | Loss: 5.87 | Tokens per second: 41200.70 | Training tokens per second (%): 9.38 | MFU (%): 38.58 | TFLOPs: 381.58 | Global batch size: 32 | Global tokens/sec: 164802.82 | Global MFU (%): 38.58 | Global TFLOPs: 1526.31 | 
2025-05-19 09:00:51,910 - root - INFO - Step: 760 | Loss: 5.89 | Tokens per second: 41110.73 | Training tokens per second (%): 10.43 | MFU (%): 38.50 | TFLOPs: 380.74 | Global batch size: 32 | Global tokens/sec: 164442.94 | Global MFU (%): 38.50 | Global TFLOPs: 1522.98 | 
2025-05-19 09:00:55,891 - root - INFO - Step: 770 | Loss: 5.79 | Tokens per second: 41165.63 | Training tokens per second (%): 9.30 | MFU (%): 38.55 | TFLOPs: 381.25 | Global batch size: 32 | Global tokens/sec: 164662.54 | Global MFU (%): 38.55 | Global TFLOPs: 1525.01 | 
2025-05-19 09:00:59,860 - root - INFO - Step: 780 | Loss: 5.86 | Tokens per second: 41295.13 | Training tokens per second (%): 7.67 | MFU (%): 38.67 | TFLOPs: 382.45 | Global batch size: 32 | Global tokens/sec: 165180.51 | Global MFU (%): 38.67 | Global TFLOPs: 1529.81 | 
2025-05-19 09:01:03,831 - root - INFO - Step: 790 | Loss: 5.73 | Tokens per second: 41259.08 | Training tokens per second (%): 8.84 | MFU (%): 38.64 | TFLOPs: 382.12 | Global batch size: 32 | Global tokens/sec: 165036.30 | Global MFU (%): 38.64 | Global TFLOPs: 1528.47 | 
2025-05-19 09:01:07,811 - root - INFO - Step: 800 | Loss: 5.80 | Tokens per second: 41180.93 | Training tokens per second (%): 9.91 | MFU (%): 38.56 | TFLOPs: 381.39 | Global batch size: 32 | Global tokens/sec: 164723.70 | Global MFU (%): 38.56 | Global TFLOPs: 1525.58 | 
2025-05-19 09:01:11,796 - root - INFO - Step: 810 | Loss: 5.99 | Tokens per second: 41115.62 | Training tokens per second (%): 10.66 | MFU (%): 38.50 | TFLOPs: 380.79 | Global batch size: 32 | Global tokens/sec: 164462.47 | Global MFU (%): 38.50 | Global TFLOPs: 1523.16 | 
2025-05-19 09:01:15,767 - root - INFO - Step: 820 | Loss: 5.87 | Tokens per second: 41271.16 | Training tokens per second (%): 8.71 | MFU (%): 38.65 | TFLOPs: 382.23 | Global batch size: 32 | Global tokens/sec: 165084.63 | Global MFU (%): 38.65 | Global TFLOPs: 1528.92 | 
2025-05-19 09:01:19,743 - root - INFO - Step: 830 | Loss: 5.84 | Tokens per second: 41217.66 | Training tokens per second (%): 10.28 | MFU (%): 38.60 | TFLOPs: 381.73 | Global batch size: 32 | Global tokens/sec: 164870.63 | Global MFU (%): 38.60 | Global TFLOPs: 1526.94 | 
2025-05-19 09:01:23,722 - root - INFO - Step: 840 | Loss: 5.79 | Tokens per second: 41176.95 | Training tokens per second (%): 10.82 | MFU (%): 38.56 | TFLOPs: 381.36 | Global batch size: 32 | Global tokens/sec: 164707.79 | Global MFU (%): 38.56 | Global TFLOPs: 1525.43 | 
2025-05-19 09:01:27,699 - root - INFO - Step: 850 | Loss: 5.81 | Tokens per second: 41211.60 | Training tokens per second (%): 9.41 | MFU (%): 38.59 | TFLOPs: 381.68 | Global batch size: 32 | Global tokens/sec: 164846.41 | Global MFU (%): 38.59 | Global TFLOPs: 1526.71 | 
2025-05-19 09:01:31,674 - root - INFO - Step: 860 | Loss: 5.76 | Tokens per second: 41221.74 | Training tokens per second (%): 9.70 | MFU (%): 38.60 | TFLOPs: 381.77 | Global batch size: 32 | Global tokens/sec: 164886.96 | Global MFU (%): 38.60 | Global TFLOPs: 1527.09 | 
2025-05-19 09:01:35,643 - root - INFO - Step: 870 | Loss: 5.67 | Tokens per second: 41288.88 | Training tokens per second (%): 9.70 | MFU (%): 38.66 | TFLOPs: 382.39 | Global batch size: 32 | Global tokens/sec: 165155.50 | Global MFU (%): 38.66 | Global TFLOPs: 1529.58 | 
2025-05-19 09:01:39,623 - root - INFO - Step: 880 | Loss: 5.63 | Tokens per second: 41176.43 | Training tokens per second (%): 9.96 | MFU (%): 38.56 | TFLOPs: 381.35 | Global batch size: 32 | Global tokens/sec: 164705.73 | Global MFU (%): 38.56 | Global TFLOPs: 1525.41 | 
2025-05-19 09:01:43,595 - root - INFO - Step: 890 | Loss: 5.77 | Tokens per second: 41255.17 | Training tokens per second (%): 9.77 | MFU (%): 38.63 | TFLOPs: 382.08 | Global batch size: 32 | Global tokens/sec: 165020.68 | Global MFU (%): 38.63 | Global TFLOPs: 1528.33 | 
2025-05-19 09:01:47,575 - root - INFO - Step: 900 | Loss: 5.62 | Tokens per second: 41174.58 | Training tokens per second (%): 10.86 | MFU (%): 38.56 | TFLOPs: 381.34 | Global batch size: 32 | Global tokens/sec: 164698.31 | Global MFU (%): 38.56 | Global TFLOPs: 1525.34 | 
2025-05-19 09:01:51,547 - root - INFO - Step: 910 | Loss: 5.66 | Tokens per second: 41259.33 | Training tokens per second (%): 9.45 | MFU (%): 38.64 | TFLOPs: 382.12 | Global batch size: 32 | Global tokens/sec: 165037.33 | Global MFU (%): 38.64 | Global TFLOPs: 1528.48 | 
2025-05-19 09:01:55,520 - root - INFO - Step: 920 | Loss: 5.83 | Tokens per second: 41240.66 | Training tokens per second (%): 10.12 | MFU (%): 38.62 | TFLOPs: 381.95 | Global batch size: 32 | Global tokens/sec: 164962.63 | Global MFU (%): 38.62 | Global TFLOPs: 1527.79 | 
2025-05-19 09:01:59,500 - root - INFO - Step: 930 | Loss: 5.71 | Tokens per second: 41171.06 | Training tokens per second (%): 9.82 | MFU (%): 38.55 | TFLOPs: 381.30 | Global batch size: 32 | Global tokens/sec: 164684.22 | Global MFU (%): 38.55 | Global TFLOPs: 1525.21 | 
2025-05-19 09:02:03,475 - root - INFO - Step: 940 | Loss: 5.70 | Tokens per second: 41234.06 | Training tokens per second (%): 10.02 | MFU (%): 38.61 | TFLOPs: 381.89 | Global batch size: 32 | Global tokens/sec: 164936.25 | Global MFU (%): 38.61 | Global TFLOPs: 1527.55 | 
2025-05-19 09:02:07,444 - root - INFO - Step: 950 | Loss: 5.72 | Tokens per second: 41287.00 | Training tokens per second (%): 10.52 | MFU (%): 38.66 | TFLOPs: 382.38 | Global batch size: 32 | Global tokens/sec: 165147.99 | Global MFU (%): 38.66 | Global TFLOPs: 1529.51 | 
2025-05-19 09:02:11,419 - root - INFO - Step: 960 | Loss: 5.85 | Tokens per second: 41220.53 | Training tokens per second (%): 8.11 | MFU (%): 38.60 | TFLOPs: 381.76 | Global batch size: 32 | Global tokens/sec: 164882.14 | Global MFU (%): 38.60 | Global TFLOPs: 1527.04 | 
2025-05-19 09:02:15,399 - root - INFO - Step: 970 | Loss: 5.74 | Tokens per second: 41177.22 | Training tokens per second (%): 8.88 | MFU (%): 38.56 | TFLOPs: 381.36 | Global batch size: 32 | Global tokens/sec: 164708.87 | Global MFU (%): 38.56 | Global TFLOPs: 1525.44 | 
2025-05-19 09:02:19,362 - root - INFO - Step: 980 | Loss: 5.72 | Tokens per second: 41349.15 | Training tokens per second (%): 8.20 | MFU (%): 38.72 | TFLOPs: 382.95 | Global batch size: 32 | Global tokens/sec: 165396.59 | Global MFU (%): 38.72 | Global TFLOPs: 1531.81 | 
2025-05-19 09:02:23,337 - root - INFO - Step: 990 | Loss: 5.62 | Tokens per second: 41224.95 | Training tokens per second (%): 9.14 | MFU (%): 38.60 | TFLOPs: 381.80 | Global batch size: 32 | Global tokens/sec: 164899.81 | Global MFU (%): 38.60 | Global TFLOPs: 1527.21 | 
2025-05-19 09:02:27,316 - root - INFO - Step: 1000 | Loss: 5.66 | Tokens per second: 41183.72 | Training tokens per second (%): 8.60 | MFU (%): 38.57 | TFLOPs: 381.42 | Global batch size: 32 | Global tokens/sec: 164734.89 | Global MFU (%): 38.57 | Global TFLOPs: 1525.68 | 
2025-05-19 09:02:27,316 - root - INFO - Training completed
[sbatch-master] task finished
