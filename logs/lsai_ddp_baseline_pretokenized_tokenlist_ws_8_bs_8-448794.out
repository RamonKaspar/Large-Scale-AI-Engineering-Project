[sbatch-master] running on nid006647
[sbatch-master] SLURM_NODELIST: nid[006647,006651]
[sbatch-master] SLURM_NNODES: 2
[sbatch-master] SLURM_NODEID: 0
[Master] Master node: nid006647
[Master] World size: 8
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
[srun] rank=0 host=nid006647 noderank=0 localrank=0
[srun] rank=1 host=nid006651 noderank=1 localrank=0
W0519 11:11:52.769000 166596 torch/distributed/run.py:792] 
W0519 11:11:52.769000 166596 torch/distributed/run.py:792] *****************************************
W0519 11:11:52.769000 166596 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0519 11:11:52.769000 166596 torch/distributed/run.py:792] *****************************************
W0519 11:11:53.344000 134125 torch/distributed/run.py:792] 
W0519 11:11:53.344000 134125 torch/distributed/run.py:792] *****************************************
W0519 11:11:53.344000 134125 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0519 11:11:53.344000 134125 torch/distributed/run.py:792] *****************************************
2025-05-19 11:12:05,582 - root - INFO - [Distributed Init] Rank 4 initialized on node 1 on GPU 0.
2025-05-19 11:12:05,606 - root - INFO - [Distributed Init] Rank 0 initialized on node 0 on GPU 0.
[rank4]:[W519 11:12:05.968482664 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank0]:[W519 11:12:06.333071643 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:12:06,153 - root - INFO - [Distributed Init] Rank 6 initialized on node 1 on GPU 2.
[rank6]:[W519 11:12:06.112985650 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 6]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:12:06,173 - root - INFO - [Distributed Init] Rank 7 initialized on node 1 on GPU 3.
2025-05-19 11:12:06,173 - root - INFO - [Distributed Init] Rank 3 initialized on node 0 on GPU 3.
2025-05-19 11:12:06,173 - root - INFO - [Distributed Init] Rank 2 initialized on node 0 on GPU 2.
[rank7]:[W519 11:12:06.132856185 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 7]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W519 11:12:06.473324464 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W519 11:12:06.473324336 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:12:06,182 - root - INFO - [Distributed Init] Rank 5 initialized on node 1 on GPU 1.
[rank5]:[W519 11:12:06.142009526 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 5]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:12:06,192 - root - INFO - [Distributed Init] Rank 1 initialized on node 0 on GPU 1.
[rank1]:[W519 11:12:06.492037767 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:12:11,079 - root - INFO - [Rank 0] All ranks ready!
2025-05-19 11:12:11,079 - root - INFO - Distributed training enabled: 8 processes
2025-05-19 11:12:11,079 - root - INFO - Master process: 0 on cuda:0
2025-05-19 11:12:11,079 - root - INFO - Experiment args: Namespace(dataset='/capstor/scratch/cscs/kasparr/project/train_data_tokenized_token-list_snappy.parquet', dataset_type='token-list', pretokenized=True, tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=10, profile=False, profile_step_start=10, profile_step_end=10, grad_max_norm=1, model_dtype='bf16', compile=False, distributed=True, backend='nccl', find_unused_parameters=False)
2025-05-19 11:12:11,079 - root - INFO - Setting up Tokenizer...
2025-05-19 11:12:11,624 - root - INFO - Setting up DataLoaders...
2025-05-19 11:12:11,625 - root - INFO - Using pretokenized data: /capstor/scratch/cscs/kasparr/project/train_data_tokenized_token-list_snappy.parquet
2025-05-19 11:12:19,821 - root - INFO - Setting up Model...
2025-05-19 11:12:32,798 - root - INFO - Model wrapped with DistributedDataParallel
2025-05-19 11:12:32,799 - root - INFO - Global batch size: 64 (local: 8 Ã— 8 processes)
2025-05-19 11:12:32,799 - root - INFO - Starting training!
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
2025-05-19 11:12:34,539 - root - INFO - Step: 1 | Loss: 11.94 | Tokens per second: 9419.79 | Training tokens per second (%): 12.47 | MFU (%): 8.82 | TFLOPs: 87.24 | Global batch size: 64 | Global tokens/sec: 75358.36 | Global MFU (%): 8.82 | Global TFLOPs: 697.93 | 
2025-05-19 11:12:38,382 - root - INFO - Step: 10 | Loss: 11.91 | Tokens per second: 38374.95 | Training tokens per second (%): 12.47 | MFU (%): 35.94 | TFLOPs: 355.41 | Global batch size: 64 | Global tokens/sec: 306999.63 | Global MFU (%): 35.94 | Global TFLOPs: 2843.26 | 
2025-05-19 11:12:42,603 - root - INFO - Step: 20 | Loss: 11.67 | Tokens per second: 38830.68 | Training tokens per second (%): 12.47 | MFU (%): 36.36 | TFLOPs: 359.63 | Global batch size: 64 | Global tokens/sec: 310645.44 | Global MFU (%): 36.36 | Global TFLOPs: 2877.02 | 
2025-05-19 11:12:46,828 - root - INFO - Step: 30 | Loss: 11.02 | Tokens per second: 38780.03 | Training tokens per second (%): 12.47 | MFU (%): 36.32 | TFLOPs: 359.16 | Global batch size: 64 | Global tokens/sec: 310240.21 | Global MFU (%): 36.32 | Global TFLOPs: 2873.27 | 
2025-05-19 11:12:51,039 - root - INFO - Step: 40 | Loss: 9.99 | Tokens per second: 38916.16 | Training tokens per second (%): 12.47 | MFU (%): 36.44 | TFLOPs: 360.42 | Global batch size: 64 | Global tokens/sec: 311329.30 | Global MFU (%): 36.44 | Global TFLOPs: 2883.35 | 
2025-05-19 11:12:55,265 - root - INFO - Step: 50 | Loss: 9.32 | Tokens per second: 38771.25 | Training tokens per second (%): 12.47 | MFU (%): 36.31 | TFLOPs: 359.08 | Global batch size: 64 | Global tokens/sec: 310170.00 | Global MFU (%): 36.31 | Global TFLOPs: 2872.62 | 
2025-05-19 11:12:59,490 - root - INFO - Step: 60 | Loss: 8.75 | Tokens per second: 38785.83 | Training tokens per second (%): 12.47 | MFU (%): 36.32 | TFLOPs: 359.21 | Global batch size: 64 | Global tokens/sec: 310286.64 | Global MFU (%): 36.32 | Global TFLOPs: 2873.70 | 
2025-05-19 11:13:03,712 - root - INFO - Step: 70 | Loss: 8.21 | Tokens per second: 38819.22 | Training tokens per second (%): 12.47 | MFU (%): 36.35 | TFLOPs: 359.52 | Global batch size: 64 | Global tokens/sec: 310553.76 | Global MFU (%): 36.35 | Global TFLOPs: 2876.17 | 
2025-05-19 11:13:07,967 - root - INFO - Step: 80 | Loss: 7.83 | Tokens per second: 38510.87 | Training tokens per second (%): 12.47 | MFU (%): 36.06 | TFLOPs: 356.67 | Global batch size: 64 | Global tokens/sec: 308086.98 | Global MFU (%): 36.06 | Global TFLOPs: 2853.33 | 
2025-05-19 11:13:12,180 - root - INFO - Step: 90 | Loss: 7.47 | Tokens per second: 38898.18 | Training tokens per second (%): 12.47 | MFU (%): 36.43 | TFLOPs: 360.25 | Global batch size: 64 | Global tokens/sec: 311185.43 | Global MFU (%): 36.43 | Global TFLOPs: 2882.02 | 
2025-05-19 11:13:16,407 - root - INFO - Step: 100 | Loss: 7.38 | Tokens per second: 38766.42 | Training tokens per second (%): 12.47 | MFU (%): 36.30 | TFLOPs: 359.03 | Global batch size: 64 | Global tokens/sec: 310131.37 | Global MFU (%): 36.30 | Global TFLOPs: 2872.26 | 
2025-05-19 11:13:20,629 - root - INFO - Step: 110 | Loss: 7.22 | Tokens per second: 38812.74 | Training tokens per second (%): 12.47 | MFU (%): 36.35 | TFLOPs: 359.46 | Global batch size: 64 | Global tokens/sec: 310501.90 | Global MFU (%): 36.35 | Global TFLOPs: 2875.69 | 
2025-05-19 11:13:24,845 - root - INFO - Step: 120 | Loss: 7.18 | Tokens per second: 38865.91 | Training tokens per second (%): 12.47 | MFU (%): 36.40 | TFLOPs: 359.95 | Global batch size: 64 | Global tokens/sec: 310927.31 | Global MFU (%): 36.40 | Global TFLOPs: 2879.63 | 
2025-05-19 11:13:29,078 - root - INFO - Step: 130 | Loss: 7.09 | Tokens per second: 38715.87 | Training tokens per second (%): 12.47 | MFU (%): 36.26 | TFLOPs: 358.56 | Global batch size: 64 | Global tokens/sec: 309726.96 | Global MFU (%): 36.26 | Global TFLOPs: 2868.51 | 
2025-05-19 11:13:33,294 - root - INFO - Step: 140 | Loss: 7.00 | Tokens per second: 38864.53 | Training tokens per second (%): 12.47 | MFU (%): 36.39 | TFLOPs: 359.94 | Global batch size: 64 | Global tokens/sec: 310916.24 | Global MFU (%): 36.39 | Global TFLOPs: 2879.53 | 
2025-05-19 11:13:37,505 - root - INFO - Step: 150 | Loss: 6.93 | Tokens per second: 38917.40 | Training tokens per second (%): 12.47 | MFU (%): 36.44 | TFLOPs: 360.43 | Global batch size: 64 | Global tokens/sec: 311339.19 | Global MFU (%): 36.44 | Global TFLOPs: 2883.45 | 
2025-05-19 11:13:41,823 - root - INFO - Step: 160 | Loss: 6.86 | Tokens per second: 37947.12 | Training tokens per second (%): 12.47 | MFU (%): 35.54 | TFLOPs: 351.44 | Global batch size: 64 | Global tokens/sec: 303576.98 | Global MFU (%): 35.54 | Global TFLOPs: 2811.56 | 
2025-05-19 11:13:46,040 - root - INFO - Step: 170 | Loss: 6.81 | Tokens per second: 38860.87 | Training tokens per second (%): 12.47 | MFU (%): 36.39 | TFLOPs: 359.91 | Global batch size: 64 | Global tokens/sec: 310887.00 | Global MFU (%): 36.39 | Global TFLOPs: 2879.26 | 
2025-05-19 11:13:50,312 - root - INFO - Step: 180 | Loss: 6.73 | Tokens per second: 38352.32 | Training tokens per second (%): 12.47 | MFU (%): 35.91 | TFLOPs: 355.20 | Global batch size: 64 | Global tokens/sec: 306818.53 | Global MFU (%): 35.91 | Global TFLOPs: 2841.58 | 
2025-05-19 11:13:54,527 - root - INFO - Step: 190 | Loss: 6.73 | Tokens per second: 38884.03 | Training tokens per second (%): 12.47 | MFU (%): 36.41 | TFLOPs: 360.12 | Global batch size: 64 | Global tokens/sec: 311072.26 | Global MFU (%): 36.41 | Global TFLOPs: 2880.97 | 
2025-05-19 11:13:58,747 - root - INFO - Step: 200 | Loss: 6.73 | Tokens per second: 38830.87 | Training tokens per second (%): 12.47 | MFU (%): 36.36 | TFLOPs: 359.63 | Global batch size: 64 | Global tokens/sec: 310646.96 | Global MFU (%): 36.36 | Global TFLOPs: 2877.03 | 
2025-05-19 11:14:02,968 - root - INFO - Step: 210 | Loss: 6.59 | Tokens per second: 38823.73 | Training tokens per second (%): 12.46 | MFU (%): 36.36 | TFLOPs: 359.56 | Global batch size: 64 | Global tokens/sec: 310589.83 | Global MFU (%): 36.36 | Global TFLOPs: 2876.51 | 
2025-05-19 11:14:07,184 - root - INFO - Step: 220 | Loss: 6.56 | Tokens per second: 38862.46 | Training tokens per second (%): 12.47 | MFU (%): 36.39 | TFLOPs: 359.92 | Global batch size: 64 | Global tokens/sec: 310899.69 | Global MFU (%): 36.39 | Global TFLOPs: 2879.38 | 
2025-05-19 11:14:11,444 - root - INFO - Step: 230 | Loss: 6.54 | Tokens per second: 38470.61 | Training tokens per second (%): 12.47 | MFU (%): 36.03 | TFLOPs: 356.29 | Global batch size: 64 | Global tokens/sec: 307764.87 | Global MFU (%): 36.03 | Global TFLOPs: 2850.34 | 
2025-05-19 11:14:15,664 - root - INFO - Step: 240 | Loss: 6.45 | Tokens per second: 38834.25 | Training tokens per second (%): 12.47 | MFU (%): 36.37 | TFLOPs: 359.66 | Global batch size: 64 | Global tokens/sec: 310674.00 | Global MFU (%): 36.37 | Global TFLOPs: 2877.29 | 
2025-05-19 11:14:19,897 - root - INFO - Step: 250 | Loss: 6.43 | Tokens per second: 38710.23 | Training tokens per second (%): 12.47 | MFU (%): 36.25 | TFLOPs: 358.51 | Global batch size: 64 | Global tokens/sec: 309681.83 | Global MFU (%): 36.25 | Global TFLOPs: 2868.10 | 
2025-05-19 11:14:24,123 - root - INFO - Step: 260 | Loss: 6.43 | Tokens per second: 38776.55 | Training tokens per second (%): 12.47 | MFU (%): 36.31 | TFLOPs: 359.13 | Global batch size: 64 | Global tokens/sec: 310212.43 | Global MFU (%): 36.31 | Global TFLOPs: 2873.01 | 
2025-05-19 11:14:28,338 - root - INFO - Step: 270 | Loss: 6.37 | Tokens per second: 38873.54 | Training tokens per second (%): 12.47 | MFU (%): 36.40 | TFLOPs: 360.02 | Global batch size: 64 | Global tokens/sec: 310988.36 | Global MFU (%): 36.40 | Global TFLOPs: 2880.20 | 
2025-05-19 11:14:32,577 - root - INFO - Step: 280 | Loss: 6.41 | Tokens per second: 38658.16 | Training tokens per second (%): 12.47 | MFU (%): 36.20 | TFLOPs: 358.03 | Global batch size: 64 | Global tokens/sec: 309265.29 | Global MFU (%): 36.20 | Global TFLOPs: 2864.24 | 
2025-05-19 11:14:36,801 - root - INFO - Step: 290 | Loss: 6.34 | Tokens per second: 38792.36 | Training tokens per second (%): 12.47 | MFU (%): 36.33 | TFLOPs: 359.27 | Global batch size: 64 | Global tokens/sec: 310338.84 | Global MFU (%): 36.33 | Global TFLOPs: 2874.18 | 
2025-05-19 11:14:41,033 - root - INFO - Step: 300 | Loss: 6.32 | Tokens per second: 38722.28 | Training tokens per second (%): 12.47 | MFU (%): 36.26 | TFLOPs: 358.62 | Global batch size: 64 | Global tokens/sec: 309778.21 | Global MFU (%): 36.26 | Global TFLOPs: 2868.99 | 
2025-05-19 11:14:45,255 - root - INFO - Step: 310 | Loss: 6.29 | Tokens per second: 38811.29 | Training tokens per second (%): 12.47 | MFU (%): 36.34 | TFLOPs: 359.45 | Global batch size: 64 | Global tokens/sec: 310490.35 | Global MFU (%): 36.34 | Global TFLOPs: 2875.58 | 
2025-05-19 11:14:49,472 - root - INFO - Step: 320 | Loss: 6.24 | Tokens per second: 38860.29 | Training tokens per second (%): 12.47 | MFU (%): 36.39 | TFLOPs: 359.90 | Global batch size: 64 | Global tokens/sec: 310882.30 | Global MFU (%): 36.39 | Global TFLOPs: 2879.21 | 
2025-05-19 11:14:53,694 - root - INFO - Step: 330 | Loss: 6.22 | Tokens per second: 38810.78 | Training tokens per second (%): 12.47 | MFU (%): 36.34 | TFLOPs: 359.44 | Global batch size: 64 | Global tokens/sec: 310486.22 | Global MFU (%): 36.34 | Global TFLOPs: 2875.55 | 
2025-05-19 11:14:57,923 - root - INFO - Step: 340 | Loss: 6.25 | Tokens per second: 38750.62 | Training tokens per second (%): 12.47 | MFU (%): 36.29 | TFLOPs: 358.89 | Global batch size: 64 | Global tokens/sec: 310004.97 | Global MFU (%): 36.29 | Global TFLOPs: 2871.09 | 
2025-05-19 11:15:02,142 - root - INFO - Step: 350 | Loss: 6.15 | Tokens per second: 38837.89 | Training tokens per second (%): 12.47 | MFU (%): 36.37 | TFLOPs: 359.69 | Global batch size: 64 | Global tokens/sec: 310703.13 | Global MFU (%): 36.37 | Global TFLOPs: 2877.55 | 
2025-05-19 11:15:06,364 - root - INFO - Step: 360 | Loss: 6.20 | Tokens per second: 38817.82 | Training tokens per second (%): 12.47 | MFU (%): 36.35 | TFLOPs: 359.51 | Global batch size: 64 | Global tokens/sec: 310542.52 | Global MFU (%): 36.35 | Global TFLOPs: 2876.07 | 
2025-05-19 11:15:10,590 - root - INFO - Step: 370 | Loss: 6.17 | Tokens per second: 38778.41 | Training tokens per second (%): 12.47 | MFU (%): 36.31 | TFLOPs: 359.14 | Global batch size: 64 | Global tokens/sec: 310227.25 | Global MFU (%): 36.31 | Global TFLOPs: 2873.15 | 
2025-05-19 11:15:14,817 - root - INFO - Step: 380 | Loss: 6.16 | Tokens per second: 38765.57 | Training tokens per second (%): 12.47 | MFU (%): 36.30 | TFLOPs: 359.02 | Global batch size: 64 | Global tokens/sec: 310124.53 | Global MFU (%): 36.30 | Global TFLOPs: 2872.20 | 
2025-05-19 11:15:19,033 - root - INFO - Step: 390 | Loss: 6.31 | Tokens per second: 38871.39 | Training tokens per second (%): 12.47 | MFU (%): 36.40 | TFLOPs: 360.00 | Global batch size: 64 | Global tokens/sec: 310971.13 | Global MFU (%): 36.40 | Global TFLOPs: 2880.04 | 
2025-05-19 11:15:23,250 - root - INFO - Step: 400 | Loss: 6.19 | Tokens per second: 38852.53 | Training tokens per second (%): 12.47 | MFU (%): 36.38 | TFLOPs: 359.83 | Global batch size: 64 | Global tokens/sec: 310820.24 | Global MFU (%): 36.38 | Global TFLOPs: 2878.64 | 
2025-05-19 11:15:27,471 - root - INFO - Step: 410 | Loss: 6.18 | Tokens per second: 38827.28 | Training tokens per second (%): 12.47 | MFU (%): 36.36 | TFLOPs: 359.60 | Global batch size: 64 | Global tokens/sec: 310618.27 | Global MFU (%): 36.36 | Global TFLOPs: 2876.77 | 
2025-05-19 11:15:31,696 - root - INFO - Step: 420 | Loss: 6.07 | Tokens per second: 38777.65 | Training tokens per second (%): 12.47 | MFU (%): 36.31 | TFLOPs: 359.14 | Global batch size: 64 | Global tokens/sec: 310221.23 | Global MFU (%): 36.31 | Global TFLOPs: 2873.09 | 
2025-05-19 11:15:35,913 - root - INFO - Step: 430 | Loss: 6.12 | Tokens per second: 38866.58 | Training tokens per second (%): 12.47 | MFU (%): 36.40 | TFLOPs: 359.96 | Global batch size: 64 | Global tokens/sec: 310932.65 | Global MFU (%): 36.40 | Global TFLOPs: 2879.68 | 
2025-05-19 11:15:40,137 - root - INFO - Step: 440 | Loss: 6.02 | Tokens per second: 38792.41 | Training tokens per second (%): 12.47 | MFU (%): 36.33 | TFLOPs: 359.27 | Global batch size: 64 | Global tokens/sec: 310339.30 | Global MFU (%): 36.33 | Global TFLOPs: 2874.19 | 
2025-05-19 11:15:44,358 - root - INFO - Step: 450 | Loss: 6.00 | Tokens per second: 38819.64 | Training tokens per second (%): 12.46 | MFU (%): 36.35 | TFLOPs: 359.53 | Global batch size: 64 | Global tokens/sec: 310557.11 | Global MFU (%): 36.35 | Global TFLOPs: 2876.20 | 
2025-05-19 11:15:48,591 - root - INFO - Step: 460 | Loss: 6.03 | Tokens per second: 38712.93 | Training tokens per second (%): 12.47 | MFU (%): 36.25 | TFLOPs: 358.54 | Global batch size: 64 | Global tokens/sec: 309703.41 | Global MFU (%): 36.25 | Global TFLOPs: 2868.30 | 
2025-05-19 11:15:52,815 - root - INFO - Step: 470 | Loss: 5.99 | Tokens per second: 38790.92 | Training tokens per second (%): 12.47 | MFU (%): 36.33 | TFLOPs: 359.26 | Global batch size: 64 | Global tokens/sec: 310327.37 | Global MFU (%): 36.33 | Global TFLOPs: 2874.07 | 
2025-05-19 11:15:57,046 - root - INFO - Step: 480 | Loss: 6.00 | Tokens per second: 38736.02 | Training tokens per second (%): 12.47 | MFU (%): 36.27 | TFLOPs: 358.75 | Global batch size: 64 | Global tokens/sec: 309888.18 | Global MFU (%): 36.27 | Global TFLOPs: 2870.01 | 
2025-05-19 11:16:01,273 - root - INFO - Step: 490 | Loss: 5.98 | Tokens per second: 38763.19 | Training tokens per second (%): 12.47 | MFU (%): 36.30 | TFLOPs: 359.00 | Global batch size: 64 | Global tokens/sec: 310105.51 | Global MFU (%): 36.30 | Global TFLOPs: 2872.02 | 
2025-05-19 11:16:05,495 - root - INFO - Step: 500 | Loss: 5.91 | Tokens per second: 38815.02 | Training tokens per second (%): 12.47 | MFU (%): 36.35 | TFLOPs: 359.48 | Global batch size: 64 | Global tokens/sec: 310520.13 | Global MFU (%): 36.35 | Global TFLOPs: 2875.86 | 
2025-05-19 11:16:09,723 - root - INFO - Step: 510 | Loss: 5.90 | Tokens per second: 38759.26 | Training tokens per second (%): 12.47 | MFU (%): 36.30 | TFLOPs: 358.97 | Global batch size: 64 | Global tokens/sec: 310074.11 | Global MFU (%): 36.30 | Global TFLOPs: 2871.73 | 
2025-05-19 11:16:13,948 - root - INFO - Step: 520 | Loss: 5.96 | Tokens per second: 38786.77 | Training tokens per second (%): 12.47 | MFU (%): 36.32 | TFLOPs: 359.22 | Global batch size: 64 | Global tokens/sec: 310294.19 | Global MFU (%): 36.32 | Global TFLOPs: 2873.77 | 
2025-05-19 11:16:18,179 - root - INFO - Step: 530 | Loss: 5.92 | Tokens per second: 38727.31 | Training tokens per second (%): 12.47 | MFU (%): 36.27 | TFLOPs: 358.67 | Global batch size: 64 | Global tokens/sec: 309818.48 | Global MFU (%): 36.27 | Global TFLOPs: 2869.36 | 
2025-05-19 11:16:22,403 - root - INFO - Step: 540 | Loss: 5.87 | Tokens per second: 38793.40 | Training tokens per second (%): 12.47 | MFU (%): 36.33 | TFLOPs: 359.28 | Global batch size: 64 | Global tokens/sec: 310347.17 | Global MFU (%): 36.33 | Global TFLOPs: 2874.26 | 
2025-05-19 11:16:26,627 - root - INFO - Step: 550 | Loss: 5.93 | Tokens per second: 38789.85 | Training tokens per second (%): 12.47 | MFU (%): 36.32 | TFLOPs: 359.25 | Global batch size: 64 | Global tokens/sec: 310318.82 | Global MFU (%): 36.32 | Global TFLOPs: 2874.00 | 
2025-05-19 11:16:30,859 - root - INFO - Step: 560 | Loss: 5.87 | Tokens per second: 38723.03 | Training tokens per second (%): 12.47 | MFU (%): 36.26 | TFLOPs: 358.63 | Global batch size: 64 | Global tokens/sec: 309784.24 | Global MFU (%): 36.26 | Global TFLOPs: 2869.04 | 
2025-05-19 11:16:35,085 - root - INFO - Step: 570 | Loss: 5.87 | Tokens per second: 38773.55 | Training tokens per second (%): 12.47 | MFU (%): 36.31 | TFLOPs: 359.10 | Global batch size: 64 | Global tokens/sec: 310188.38 | Global MFU (%): 36.31 | Global TFLOPs: 2872.79 | 
2025-05-19 11:16:39,312 - root - INFO - Step: 580 | Loss: 5.89 | Tokens per second: 38773.66 | Training tokens per second (%): 12.47 | MFU (%): 36.31 | TFLOPs: 359.10 | Global batch size: 64 | Global tokens/sec: 310189.26 | Global MFU (%): 36.31 | Global TFLOPs: 2872.80 | 
2025-05-19 11:16:43,537 - root - INFO - Step: 590 | Loss: 5.85 | Tokens per second: 38782.33 | Training tokens per second (%): 12.47 | MFU (%): 36.32 | TFLOPs: 359.18 | Global batch size: 64 | Global tokens/sec: 310258.66 | Global MFU (%): 36.32 | Global TFLOPs: 2873.44 | 
2025-05-19 11:16:47,760 - root - INFO - Step: 600 | Loss: 5.78 | Tokens per second: 38802.66 | Training tokens per second (%): 12.47 | MFU (%): 36.34 | TFLOPs: 359.37 | Global batch size: 64 | Global tokens/sec: 310421.30 | Global MFU (%): 36.34 | Global TFLOPs: 2874.94 | 
2025-05-19 11:16:51,987 - root - INFO - Step: 610 | Loss: 5.81 | Tokens per second: 38765.21 | Training tokens per second (%): 12.47 | MFU (%): 36.30 | TFLOPs: 359.02 | Global batch size: 64 | Global tokens/sec: 310121.71 | Global MFU (%): 36.30 | Global TFLOPs: 2872.17 | 
2025-05-19 11:16:56,211 - root - INFO - Step: 620 | Loss: 5.80 | Tokens per second: 38798.74 | Training tokens per second (%): 12.47 | MFU (%): 36.33 | TFLOPs: 359.33 | Global batch size: 64 | Global tokens/sec: 310389.89 | Global MFU (%): 36.33 | Global TFLOPs: 2874.65 | 
2025-05-19 11:17:00,430 - root - INFO - Step: 630 | Loss: 5.73 | Tokens per second: 38841.96 | Training tokens per second (%): 12.47 | MFU (%): 36.37 | TFLOPs: 359.73 | Global batch size: 64 | Global tokens/sec: 310735.68 | Global MFU (%): 36.37 | Global TFLOPs: 2877.86 | 
2025-05-19 11:17:04,662 - root - INFO - Step: 640 | Loss: 5.76 | Tokens per second: 38718.15 | Training tokens per second (%): 12.47 | MFU (%): 36.26 | TFLOPs: 358.59 | Global batch size: 64 | Global tokens/sec: 309745.22 | Global MFU (%): 36.26 | Global TFLOPs: 2868.68 | 
2025-05-19 11:17:08,886 - root - INFO - Step: 650 | Loss: 5.72 | Tokens per second: 38790.28 | Training tokens per second (%): 12.47 | MFU (%): 36.32 | TFLOPs: 359.25 | Global batch size: 64 | Global tokens/sec: 310322.25 | Global MFU (%): 36.32 | Global TFLOPs: 2874.03 | 
2025-05-19 11:17:13,107 - root - INFO - Step: 660 | Loss: 5.73 | Tokens per second: 38821.66 | Training tokens per second (%): 12.47 | MFU (%): 36.35 | TFLOPs: 359.54 | Global batch size: 64 | Global tokens/sec: 310573.28 | Global MFU (%): 36.35 | Global TFLOPs: 2876.35 | 
2025-05-19 11:17:17,330 - root - INFO - Step: 670 | Loss: 5.78 | Tokens per second: 38809.58 | Training tokens per second (%): 12.47 | MFU (%): 36.34 | TFLOPs: 359.43 | Global batch size: 64 | Global tokens/sec: 310476.65 | Global MFU (%): 36.34 | Global TFLOPs: 2875.46 | 
2025-05-19 11:17:21,558 - root - INFO - Step: 680 | Loss: 5.74 | Tokens per second: 38754.02 | Training tokens per second (%): 12.47 | MFU (%): 36.29 | TFLOPs: 358.92 | Global batch size: 64 | Global tokens/sec: 310032.13 | Global MFU (%): 36.29 | Global TFLOPs: 2871.34 | 
2025-05-19 11:17:25,780 - root - INFO - Step: 690 | Loss: 5.73 | Tokens per second: 38812.09 | Training tokens per second (%): 12.47 | MFU (%): 36.35 | TFLOPs: 359.46 | Global batch size: 64 | Global tokens/sec: 310496.73 | Global MFU (%): 36.35 | Global TFLOPs: 2875.64 | 
2025-05-19 11:17:30,010 - root - INFO - Step: 700 | Loss: 5.59 | Tokens per second: 38738.02 | Training tokens per second (%): 12.47 | MFU (%): 36.28 | TFLOPs: 358.77 | Global batch size: 64 | Global tokens/sec: 309904.14 | Global MFU (%): 36.28 | Global TFLOPs: 2870.16 | 
2025-05-19 11:17:34,235 - root - INFO - Step: 710 | Loss: 5.71 | Tokens per second: 38788.78 | Training tokens per second (%): 12.47 | MFU (%): 36.32 | TFLOPs: 359.24 | Global batch size: 64 | Global tokens/sec: 310310.21 | Global MFU (%): 36.32 | Global TFLOPs: 2873.92 | 
2025-05-19 11:17:38,459 - root - INFO - Step: 720 | Loss: 5.69 | Tokens per second: 38798.16 | Training tokens per second (%): 12.47 | MFU (%): 36.33 | TFLOPs: 359.33 | Global batch size: 64 | Global tokens/sec: 310385.24 | Global MFU (%): 36.33 | Global TFLOPs: 2874.61 | 
2025-05-19 11:17:42,684 - root - INFO - Step: 730 | Loss: 5.69 | Tokens per second: 38778.13 | Training tokens per second (%): 12.47 | MFU (%): 36.31 | TFLOPs: 359.14 | Global batch size: 64 | Global tokens/sec: 310225.03 | Global MFU (%): 36.31 | Global TFLOPs: 2873.13 | 
2025-05-19 11:17:46,912 - root - INFO - Step: 740 | Loss: 5.67 | Tokens per second: 38763.32 | Training tokens per second (%): 12.47 | MFU (%): 36.30 | TFLOPs: 359.00 | Global batch size: 64 | Global tokens/sec: 310106.60 | Global MFU (%): 36.30 | Global TFLOPs: 2872.03 | 
2025-05-19 11:17:51,145 - root - INFO - Step: 750 | Loss: 5.65 | Tokens per second: 38709.89 | Training tokens per second (%): 12.47 | MFU (%): 36.25 | TFLOPs: 358.51 | Global batch size: 64 | Global tokens/sec: 309679.15 | Global MFU (%): 36.25 | Global TFLOPs: 2868.07 | 
2025-05-19 11:17:55,373 - root - INFO - Step: 760 | Loss: 5.58 | Tokens per second: 38756.18 | Training tokens per second (%): 12.47 | MFU (%): 36.29 | TFLOPs: 358.94 | Global batch size: 64 | Global tokens/sec: 310049.45 | Global MFU (%): 36.29 | Global TFLOPs: 2871.50 | 
2025-05-19 11:17:59,588 - root - INFO - Step: 770 | Loss: 5.64 | Tokens per second: 38881.17 | Training tokens per second (%): 12.47 | MFU (%): 36.41 | TFLOPs: 360.10 | Global batch size: 64 | Global tokens/sec: 311049.34 | Global MFU (%): 36.41 | Global TFLOPs: 2880.76 | 
2025-05-19 11:18:03,819 - root - INFO - Step: 780 | Loss: 5.63 | Tokens per second: 38722.63 | Training tokens per second (%): 12.47 | MFU (%): 36.26 | TFLOPs: 358.63 | Global batch size: 64 | Global tokens/sec: 309781.03 | Global MFU (%): 36.26 | Global TFLOPs: 2869.01 | 
2025-05-19 11:18:08,042 - root - INFO - Step: 790 | Loss: 5.60 | Tokens per second: 38805.94 | Training tokens per second (%): 12.47 | MFU (%): 36.34 | TFLOPs: 359.40 | Global batch size: 64 | Global tokens/sec: 310447.51 | Global MFU (%): 36.34 | Global TFLOPs: 2875.19 | 
2025-05-19 11:18:12,272 - root - INFO - Step: 800 | Loss: 5.66 | Tokens per second: 38737.12 | Training tokens per second (%): 12.47 | MFU (%): 36.28 | TFLOPs: 358.76 | Global batch size: 64 | Global tokens/sec: 309896.98 | Global MFU (%): 36.28 | Global TFLOPs: 2870.09 | 
2025-05-19 11:18:16,498 - root - INFO - Step: 810 | Loss: 5.68 | Tokens per second: 38779.55 | Training tokens per second (%): 12.47 | MFU (%): 36.31 | TFLOPs: 359.15 | Global batch size: 64 | Global tokens/sec: 310236.42 | Global MFU (%): 36.31 | Global TFLOPs: 2873.23 | 
2025-05-19 11:18:20,723 - root - INFO - Step: 820 | Loss: 5.62 | Tokens per second: 38785.56 | Training tokens per second (%): 12.47 | MFU (%): 36.32 | TFLOPs: 359.21 | Global batch size: 64 | Global tokens/sec: 310284.44 | Global MFU (%): 36.32 | Global TFLOPs: 2873.68 | 
2025-05-19 11:18:24,943 - root - INFO - Step: 830 | Loss: 5.53 | Tokens per second: 38826.55 | Training tokens per second (%): 12.47 | MFU (%): 36.36 | TFLOPs: 359.59 | Global batch size: 64 | Global tokens/sec: 310612.42 | Global MFU (%): 36.36 | Global TFLOPs: 2876.71 | 
2025-05-19 11:18:29,174 - root - INFO - Step: 840 | Loss: 5.59 | Tokens per second: 38737.12 | Training tokens per second (%): 12.47 | MFU (%): 36.28 | TFLOPs: 358.76 | Global batch size: 64 | Global tokens/sec: 309896.94 | Global MFU (%): 36.28 | Global TFLOPs: 2870.09 | 
2025-05-19 11:18:33,394 - root - INFO - Step: 850 | Loss: 5.55 | Tokens per second: 38830.63 | Training tokens per second (%): 12.47 | MFU (%): 36.36 | TFLOPs: 359.63 | Global batch size: 64 | Global tokens/sec: 310645.03 | Global MFU (%): 36.36 | Global TFLOPs: 2877.02 | 
2025-05-19 11:18:37,623 - root - INFO - Step: 860 | Loss: 5.51 | Tokens per second: 38741.83 | Training tokens per second (%): 12.47 | MFU (%): 36.28 | TFLOPs: 358.80 | Global batch size: 64 | Global tokens/sec: 309934.67 | Global MFU (%): 36.28 | Global TFLOPs: 2870.44 | 
2025-05-19 11:18:41,845 - root - INFO - Step: 870 | Loss: 5.54 | Tokens per second: 38820.83 | Training tokens per second (%): 12.47 | MFU (%): 36.35 | TFLOPs: 359.54 | Global batch size: 64 | Global tokens/sec: 310566.65 | Global MFU (%): 36.35 | Global TFLOPs: 2876.29 | 
2025-05-19 11:18:46,069 - root - INFO - Step: 880 | Loss: 5.51 | Tokens per second: 38788.63 | Training tokens per second (%): 12.47 | MFU (%): 36.32 | TFLOPs: 359.24 | Global batch size: 64 | Global tokens/sec: 310309.07 | Global MFU (%): 36.32 | Global TFLOPs: 2873.91 | 
2025-05-19 11:18:50,295 - root - INFO - Step: 890 | Loss: 5.48 | Tokens per second: 38780.39 | Training tokens per second (%): 12.47 | MFU (%): 36.32 | TFLOPs: 359.16 | Global batch size: 64 | Global tokens/sec: 310243.13 | Global MFU (%): 36.32 | Global TFLOPs: 2873.29 | 
2025-05-19 11:18:54,522 - root - INFO - Step: 900 | Loss: 5.50 | Tokens per second: 38763.06 | Training tokens per second (%): 12.47 | MFU (%): 36.30 | TFLOPs: 359.00 | Global batch size: 64 | Global tokens/sec: 310104.47 | Global MFU (%): 36.30 | Global TFLOPs: 2872.01 | 
2025-05-19 11:18:58,742 - root - INFO - Step: 910 | Loss: 5.49 | Tokens per second: 38830.80 | Training tokens per second (%): 12.47 | MFU (%): 36.36 | TFLOPs: 359.63 | Global batch size: 64 | Global tokens/sec: 310646.36 | Global MFU (%): 36.36 | Global TFLOPs: 2877.03 | 
2025-05-19 11:19:02,970 - root - INFO - Step: 920 | Loss: 5.49 | Tokens per second: 38762.45 | Training tokens per second (%): 12.47 | MFU (%): 36.30 | TFLOPs: 359.00 | Global batch size: 64 | Global tokens/sec: 310099.63 | Global MFU (%): 36.30 | Global TFLOPs: 2871.97 | 
2025-05-19 11:19:07,194 - root - INFO - Step: 930 | Loss: 5.52 | Tokens per second: 38793.67 | Training tokens per second (%): 12.47 | MFU (%): 36.33 | TFLOPs: 359.28 | Global batch size: 64 | Global tokens/sec: 310349.39 | Global MFU (%): 36.33 | Global TFLOPs: 2874.28 | 
2025-05-19 11:19:11,413 - root - INFO - Step: 940 | Loss: 5.45 | Tokens per second: 38836.60 | Training tokens per second (%): 12.47 | MFU (%): 36.37 | TFLOPs: 359.68 | Global batch size: 64 | Global tokens/sec: 310692.82 | Global MFU (%): 36.37 | Global TFLOPs: 2877.46 | 
2025-05-19 11:19:15,635 - root - INFO - Step: 950 | Loss: 5.48 | Tokens per second: 38818.11 | Training tokens per second (%): 12.47 | MFU (%): 36.35 | TFLOPs: 359.51 | Global batch size: 64 | Global tokens/sec: 310544.90 | Global MFU (%): 36.35 | Global TFLOPs: 2876.09 | 
2025-05-19 11:19:19,856 - root - INFO - Step: 960 | Loss: 5.46 | Tokens per second: 38818.54 | Training tokens per second (%): 12.47 | MFU (%): 36.35 | TFLOPs: 359.52 | Global batch size: 64 | Global tokens/sec: 310548.30 | Global MFU (%): 36.35 | Global TFLOPs: 2876.12 | 
2025-05-19 11:19:24,078 - root - INFO - Step: 970 | Loss: 5.46 | Tokens per second: 38810.70 | Training tokens per second (%): 12.47 | MFU (%): 36.34 | TFLOPs: 359.44 | Global batch size: 64 | Global tokens/sec: 310485.57 | Global MFU (%): 36.34 | Global TFLOPs: 2875.54 | 
2025-05-19 11:19:28,315 - root - INFO - Step: 980 | Loss: 5.44 | Tokens per second: 38680.68 | Training tokens per second (%): 12.47 | MFU (%): 36.22 | TFLOPs: 358.24 | Global batch size: 64 | Global tokens/sec: 309445.40 | Global MFU (%): 36.22 | Global TFLOPs: 2865.91 | 
2025-05-19 11:19:32,537 - root - INFO - Step: 990 | Loss: 5.40 | Tokens per second: 38805.32 | Training tokens per second (%): 12.47 | MFU (%): 36.34 | TFLOPs: 359.39 | Global batch size: 64 | Global tokens/sec: 310442.54 | Global MFU (%): 36.34 | Global TFLOPs: 2875.14 | 
2025-05-19 11:19:36,766 - root - INFO - Step: 1000 | Loss: 5.43 | Tokens per second: 38748.68 | Training tokens per second (%): 12.47 | MFU (%): 36.29 | TFLOPs: 358.87 | Global batch size: 64 | Global tokens/sec: 309989.47 | Global MFU (%): 36.29 | Global TFLOPs: 2870.95 | 
2025-05-19 11:19:36,767 - root - INFO - Training completed
[sbatch-master] task finished
