[sbatch-master] running on nid006654
[sbatch-master] SLURM_NODELIST: nid[006654,006656,006660]
[sbatch-master] SLURM_NNODES: 3
[sbatch-master] SLURM_NODEID: 0
[Master] Master node: nid006654
[Master] World size: 12
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
[srun] rank=0 host=nid006654 noderank=0 localrank=0
[srun] rank=2 host=nid006660 noderank=2 localrank=0
[srun] rank=1 host=nid006656 noderank=1 localrank=0
W0519 11:28:25.332000 55709 torch/distributed/run.py:792] 
W0519 11:28:25.332000 55709 torch/distributed/run.py:792] *****************************************
W0519 11:28:25.332000 55709 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0519 11:28:25.332000 55709 torch/distributed/run.py:792] *****************************************
W0519 11:28:26.025000 38752 torch/distributed/run.py:792] 
W0519 11:28:26.025000 38752 torch/distributed/run.py:792] *****************************************
W0519 11:28:26.025000 38752 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0519 11:28:26.025000 38752 torch/distributed/run.py:792] *****************************************
W0519 11:28:26.892000 287252 torch/distributed/run.py:792] 
W0519 11:28:26.892000 287252 torch/distributed/run.py:792] *****************************************
W0519 11:28:26.892000 287252 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0519 11:28:26.892000 287252 torch/distributed/run.py:792] *****************************************
2025-05-19 11:28:40,917 - root - INFO - [Distributed Init] Rank 8 initialized on node 2 on GPU 0.
[rank8]:[W519 11:28:41.696102061 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 8]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:28:41,437 - root - INFO - [Distributed Init] Rank 0 initialized on node 0 on GPU 0.
2025-05-19 11:28:41,450 - root - INFO - [Distributed Init] Rank 11 initialized on node 2 on GPU 3.
[rank11]:[W519 11:28:41.782375363 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 11]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:28:41,510 - root - INFO - [Distributed Init] Rank 9 initialized on node 2 on GPU 1.
[rank9]:[W519 11:28:41.841860134 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 9]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:28:41,521 - root - INFO - [Distributed Init] Rank 10 initialized on node 2 on GPU 2.
[rank10]:[W519 11:28:41.852768096 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 10]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:28:41,898 - root - INFO - [Distributed Init] Rank 4 initialized on node 1 on GPU 0.
[rank0]:[W519 11:28:42.225539064 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:28:42,081 - root - INFO - [Distributed Init] Rank 3 initialized on node 0 on GPU 3.
2025-05-19 11:28:42,081 - root - INFO - [Distributed Init] Rank 1 initialized on node 0 on GPU 1.
[rank3]:[W519 11:28:42.299598672 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W519 11:28:42.299599056 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:28:42,154 - root - INFO - [Distributed Init] Rank 2 initialized on node 0 on GPU 2.
[rank2]:[W519 11:28:42.372121558 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W519 11:28:42.955161241 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:28:42,546 - root - INFO - [Distributed Init] Rank 6 initialized on node 1 on GPU 2.
2025-05-19 11:28:42,546 - root - INFO - [Distributed Init] Rank 7 initialized on node 1 on GPU 3.
2025-05-19 11:28:42,546 - root - INFO - [Distributed Init] Rank 5 initialized on node 1 on GPU 1.
[rank6]:[W519 11:28:42.093547117 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 6]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W519 11:28:42.093547917 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 7]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W519 11:28:42.093552461 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 5]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:28:48,610 - root - INFO - [Rank 0] All ranks ready!
2025-05-19 11:28:48,611 - root - INFO - Distributed training enabled: 12 processes
2025-05-19 11:28:48,611 - root - INFO - Master process: 0 on cuda:0
2025-05-19 11:28:48,611 - root - INFO - Experiment args: Namespace(dataset='/capstor/scratch/cscs/kasparr/project/train_data_tokenized_token-list_snappy.parquet', dataset_type='token-list', pretokenized=True, tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=10, profile=False, profile_step_start=10, profile_step_end=10, grad_max_norm=1, model_dtype='bf16', compile=False, distributed=True, backend='nccl', find_unused_parameters=False)
2025-05-19 11:28:48,611 - root - INFO - Setting up Tokenizer...
2025-05-19 11:28:49,150 - root - INFO - Setting up DataLoaders...
2025-05-19 11:28:49,150 - root - INFO - Using pretokenized data: /capstor/scratch/cscs/kasparr/project/train_data_tokenized_token-list_snappy.parquet
2025-05-19 11:28:57,469 - root - INFO - Setting up Model...
2025-05-19 11:29:05,940 - root - INFO - Model wrapped with DistributedDataParallel
2025-05-19 11:29:05,941 - root - INFO - Global batch size: 96 (local: 8 Ã— 12 processes)
2025-05-19 11:29:05,941 - root - INFO - Starting training!
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
2025-05-19 11:29:07,550 - root - INFO - Step: 1 | Loss: 11.94 | Tokens per second: 10189.66 | Training tokens per second (%): 8.32 | MFU (%): 9.54 | TFLOPs: 94.37 | Global batch size: 96 | Global tokens/sec: 122275.89 | Global MFU (%): 9.54 | Global TFLOPs: 1132.45 | 
2025-05-19 11:29:11,740 - root - INFO - Step: 10 | Loss: 11.91 | Tokens per second: 35195.87 | Training tokens per second (%): 8.31 | MFU (%): 32.96 | TFLOPs: 325.96 | Global batch size: 96 | Global tokens/sec: 422350.48 | Global MFU (%): 32.96 | Global TFLOPs: 3911.57 | 
2025-05-19 11:29:15,987 - root - INFO - Step: 20 | Loss: 11.66 | Tokens per second: 38583.29 | Training tokens per second (%): 8.31 | MFU (%): 36.13 | TFLOPs: 357.34 | Global batch size: 96 | Global tokens/sec: 462999.44 | Global MFU (%): 36.13 | Global TFLOPs: 4288.04 | 
2025-05-19 11:29:20,236 - root - INFO - Step: 30 | Loss: 11.02 | Tokens per second: 38567.33 | Training tokens per second (%): 8.31 | MFU (%): 36.12 | TFLOPs: 357.19 | Global batch size: 96 | Global tokens/sec: 462807.91 | Global MFU (%): 36.12 | Global TFLOPs: 4286.26 | 
2025-05-19 11:29:24,494 - root - INFO - Step: 40 | Loss: 9.98 | Tokens per second: 38490.96 | Training tokens per second (%): 8.31 | MFU (%): 36.04 | TFLOPs: 356.48 | Global batch size: 96 | Global tokens/sec: 461891.47 | Global MFU (%): 36.04 | Global TFLOPs: 4277.77 | 
2025-05-19 11:29:28,735 - root - INFO - Step: 50 | Loss: 9.34 | Tokens per second: 38634.83 | Training tokens per second (%): 8.31 | MFU (%): 36.18 | TFLOPs: 357.81 | Global batch size: 96 | Global tokens/sec: 463617.98 | Global MFU (%): 36.18 | Global TFLOPs: 4293.76 | 
2025-05-19 11:29:33,020 - root - INFO - Step: 60 | Loss: 8.73 | Tokens per second: 38245.59 | Training tokens per second (%): 8.31 | MFU (%): 35.81 | TFLOPs: 354.21 | Global batch size: 96 | Global tokens/sec: 458947.13 | Global MFU (%): 35.81 | Global TFLOPs: 4250.51 | 
2025-05-19 11:29:37,265 - root - INFO - Step: 70 | Loss: 8.19 | Tokens per second: 38599.78 | Training tokens per second (%): 8.31 | MFU (%): 36.15 | TFLOPs: 357.49 | Global batch size: 96 | Global tokens/sec: 463197.39 | Global MFU (%): 36.15 | Global TFLOPs: 4289.87 | 
2025-05-19 11:29:41,501 - root - INFO - Step: 80 | Loss: 7.80 | Tokens per second: 38681.06 | Training tokens per second (%): 8.31 | MFU (%): 36.22 | TFLOPs: 358.24 | Global batch size: 96 | Global tokens/sec: 464172.72 | Global MFU (%): 36.22 | Global TFLOPs: 4298.90 | 
2025-05-19 11:29:45,751 - root - INFO - Step: 90 | Loss: 7.49 | Tokens per second: 38558.15 | Training tokens per second (%): 8.31 | MFU (%): 36.11 | TFLOPs: 357.10 | Global batch size: 96 | Global tokens/sec: 462697.82 | Global MFU (%): 36.11 | Global TFLOPs: 4285.24 | 
2025-05-19 11:29:49,991 - root - INFO - Step: 100 | Loss: 7.34 | Tokens per second: 38649.32 | Training tokens per second (%): 8.31 | MFU (%): 36.19 | TFLOPs: 357.95 | Global batch size: 96 | Global tokens/sec: 463791.83 | Global MFU (%): 36.19 | Global TFLOPs: 4295.38 | 
2025-05-19 11:29:54,227 - root - INFO - Step: 110 | Loss: 7.20 | Tokens per second: 38687.29 | Training tokens per second (%): 8.31 | MFU (%): 36.23 | TFLOPs: 358.30 | Global batch size: 96 | Global tokens/sec: 464247.49 | Global MFU (%): 36.23 | Global TFLOPs: 4299.60 | 
2025-05-19 11:29:58,481 - root - INFO - Step: 120 | Loss: 7.18 | Tokens per second: 38521.77 | Training tokens per second (%): 8.31 | MFU (%): 36.07 | TFLOPs: 356.77 | Global batch size: 96 | Global tokens/sec: 462261.27 | Global MFU (%): 36.07 | Global TFLOPs: 4281.20 | 
2025-05-19 11:30:02,720 - root - INFO - Step: 130 | Loss: 7.06 | Tokens per second: 38653.78 | Training tokens per second (%): 8.31 | MFU (%): 36.20 | TFLOPs: 357.99 | Global batch size: 96 | Global tokens/sec: 463845.39 | Global MFU (%): 36.20 | Global TFLOPs: 4295.87 | 
2025-05-19 11:30:06,973 - root - INFO - Step: 140 | Loss: 6.98 | Tokens per second: 38527.00 | Training tokens per second (%): 8.31 | MFU (%): 36.08 | TFLOPs: 356.82 | Global batch size: 96 | Global tokens/sec: 462324.06 | Global MFU (%): 36.08 | Global TFLOPs: 4281.78 | 
2025-05-19 11:30:11,221 - root - INFO - Step: 150 | Loss: 6.88 | Tokens per second: 38574.16 | Training tokens per second (%): 8.31 | MFU (%): 36.12 | TFLOPs: 357.25 | Global batch size: 96 | Global tokens/sec: 462889.91 | Global MFU (%): 36.12 | Global TFLOPs: 4287.02 | 
2025-05-19 11:30:15,466 - root - INFO - Step: 160 | Loss: 6.85 | Tokens per second: 38600.95 | Training tokens per second (%): 8.31 | MFU (%): 36.15 | TFLOPs: 357.50 | Global batch size: 96 | Global tokens/sec: 463211.42 | Global MFU (%): 36.15 | Global TFLOPs: 4290.00 | 
2025-05-19 11:30:19,720 - root - INFO - Step: 170 | Loss: 6.75 | Tokens per second: 38523.88 | Training tokens per second (%): 8.31 | MFU (%): 36.08 | TFLOPs: 356.79 | Global batch size: 96 | Global tokens/sec: 462286.51 | Global MFU (%): 36.08 | Global TFLOPs: 4281.43 | 
2025-05-19 11:30:23,964 - root - INFO - Step: 180 | Loss: 6.66 | Tokens per second: 38615.62 | Training tokens per second (%): 8.31 | MFU (%): 36.16 | TFLOPs: 357.64 | Global batch size: 96 | Global tokens/sec: 463387.48 | Global MFU (%): 36.16 | Global TFLOPs: 4291.63 | 
2025-05-19 11:30:28,216 - root - INFO - Step: 190 | Loss: 6.66 | Tokens per second: 38530.57 | Training tokens per second (%): 8.31 | MFU (%): 36.08 | TFLOPs: 356.85 | Global batch size: 96 | Global tokens/sec: 462366.83 | Global MFU (%): 36.08 | Global TFLOPs: 4282.18 | 
2025-05-19 11:30:32,452 - root - INFO - Step: 200 | Loss: 6.61 | Tokens per second: 38684.94 | Training tokens per second (%): 8.31 | MFU (%): 36.23 | TFLOPs: 358.28 | Global batch size: 96 | Global tokens/sec: 464219.24 | Global MFU (%): 36.23 | Global TFLOPs: 4299.33 | 
2025-05-19 11:30:36,696 - root - INFO - Step: 210 | Loss: 6.57 | Tokens per second: 38616.16 | Training tokens per second (%): 8.31 | MFU (%): 36.16 | TFLOPs: 357.64 | Global batch size: 96 | Global tokens/sec: 463393.89 | Global MFU (%): 36.16 | Global TFLOPs: 4291.69 | 
2025-05-19 11:30:40,943 - root - INFO - Step: 220 | Loss: 6.48 | Tokens per second: 38585.05 | Training tokens per second (%): 8.31 | MFU (%): 36.13 | TFLOPs: 357.35 | Global batch size: 96 | Global tokens/sec: 463020.59 | Global MFU (%): 36.13 | Global TFLOPs: 4288.23 | 
2025-05-19 11:30:45,186 - root - INFO - Step: 230 | Loss: 6.43 | Tokens per second: 38614.39 | Training tokens per second (%): 8.31 | MFU (%): 36.16 | TFLOPs: 357.62 | Global batch size: 96 | Global tokens/sec: 463372.70 | Global MFU (%): 36.16 | Global TFLOPs: 4291.49 | 
2025-05-19 11:30:49,441 - root - INFO - Step: 240 | Loss: 6.41 | Tokens per second: 38517.78 | Training tokens per second (%): 8.31 | MFU (%): 36.07 | TFLOPs: 356.73 | Global batch size: 96 | Global tokens/sec: 462213.36 | Global MFU (%): 36.07 | Global TFLOPs: 4280.76 | 
2025-05-19 11:30:53,684 - root - INFO - Step: 250 | Loss: 6.41 | Tokens per second: 38617.03 | Training tokens per second (%): 8.31 | MFU (%): 36.16 | TFLOPs: 357.65 | Global batch size: 96 | Global tokens/sec: 463404.34 | Global MFU (%): 36.16 | Global TFLOPs: 4291.79 | 
2025-05-19 11:30:57,931 - root - INFO - Step: 260 | Loss: 6.37 | Tokens per second: 38588.03 | Training tokens per second (%): 8.31 | MFU (%): 36.14 | TFLOPs: 357.38 | Global batch size: 96 | Global tokens/sec: 463056.37 | Global MFU (%): 36.14 | Global TFLOPs: 4288.56 | 
2025-05-19 11:31:02,223 - root - INFO - Step: 270 | Loss: 6.29 | Tokens per second: 38177.31 | Training tokens per second (%): 8.31 | MFU (%): 35.75 | TFLOPs: 353.58 | Global batch size: 96 | Global tokens/sec: 458127.69 | Global MFU (%): 35.75 | Global TFLOPs: 4242.92 | 
2025-05-19 11:31:06,466 - root - INFO - Step: 280 | Loss: 6.30 | Tokens per second: 38615.78 | Training tokens per second (%): 8.31 | MFU (%): 36.16 | TFLOPs: 357.64 | Global batch size: 96 | Global tokens/sec: 463389.38 | Global MFU (%): 36.16 | Global TFLOPs: 4291.65 | 
2025-05-19 11:31:10,703 - root - INFO - Step: 290 | Loss: 6.27 | Tokens per second: 38682.11 | Training tokens per second (%): 8.31 | MFU (%): 36.22 | TFLOPs: 358.25 | Global batch size: 96 | Global tokens/sec: 464185.30 | Global MFU (%): 36.22 | Global TFLOPs: 4299.02 | 
2025-05-19 11:31:14,944 - root - INFO - Step: 300 | Loss: 6.23 | Tokens per second: 38631.51 | Training tokens per second (%): 8.31 | MFU (%): 36.18 | TFLOPs: 357.78 | Global batch size: 96 | Global tokens/sec: 463578.17 | Global MFU (%): 36.18 | Global TFLOPs: 4293.40 | 
2025-05-19 11:31:19,189 - root - INFO - Step: 310 | Loss: 6.24 | Tokens per second: 38609.00 | Training tokens per second (%): 8.31 | MFU (%): 36.16 | TFLOPs: 357.57 | Global batch size: 96 | Global tokens/sec: 463308.02 | Global MFU (%): 36.16 | Global TFLOPs: 4290.89 | 
2025-05-19 11:31:23,422 - root - INFO - Step: 320 | Loss: 6.17 | Tokens per second: 38705.08 | Training tokens per second (%): 8.31 | MFU (%): 36.25 | TFLOPs: 358.46 | Global batch size: 96 | Global tokens/sec: 464460.92 | Global MFU (%): 36.25 | Global TFLOPs: 4301.57 | 
2025-05-19 11:31:27,664 - root - INFO - Step: 330 | Loss: 6.17 | Tokens per second: 38636.58 | Training tokens per second (%): 8.31 | MFU (%): 36.18 | TFLOPs: 357.83 | Global batch size: 96 | Global tokens/sec: 463638.97 | Global MFU (%): 36.18 | Global TFLOPs: 4293.96 | 
2025-05-19 11:31:31,917 - root - INFO - Step: 340 | Loss: 6.17 | Tokens per second: 38526.93 | Training tokens per second (%): 8.31 | MFU (%): 36.08 | TFLOPs: 356.81 | Global batch size: 96 | Global tokens/sec: 462323.14 | Global MFU (%): 36.08 | Global TFLOPs: 4281.77 | 
2025-05-19 11:31:36,148 - root - INFO - Step: 350 | Loss: 6.08 | Tokens per second: 38731.74 | Training tokens per second (%): 8.31 | MFU (%): 36.27 | TFLOPs: 358.71 | Global batch size: 96 | Global tokens/sec: 464780.93 | Global MFU (%): 36.27 | Global TFLOPs: 4304.54 | 
2025-05-19 11:31:40,391 - root - INFO - Step: 360 | Loss: 6.13 | Tokens per second: 38615.14 | Training tokens per second (%): 8.31 | MFU (%): 36.16 | TFLOPs: 357.63 | Global batch size: 96 | Global tokens/sec: 463381.69 | Global MFU (%): 36.16 | Global TFLOPs: 4291.58 | 
2025-05-19 11:31:44,627 - root - INFO - Step: 370 | Loss: 6.11 | Tokens per second: 38686.75 | Training tokens per second (%): 8.31 | MFU (%): 36.23 | TFLOPs: 358.29 | Global batch size: 96 | Global tokens/sec: 464240.99 | Global MFU (%): 36.23 | Global TFLOPs: 4299.53 | 
2025-05-19 11:31:48,870 - root - INFO - Step: 380 | Loss: 6.09 | Tokens per second: 38622.97 | Training tokens per second (%): 8.31 | MFU (%): 36.17 | TFLOPs: 357.70 | Global batch size: 96 | Global tokens/sec: 463475.60 | Global MFU (%): 36.17 | Global TFLOPs: 4292.45 | 
2025-05-19 11:31:53,109 - root - INFO - Step: 390 | Loss: 6.06 | Tokens per second: 38658.52 | Training tokens per second (%): 8.31 | MFU (%): 36.20 | TFLOPs: 358.03 | Global batch size: 96 | Global tokens/sec: 463902.27 | Global MFU (%): 36.20 | Global TFLOPs: 4296.40 | 
2025-05-19 11:31:57,354 - root - INFO - Step: 400 | Loss: 5.98 | Tokens per second: 38601.23 | Training tokens per second (%): 8.31 | MFU (%): 36.15 | TFLOPs: 357.50 | Global batch size: 96 | Global tokens/sec: 463214.76 | Global MFU (%): 36.15 | Global TFLOPs: 4290.03 | 
2025-05-19 11:32:01,592 - root - INFO - Step: 410 | Loss: 6.00 | Tokens per second: 38660.68 | Training tokens per second (%): 8.31 | MFU (%): 36.20 | TFLOPs: 358.05 | Global batch size: 96 | Global tokens/sec: 463928.14 | Global MFU (%): 36.20 | Global TFLOPs: 4296.64 | 
2025-05-19 11:32:05,828 - root - INFO - Step: 420 | Loss: 5.97 | Tokens per second: 38686.76 | Training tokens per second (%): 8.31 | MFU (%): 36.23 | TFLOPs: 358.29 | Global batch size: 96 | Global tokens/sec: 464241.16 | Global MFU (%): 36.23 | Global TFLOPs: 4299.54 | 
2025-05-19 11:32:10,065 - root - INFO - Step: 430 | Loss: 5.95 | Tokens per second: 38674.96 | Training tokens per second (%): 8.31 | MFU (%): 36.22 | TFLOPs: 358.19 | Global batch size: 96 | Global tokens/sec: 464099.48 | Global MFU (%): 36.22 | Global TFLOPs: 4298.22 | 
2025-05-19 11:32:14,309 - root - INFO - Step: 440 | Loss: 6.24 | Tokens per second: 38607.93 | Training tokens per second (%): 8.31 | MFU (%): 36.15 | TFLOPs: 357.56 | Global batch size: 96 | Global tokens/sec: 463295.18 | Global MFU (%): 36.15 | Global TFLOPs: 4290.78 | 
2025-05-19 11:32:18,553 - root - INFO - Step: 450 | Loss: 6.12 | Tokens per second: 38609.42 | Training tokens per second (%): 8.31 | MFU (%): 36.16 | TFLOPs: 357.58 | Global batch size: 96 | Global tokens/sec: 463313.05 | Global MFU (%): 36.16 | Global TFLOPs: 4290.94 | 
2025-05-19 11:32:22,794 - root - INFO - Step: 460 | Loss: 6.05 | Tokens per second: 38639.19 | Training tokens per second (%): 8.31 | MFU (%): 36.18 | TFLOPs: 357.85 | Global batch size: 96 | Global tokens/sec: 463670.26 | Global MFU (%): 36.18 | Global TFLOPs: 4294.25 | 
2025-05-19 11:32:27,025 - root - INFO - Step: 470 | Loss: 5.98 | Tokens per second: 38734.92 | Training tokens per second (%): 8.31 | MFU (%): 36.27 | TFLOPs: 358.74 | Global batch size: 96 | Global tokens/sec: 464819.09 | Global MFU (%): 36.27 | Global TFLOPs: 4304.89 | 
2025-05-19 11:32:31,266 - root - INFO - Step: 480 | Loss: 5.99 | Tokens per second: 38635.59 | Training tokens per second (%): 8.31 | MFU (%): 36.18 | TFLOPs: 357.82 | Global batch size: 96 | Global tokens/sec: 463627.12 | Global MFU (%): 36.18 | Global TFLOPs: 4293.85 | 
2025-05-19 11:32:35,515 - root - INFO - Step: 490 | Loss: 5.95 | Tokens per second: 38563.66 | Training tokens per second (%): 8.31 | MFU (%): 36.11 | TFLOPs: 357.15 | Global batch size: 96 | Global tokens/sec: 462763.94 | Global MFU (%): 36.11 | Global TFLOPs: 4285.86 | 
2025-05-19 11:32:39,757 - root - INFO - Step: 500 | Loss: 5.92 | Tokens per second: 38634.67 | Training tokens per second (%): 8.31 | MFU (%): 36.18 | TFLOPs: 357.81 | Global batch size: 96 | Global tokens/sec: 463616.00 | Global MFU (%): 36.18 | Global TFLOPs: 4293.75 | 
2025-05-19 11:32:44,008 - root - INFO - Step: 510 | Loss: 5.89 | Tokens per second: 38546.18 | Training tokens per second (%): 8.31 | MFU (%): 36.10 | TFLOPs: 356.99 | Global batch size: 96 | Global tokens/sec: 462554.10 | Global MFU (%): 36.10 | Global TFLOPs: 4283.91 | 
2025-05-19 11:32:48,248 - root - INFO - Step: 520 | Loss: 5.87 | Tokens per second: 38643.40 | Training tokens per second (%): 8.31 | MFU (%): 36.19 | TFLOPs: 357.89 | Global batch size: 96 | Global tokens/sec: 463720.85 | Global MFU (%): 36.19 | Global TFLOPs: 4294.72 | 
2025-05-19 11:32:52,485 - root - INFO - Step: 530 | Loss: 5.91 | Tokens per second: 38676.15 | Training tokens per second (%): 8.31 | MFU (%): 36.22 | TFLOPs: 358.20 | Global batch size: 96 | Global tokens/sec: 464113.84 | Global MFU (%): 36.22 | Global TFLOPs: 4298.36 | 
2025-05-19 11:32:56,729 - root - INFO - Step: 540 | Loss: 5.90 | Tokens per second: 38611.79 | Training tokens per second (%): 8.31 | MFU (%): 36.16 | TFLOPs: 357.60 | Global batch size: 96 | Global tokens/sec: 463341.48 | Global MFU (%): 36.16 | Global TFLOPs: 4291.20 | 
2025-05-19 11:33:00,965 - root - INFO - Step: 550 | Loss: 5.84 | Tokens per second: 38690.08 | Training tokens per second (%): 8.31 | MFU (%): 36.23 | TFLOPs: 358.33 | Global batch size: 96 | Global tokens/sec: 464280.98 | Global MFU (%): 36.23 | Global TFLOPs: 4299.91 | 
2025-05-19 11:33:05,201 - root - INFO - Step: 560 | Loss: 5.87 | Tokens per second: 38678.22 | Training tokens per second (%): 8.31 | MFU (%): 36.22 | TFLOPs: 358.22 | Global batch size: 96 | Global tokens/sec: 464138.64 | Global MFU (%): 36.22 | Global TFLOPs: 4298.59 | 
2025-05-19 11:33:09,437 - root - INFO - Step: 570 | Loss: 5.85 | Tokens per second: 38685.18 | Training tokens per second (%): 8.31 | MFU (%): 36.23 | TFLOPs: 358.28 | Global batch size: 96 | Global tokens/sec: 464222.15 | Global MFU (%): 36.23 | Global TFLOPs: 4299.36 | 
2025-05-19 11:33:13,674 - root - INFO - Step: 580 | Loss: 5.86 | Tokens per second: 38675.18 | Training tokens per second (%): 8.31 | MFU (%): 36.22 | TFLOPs: 358.19 | Global batch size: 96 | Global tokens/sec: 464102.17 | Global MFU (%): 36.22 | Global TFLOPs: 4298.25 | 
2025-05-19 11:33:17,915 - root - INFO - Step: 590 | Loss: 5.85 | Tokens per second: 38644.56 | Training tokens per second (%): 8.31 | MFU (%): 36.19 | TFLOPs: 357.90 | Global batch size: 96 | Global tokens/sec: 463734.76 | Global MFU (%): 36.19 | Global TFLOPs: 4294.85 | 
2025-05-19 11:33:22,157 - root - INFO - Step: 600 | Loss: 5.78 | Tokens per second: 38626.68 | Training tokens per second (%): 8.31 | MFU (%): 36.17 | TFLOPs: 357.74 | Global batch size: 96 | Global tokens/sec: 463520.13 | Global MFU (%): 36.17 | Global TFLOPs: 4292.86 | 
2025-05-19 11:33:26,395 - root - INFO - Step: 610 | Loss: 5.80 | Tokens per second: 38662.58 | Training tokens per second (%): 8.31 | MFU (%): 36.21 | TFLOPs: 358.07 | Global batch size: 96 | Global tokens/sec: 463951.00 | Global MFU (%): 36.21 | Global TFLOPs: 4296.85 | 
2025-05-19 11:33:30,642 - root - INFO - Step: 620 | Loss: 5.75 | Tokens per second: 38586.49 | Training tokens per second (%): 8.31 | MFU (%): 36.13 | TFLOPs: 357.37 | Global batch size: 96 | Global tokens/sec: 463037.93 | Global MFU (%): 36.13 | Global TFLOPs: 4288.39 | 
2025-05-19 11:33:34,868 - root - INFO - Step: 630 | Loss: 5.74 | Tokens per second: 38779.83 | Training tokens per second (%): 8.31 | MFU (%): 36.32 | TFLOPs: 359.16 | Global batch size: 96 | Global tokens/sec: 465357.91 | Global MFU (%): 36.32 | Global TFLOPs: 4309.88 | 
2025-05-19 11:33:39,104 - root - INFO - Step: 640 | Loss: 5.77 | Tokens per second: 38677.04 | Training tokens per second (%): 8.31 | MFU (%): 36.22 | TFLOPs: 358.20 | Global batch size: 96 | Global tokens/sec: 464124.50 | Global MFU (%): 36.22 | Global TFLOPs: 4298.46 | 
2025-05-19 11:33:43,339 - root - INFO - Step: 650 | Loss: 5.75 | Tokens per second: 38699.43 | Training tokens per second (%): 8.31 | MFU (%): 36.24 | TFLOPs: 358.41 | Global batch size: 96 | Global tokens/sec: 464393.12 | Global MFU (%): 36.24 | Global TFLOPs: 4300.94 | 
2025-05-19 11:33:47,579 - root - INFO - Step: 660 | Loss: 5.77 | Tokens per second: 38648.96 | Training tokens per second (%): 8.31 | MFU (%): 36.19 | TFLOPs: 357.94 | Global batch size: 96 | Global tokens/sec: 463787.51 | Global MFU (%): 36.19 | Global TFLOPs: 4295.33 | 
2025-05-19 11:33:51,823 - root - INFO - Step: 670 | Loss: 5.73 | Tokens per second: 38603.67 | Training tokens per second (%): 8.31 | MFU (%): 36.15 | TFLOPs: 357.53 | Global batch size: 96 | Global tokens/sec: 463244.09 | Global MFU (%): 36.15 | Global TFLOPs: 4290.30 | 
2025-05-19 11:33:56,061 - root - INFO - Step: 680 | Loss: 5.71 | Tokens per second: 38667.89 | Training tokens per second (%): 8.31 | MFU (%): 36.21 | TFLOPs: 358.12 | Global batch size: 96 | Global tokens/sec: 464014.67 | Global MFU (%): 36.21 | Global TFLOPs: 4297.44 | 
2025-05-19 11:34:00,300 - root - INFO - Step: 690 | Loss: 5.72 | Tokens per second: 38662.57 | Training tokens per second (%): 8.31 | MFU (%): 36.21 | TFLOPs: 358.07 | Global batch size: 96 | Global tokens/sec: 463950.84 | Global MFU (%): 36.21 | Global TFLOPs: 4296.85 | 
2025-05-19 11:34:04,535 - root - INFO - Step: 700 | Loss: 5.65 | Tokens per second: 38691.63 | Training tokens per second (%): 8.31 | MFU (%): 36.23 | TFLOPs: 358.34 | Global batch size: 96 | Global tokens/sec: 464299.60 | Global MFU (%): 36.23 | Global TFLOPs: 4300.08 | 
2025-05-19 11:34:08,778 - root - INFO - Step: 710 | Loss: 5.69 | Tokens per second: 38616.51 | Training tokens per second (%): 8.31 | MFU (%): 36.16 | TFLOPs: 357.64 | Global batch size: 96 | Global tokens/sec: 463398.07 | Global MFU (%): 36.16 | Global TFLOPs: 4291.73 | 
2025-05-19 11:34:13,033 - root - INFO - Step: 720 | Loss: 5.66 | Tokens per second: 38509.56 | Training tokens per second (%): 8.31 | MFU (%): 36.06 | TFLOPs: 356.65 | Global batch size: 96 | Global tokens/sec: 462114.72 | Global MFU (%): 36.06 | Global TFLOPs: 4279.84 | 
2025-05-19 11:34:17,286 - root - INFO - Step: 730 | Loss: 5.65 | Tokens per second: 38531.75 | Training tokens per second (%): 8.31 | MFU (%): 36.08 | TFLOPs: 356.86 | Global batch size: 96 | Global tokens/sec: 462380.95 | Global MFU (%): 36.08 | Global TFLOPs: 4282.31 | 
2025-05-19 11:34:21,528 - root - INFO - Step: 740 | Loss: 5.64 | Tokens per second: 38631.34 | Training tokens per second (%): 8.31 | MFU (%): 36.18 | TFLOPs: 357.78 | Global batch size: 96 | Global tokens/sec: 463576.05 | Global MFU (%): 36.18 | Global TFLOPs: 4293.38 | 
2025-05-19 11:34:25,774 - root - INFO - Step: 750 | Loss: 5.67 | Tokens per second: 38596.31 | Training tokens per second (%): 8.31 | MFU (%): 36.14 | TFLOPs: 357.46 | Global batch size: 96 | Global tokens/sec: 463155.76 | Global MFU (%): 36.14 | Global TFLOPs: 4289.48 | 
2025-05-19 11:34:30,013 - root - INFO - Step: 760 | Loss: 5.61 | Tokens per second: 38655.09 | Training tokens per second (%): 8.31 | MFU (%): 36.20 | TFLOPs: 358.00 | Global batch size: 96 | Global tokens/sec: 463861.06 | Global MFU (%): 36.20 | Global TFLOPs: 4296.02 | 
2025-05-19 11:34:34,257 - root - INFO - Step: 770 | Loss: 5.62 | Tokens per second: 38609.40 | Training tokens per second (%): 8.31 | MFU (%): 36.16 | TFLOPs: 357.58 | Global batch size: 96 | Global tokens/sec: 463312.80 | Global MFU (%): 36.16 | Global TFLOPs: 4290.94 | 
2025-05-19 11:34:38,506 - root - INFO - Step: 780 | Loss: 5.60 | Tokens per second: 38570.30 | Training tokens per second (%): 8.31 | MFU (%): 36.12 | TFLOPs: 357.22 | Global batch size: 96 | Global tokens/sec: 462843.63 | Global MFU (%): 36.12 | Global TFLOPs: 4286.59 | 
2025-05-19 11:34:42,742 - root - INFO - Step: 790 | Loss: 5.59 | Tokens per second: 38679.78 | Training tokens per second (%): 8.31 | MFU (%): 36.22 | TFLOPs: 358.23 | Global batch size: 96 | Global tokens/sec: 464157.31 | Global MFU (%): 36.22 | Global TFLOPs: 4298.76 | 
2025-05-19 11:34:46,981 - root - INFO - Step: 800 | Loss: 5.63 | Tokens per second: 38656.55 | Training tokens per second (%): 8.31 | MFU (%): 36.20 | TFLOPs: 358.01 | Global batch size: 96 | Global tokens/sec: 463878.64 | Global MFU (%): 36.20 | Global TFLOPs: 4296.18 | 
2025-05-19 11:34:51,223 - root - INFO - Step: 810 | Loss: 5.60 | Tokens per second: 38630.61 | Training tokens per second (%): 8.31 | MFU (%): 36.18 | TFLOPs: 357.77 | Global batch size: 96 | Global tokens/sec: 463567.27 | Global MFU (%): 36.18 | Global TFLOPs: 4293.30 | 
2025-05-19 11:34:55,474 - root - INFO - Step: 820 | Loss: 5.57 | Tokens per second: 38547.89 | Training tokens per second (%): 8.31 | MFU (%): 36.10 | TFLOPs: 357.01 | Global batch size: 96 | Global tokens/sec: 462574.64 | Global MFU (%): 36.10 | Global TFLOPs: 4284.10 | 
2025-05-19 11:34:59,712 - root - INFO - Step: 830 | Loss: 5.55 | Tokens per second: 38662.74 | Training tokens per second (%): 8.31 | MFU (%): 36.21 | TFLOPs: 358.07 | Global batch size: 96 | Global tokens/sec: 463952.92 | Global MFU (%): 36.21 | Global TFLOPs: 4296.87 | 
2025-05-19 11:35:03,950 - root - INFO - Step: 840 | Loss: 5.57 | Tokens per second: 38665.81 | Training tokens per second (%): 8.31 | MFU (%): 36.21 | TFLOPs: 358.10 | Global batch size: 96 | Global tokens/sec: 463989.71 | Global MFU (%): 36.21 | Global TFLOPs: 4297.21 | 
2025-05-19 11:35:08,195 - root - INFO - Step: 850 | Loss: 5.54 | Tokens per second: 38602.00 | Training tokens per second (%): 8.31 | MFU (%): 36.15 | TFLOPs: 357.51 | Global batch size: 96 | Global tokens/sec: 463223.96 | Global MFU (%): 36.15 | Global TFLOPs: 4290.12 | 
2025-05-19 11:35:12,438 - root - INFO - Step: 860 | Loss: 5.54 | Tokens per second: 38624.38 | Training tokens per second (%): 8.31 | MFU (%): 36.17 | TFLOPs: 357.72 | Global batch size: 96 | Global tokens/sec: 463492.54 | Global MFU (%): 36.17 | Global TFLOPs: 4292.60 | 
2025-05-19 11:35:16,682 - root - INFO - Step: 870 | Loss: 5.56 | Tokens per second: 38609.63 | Training tokens per second (%): 8.31 | MFU (%): 36.16 | TFLOPs: 357.58 | Global batch size: 96 | Global tokens/sec: 463315.58 | Global MFU (%): 36.16 | Global TFLOPs: 4290.96 | 
2025-05-19 11:35:20,921 - root - INFO - Step: 880 | Loss: 5.54 | Tokens per second: 38658.01 | Training tokens per second (%): 8.31 | MFU (%): 36.20 | TFLOPs: 358.03 | Global batch size: 96 | Global tokens/sec: 463896.14 | Global MFU (%): 36.20 | Global TFLOPs: 4296.34 | 
2025-05-19 11:35:25,160 - root - INFO - Step: 890 | Loss: 5.56 | Tokens per second: 38654.85 | Training tokens per second (%): 8.31 | MFU (%): 36.20 | TFLOPs: 358.00 | Global batch size: 96 | Global tokens/sec: 463858.23 | Global MFU (%): 36.20 | Global TFLOPs: 4295.99 | 
2025-05-19 11:35:29,400 - root - INFO - Step: 900 | Loss: 5.53 | Tokens per second: 38646.49 | Training tokens per second (%): 8.31 | MFU (%): 36.19 | TFLOPs: 357.92 | Global batch size: 96 | Global tokens/sec: 463757.84 | Global MFU (%): 36.19 | Global TFLOPs: 4295.06 | 
2025-05-19 11:35:33,637 - root - INFO - Step: 910 | Loss: 5.53 | Tokens per second: 38675.86 | Training tokens per second (%): 8.31 | MFU (%): 36.22 | TFLOPs: 358.19 | Global batch size: 96 | Global tokens/sec: 464110.35 | Global MFU (%): 36.22 | Global TFLOPs: 4298.32 | 
2025-05-19 11:35:37,877 - root - INFO - Step: 920 | Loss: 5.46 | Tokens per second: 38648.62 | Training tokens per second (%): 8.31 | MFU (%): 36.19 | TFLOPs: 357.94 | Global batch size: 96 | Global tokens/sec: 463783.43 | Global MFU (%): 36.19 | Global TFLOPs: 4295.30 | 
2025-05-19 11:35:42,110 - root - INFO - Step: 930 | Loss: 5.47 | Tokens per second: 38710.31 | Training tokens per second (%): 8.31 | MFU (%): 36.25 | TFLOPs: 358.51 | Global batch size: 96 | Global tokens/sec: 464523.70 | Global MFU (%): 36.25 | Global TFLOPs: 4302.15 | 
2025-05-19 11:35:46,356 - root - INFO - Step: 940 | Loss: 5.48 | Tokens per second: 38594.91 | Training tokens per second (%): 8.31 | MFU (%): 36.14 | TFLOPs: 357.44 | Global batch size: 96 | Global tokens/sec: 463138.92 | Global MFU (%): 36.14 | Global TFLOPs: 4289.33 | 
2025-05-19 11:35:50,603 - root - INFO - Step: 950 | Loss: 5.48 | Tokens per second: 38586.44 | Training tokens per second (%): 8.31 | MFU (%): 36.13 | TFLOPs: 357.37 | Global batch size: 96 | Global tokens/sec: 463037.27 | Global MFU (%): 36.13 | Global TFLOPs: 4288.39 | 
2025-05-19 11:35:54,841 - root - INFO - Step: 960 | Loss: 5.45 | Tokens per second: 38667.41 | Training tokens per second (%): 8.31 | MFU (%): 36.21 | TFLOPs: 358.12 | Global batch size: 96 | Global tokens/sec: 464008.88 | Global MFU (%): 36.21 | Global TFLOPs: 4297.39 | 
2025-05-19 11:35:59,081 - root - INFO - Step: 970 | Loss: 5.45 | Tokens per second: 38642.44 | Training tokens per second (%): 8.31 | MFU (%): 36.19 | TFLOPs: 357.88 | Global batch size: 96 | Global tokens/sec: 463709.29 | Global MFU (%): 36.19 | Global TFLOPs: 4294.61 | 
2025-05-19 11:36:03,326 - root - INFO - Step: 980 | Loss: 5.36 | Tokens per second: 38608.69 | Training tokens per second (%): 8.31 | MFU (%): 36.15 | TFLOPs: 357.57 | Global batch size: 96 | Global tokens/sec: 463304.24 | Global MFU (%): 36.15 | Global TFLOPs: 4290.86 | 
2025-05-19 11:36:07,566 - root - INFO - Step: 990 | Loss: 5.45 | Tokens per second: 38645.53 | Training tokens per second (%): 8.31 | MFU (%): 36.19 | TFLOPs: 357.91 | Global batch size: 96 | Global tokens/sec: 463746.36 | Global MFU (%): 36.19 | Global TFLOPs: 4294.95 | 
2025-05-19 11:36:11,805 - root - INFO - Step: 1000 | Loss: 5.47 | Tokens per second: 38659.00 | Training tokens per second (%): 8.31 | MFU (%): 36.20 | TFLOPs: 358.04 | Global batch size: 96 | Global tokens/sec: 463907.97 | Global MFU (%): 36.20 | Global TFLOPs: 4296.45 | 
2025-05-19 11:36:11,805 - root - INFO - Training completed
[sbatch-master] task finished
