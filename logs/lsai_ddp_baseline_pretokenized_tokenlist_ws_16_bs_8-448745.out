[sbatch-master] running on nid006789
[sbatch-master] SLURM_NODELIST: nid[006789,007006-007008]
[sbatch-master] SLURM_NNODES: 4
[sbatch-master] SLURM_NODEID: 0
[Master] Master node: nid006789
[Master] World size: 16
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
[srun] rank=0 host=nid006789 noderank=0 localrank=0
W0519 11:03:19.741000 290567 torch/distributed/run.py:792] 
W0519 11:03:19.741000 290567 torch/distributed/run.py:792] *****************************************
W0519 11:03:19.741000 290567 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0519 11:03:19.741000 290567 torch/distributed/run.py:792] *****************************************
[srun] rank=2 host=nid007007 noderank=2 localrank=0
[srun] rank=1 host=nid007006 noderank=1 localrank=0
[srun] rank=3 host=nid007008 noderank=3 localrank=0
W0519 11:03:23.994000 185759 torch/distributed/run.py:792] 
W0519 11:03:23.994000 185759 torch/distributed/run.py:792] *****************************************
W0519 11:03:23.994000 185759 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0519 11:03:23.994000 185759 torch/distributed/run.py:792] *****************************************
W0519 11:03:24.013000 265156 torch/distributed/run.py:792] 
W0519 11:03:24.013000 265156 torch/distributed/run.py:792] *****************************************
W0519 11:03:24.013000 265156 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0519 11:03:24.013000 265156 torch/distributed/run.py:792] *****************************************
W0519 11:03:24.049000 251178 torch/distributed/run.py:792] 
W0519 11:03:24.049000 251178 torch/distributed/run.py:792] *****************************************
W0519 11:03:24.049000 251178 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0519 11:03:24.049000 251178 torch/distributed/run.py:792] *****************************************
2025-05-19 11:03:36,338 - root - INFO - [Distributed Init] Rank 0 initialized on node 0 on GPU 0.
[rank0]:[W519 11:03:36.545012679 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:03:36,791 - root - INFO - [Distributed Init] Rank 2 initialized on node 0 on GPU 2.
[rank2]:[W519 11:03:36.562090385 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:03:36,931 - root - INFO - [Distributed Init] Rank 3 initialized on node 0 on GPU 3.
[rank3]:[W519 11:03:36.702740918 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:03:36,991 - root - INFO - [Distributed Init] Rank 1 initialized on node 0 on GPU 1.
[rank1]:[W519 11:03:36.767747107 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:03:37,504 - root - INFO - [Distributed Init] Rank 4 initialized on node 1 on GPU 0.
2025-05-19 11:03:37,537 - root - INFO - [Distributed Init] Rank 12 initialized on node 3 on GPU 0.
2025-05-19 11:03:38,083 - root - INFO - [Distributed Init] Rank 8 initialized on node 2 on GPU 0.
[rank4]:[W519 11:03:38.837660146 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank12]:[W519 11:03:38.902100751 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 12]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:03:38,229 - root - INFO - [Distributed Init] Rank 14 initialized on node 3 on GPU 2.
[rank14]:[W519 11:03:38.992403505 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 14]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:03:38,242 - root - INFO - [Distributed Init] Rank 5 initialized on node 1 on GPU 1.
[rank5]:[W519 11:03:38.943043078 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 5]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:03:38,289 - root - INFO - [Distributed Init] Rank 15 initialized on node 3 on GPU 3.
2025-05-19 11:03:38,289 - root - INFO - [Distributed Init] Rank 13 initialized on node 3 on GPU 1.
[rank15]:[W519 11:03:38.053235980 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 15]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank13]:[W519 11:03:38.053348969 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 13]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:03:38,332 - root - INFO - [Distributed Init] Rank 7 initialized on node 1 on GPU 3.
[rank7]:[W519 11:03:38.033229934 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 7]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:03:38,341 - root - INFO - [Distributed Init] Rank 6 initialized on node 1 on GPU 2.
[rank6]:[W519 11:03:38.042196870 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 6]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank8]:[W519 11:03:38.729210039 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 8]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:03:38,802 - root - INFO - [Distributed Init] Rank 9 initialized on node 2 on GPU 1.
[rank9]:[W519 11:03:38.832653883 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 9]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:03:38,832 - root - INFO - [Distributed Init] Rank 10 initialized on node 2 on GPU 2.
[rank10]:[W519 11:03:38.862187467 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 10]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:03:43,790 - root - INFO - [Distributed Init] Rank 11 initialized on node 2 on GPU 3.
[rank11]:[W519 11:03:43.820749171 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 11]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:03:47,616 - root - INFO - [Rank 0] All ranks ready!
2025-05-19 11:03:47,616 - root - INFO - Distributed training enabled: 16 processes
2025-05-19 11:03:47,616 - root - INFO - Master process: 0 on cuda:0
2025-05-19 11:03:47,616 - root - INFO - Experiment args: Namespace(dataset='/capstor/scratch/cscs/kasparr/project/train_data_tokenized_token-list_snappy.parquet', dataset_type='token-list', pretokenized=True, tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=10, profile=False, profile_step_start=10, profile_step_end=10, grad_max_norm=1, model_dtype='bf16', compile=False, distributed=True, backend='nccl', find_unused_parameters=False)
2025-05-19 11:03:47,616 - root - INFO - Setting up Tokenizer...
2025-05-19 11:03:48,181 - root - INFO - Setting up DataLoaders...
2025-05-19 11:03:48,181 - root - INFO - Using pretokenized data: /capstor/scratch/cscs/kasparr/project/train_data_tokenized_token-list_snappy.parquet
2025-05-19 11:03:56,381 - root - INFO - Setting up Model...
2025-05-19 11:04:05,105 - root - INFO - Model wrapped with DistributedDataParallel
2025-05-19 11:04:05,106 - root - INFO - Global batch size: 128 (local: 8 × 16 processes)
2025-05-19 11:04:05,107 - root - INFO - Starting training!
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
2025-05-19 11:04:06,799 - root - INFO - Step: 1 | Loss: 11.94 | Tokens per second: 9688.69 | Training tokens per second (%): 6.24 | MFU (%): 9.07 | TFLOPs: 89.73 | Global batch size: 128 | Global tokens/sec: 155019.04 | Global MFU (%): 9.07 | Global TFLOPs: 1435.70 | 
2025-05-19 11:04:11,282 - root - INFO - Step: 10 | Loss: 11.90 | Tokens per second: 32897.13 | Training tokens per second (%): 6.24 | MFU (%): 30.81 | TFLOPs: 304.67 | Global batch size: 128 | Global tokens/sec: 526354.06 | Global MFU (%): 30.81 | Global TFLOPs: 4874.79 | 
2025-05-19 11:04:15,574 - root - INFO - Step: 20 | Loss: 11.65 | Tokens per second: 38177.66 | Training tokens per second (%): 6.23 | MFU (%): 35.75 | TFLOPs: 353.58 | Global batch size: 128 | Global tokens/sec: 610842.51 | Global MFU (%): 35.75 | Global TFLOPs: 5657.27 | 
2025-05-19 11:04:19,865 - root - INFO - Step: 30 | Loss: 10.98 | Tokens per second: 38187.63 | Training tokens per second (%): 6.23 | MFU (%): 35.76 | TFLOPs: 353.67 | Global batch size: 128 | Global tokens/sec: 611002.01 | Global MFU (%): 35.76 | Global TFLOPs: 5658.75 | 
2025-05-19 11:04:24,163 - root - INFO - Step: 40 | Loss: 9.96 | Tokens per second: 38131.96 | Training tokens per second (%): 6.23 | MFU (%): 35.71 | TFLOPs: 353.16 | Global batch size: 128 | Global tokens/sec: 610111.29 | Global MFU (%): 35.71 | Global TFLOPs: 5650.50 | 
2025-05-19 11:04:28,454 - root - INFO - Step: 50 | Loss: 9.30 | Tokens per second: 38190.22 | Training tokens per second (%): 6.23 | MFU (%): 35.76 | TFLOPs: 353.70 | Global batch size: 128 | Global tokens/sec: 611043.46 | Global MFU (%): 35.76 | Global TFLOPs: 5659.14 | 
2025-05-19 11:04:32,749 - root - INFO - Step: 60 | Loss: 8.70 | Tokens per second: 38151.55 | Training tokens per second (%): 6.23 | MFU (%): 35.73 | TFLOPs: 353.34 | Global batch size: 128 | Global tokens/sec: 610424.77 | Global MFU (%): 35.73 | Global TFLOPs: 5653.41 | 
2025-05-19 11:04:37,041 - root - INFO - Step: 70 | Loss: 8.18 | Tokens per second: 38180.73 | Training tokens per second (%): 6.23 | MFU (%): 35.75 | TFLOPs: 353.61 | Global batch size: 128 | Global tokens/sec: 610891.66 | Global MFU (%): 35.75 | Global TFLOPs: 5657.73 | 
2025-05-19 11:04:41,332 - root - INFO - Step: 80 | Loss: 7.77 | Tokens per second: 38188.87 | Training tokens per second (%): 6.23 | MFU (%): 35.76 | TFLOPs: 353.68 | Global batch size: 128 | Global tokens/sec: 611021.84 | Global MFU (%): 35.76 | Global TFLOPs: 5658.94 | 
2025-05-19 11:04:45,623 - root - INFO - Step: 90 | Loss: 7.46 | Tokens per second: 38187.32 | Training tokens per second (%): 6.24 | MFU (%): 35.76 | TFLOPs: 353.67 | Global batch size: 128 | Global tokens/sec: 610997.07 | Global MFU (%): 35.76 | Global TFLOPs: 5658.71 | 
2025-05-19 11:04:49,907 - root - INFO - Step: 100 | Loss: 7.32 | Tokens per second: 38252.86 | Training tokens per second (%): 6.23 | MFU (%): 35.82 | TFLOPs: 354.28 | Global batch size: 128 | Global tokens/sec: 612045.68 | Global MFU (%): 35.82 | Global TFLOPs: 5668.42 | 
2025-05-19 11:04:54,198 - root - INFO - Step: 110 | Loss: 7.17 | Tokens per second: 38181.97 | Training tokens per second (%): 6.23 | MFU (%): 35.76 | TFLOPs: 353.62 | Global batch size: 128 | Global tokens/sec: 610911.45 | Global MFU (%): 35.76 | Global TFLOPs: 5657.91 | 
2025-05-19 11:04:58,492 - root - INFO - Step: 120 | Loss: 7.10 | Tokens per second: 38164.12 | Training tokens per second (%): 6.23 | MFU (%): 35.74 | TFLOPs: 353.45 | Global batch size: 128 | Global tokens/sec: 610625.96 | Global MFU (%): 35.74 | Global TFLOPs: 5655.27 | 
2025-05-19 11:05:02,840 - root - INFO - Step: 130 | Loss: 7.03 | Tokens per second: 37688.48 | Training tokens per second (%): 6.23 | MFU (%): 35.29 | TFLOPs: 349.05 | Global batch size: 128 | Global tokens/sec: 603015.64 | Global MFU (%): 35.29 | Global TFLOPs: 5584.79 | 
2025-05-19 11:05:07,139 - root - INFO - Step: 140 | Loss: 6.94 | Tokens per second: 38123.15 | Training tokens per second (%): 6.24 | MFU (%): 35.70 | TFLOPs: 353.07 | Global batch size: 128 | Global tokens/sec: 609970.37 | Global MFU (%): 35.70 | Global TFLOPs: 5649.20 | 
2025-05-19 11:05:11,431 - root - INFO - Step: 150 | Loss: 6.84 | Tokens per second: 38176.79 | Training tokens per second (%): 6.23 | MFU (%): 35.75 | TFLOPs: 353.57 | Global batch size: 128 | Global tokens/sec: 610828.56 | Global MFU (%): 35.75 | Global TFLOPs: 5657.15 | 
2025-05-19 11:05:15,725 - root - INFO - Step: 160 | Loss: 6.79 | Tokens per second: 38162.06 | Training tokens per second (%): 6.23 | MFU (%): 35.74 | TFLOPs: 353.44 | Global batch size: 128 | Global tokens/sec: 610592.96 | Global MFU (%): 35.74 | Global TFLOPs: 5654.96 | 
2025-05-19 11:05:20,054 - root - INFO - Step: 170 | Loss: 6.67 | Tokens per second: 37854.06 | Training tokens per second (%): 6.23 | MFU (%): 35.45 | TFLOPs: 350.58 | Global batch size: 128 | Global tokens/sec: 605664.99 | Global MFU (%): 35.45 | Global TFLOPs: 5609.32 | 
2025-05-19 11:05:24,348 - root - INFO - Step: 180 | Loss: 6.68 | Tokens per second: 38159.60 | Training tokens per second (%): 6.23 | MFU (%): 35.73 | TFLOPs: 353.41 | Global batch size: 128 | Global tokens/sec: 610553.56 | Global MFU (%): 35.73 | Global TFLOPs: 5654.60 | 
2025-05-19 11:05:28,641 - root - INFO - Step: 190 | Loss: 6.79 | Tokens per second: 38173.10 | Training tokens per second (%): 6.23 | MFU (%): 35.75 | TFLOPs: 353.54 | Global batch size: 128 | Global tokens/sec: 610769.64 | Global MFU (%): 35.75 | Global TFLOPs: 5656.60 | 
2025-05-19 11:05:32,964 - root - INFO - Step: 200 | Loss: 6.64 | Tokens per second: 37905.85 | Training tokens per second (%): 6.23 | MFU (%): 35.50 | TFLOPs: 351.06 | Global batch size: 128 | Global tokens/sec: 606493.65 | Global MFU (%): 35.50 | Global TFLOPs: 5617.00 | 
2025-05-19 11:05:37,343 - root - INFO - Step: 210 | Loss: 6.54 | Tokens per second: 37417.89 | Training tokens per second (%): 6.23 | MFU (%): 35.04 | TFLOPs: 346.54 | Global batch size: 128 | Global tokens/sec: 598686.31 | Global MFU (%): 35.04 | Global TFLOPs: 5544.69 | 
2025-05-19 11:05:41,633 - root - INFO - Step: 220 | Loss: 6.48 | Tokens per second: 38200.97 | Training tokens per second (%): 6.23 | MFU (%): 35.77 | TFLOPs: 353.80 | Global batch size: 128 | Global tokens/sec: 611215.55 | Global MFU (%): 35.77 | Global TFLOPs: 5660.73 | 
2025-05-19 11:05:45,923 - root - INFO - Step: 230 | Loss: 6.47 | Tokens per second: 38203.64 | Training tokens per second (%): 6.23 | MFU (%): 35.78 | TFLOPs: 353.82 | Global batch size: 128 | Global tokens/sec: 611258.19 | Global MFU (%): 35.78 | Global TFLOPs: 5661.12 | 
2025-05-19 11:05:50,214 - root - INFO - Step: 240 | Loss: 6.34 | Tokens per second: 38185.00 | Training tokens per second (%): 6.23 | MFU (%): 35.76 | TFLOPs: 353.65 | Global batch size: 128 | Global tokens/sec: 610960.01 | Global MFU (%): 35.76 | Global TFLOPs: 5658.36 | 
2025-05-19 11:05:54,494 - root - INFO - Step: 250 | Loss: 6.35 | Tokens per second: 38290.81 | Training tokens per second (%): 6.23 | MFU (%): 35.86 | TFLOPs: 354.63 | Global batch size: 128 | Global tokens/sec: 612652.95 | Global MFU (%): 35.86 | Global TFLOPs: 5674.04 | 
2025-05-19 11:05:58,789 - root - INFO - Step: 260 | Loss: 6.33 | Tokens per second: 38154.69 | Training tokens per second (%): 6.23 | MFU (%): 35.73 | TFLOPs: 353.37 | Global batch size: 128 | Global tokens/sec: 610475.10 | Global MFU (%): 35.73 | Global TFLOPs: 5653.87 | 
2025-05-19 11:06:03,079 - root - INFO - Step: 270 | Loss: 6.31 | Tokens per second: 38196.00 | Training tokens per second (%): 6.23 | MFU (%): 35.77 | TFLOPs: 353.75 | Global batch size: 128 | Global tokens/sec: 611135.97 | Global MFU (%): 35.77 | Global TFLOPs: 5659.99 | 
2025-05-19 11:06:07,418 - root - INFO - Step: 280 | Loss: 6.32 | Tokens per second: 37767.43 | Training tokens per second (%): 6.23 | MFU (%): 35.37 | TFLOPs: 349.78 | Global batch size: 128 | Global tokens/sec: 604278.93 | Global MFU (%): 35.37 | Global TFLOPs: 5596.49 | 
2025-05-19 11:06:11,844 - root - INFO - Step: 290 | Loss: 6.25 | Tokens per second: 37018.01 | Training tokens per second (%): 6.23 | MFU (%): 34.67 | TFLOPs: 342.84 | Global batch size: 128 | Global tokens/sec: 592288.12 | Global MFU (%): 34.67 | Global TFLOPs: 5485.43 | 
2025-05-19 11:06:16,125 - root - INFO - Step: 300 | Loss: 6.24 | Tokens per second: 38282.29 | Training tokens per second (%): 6.24 | MFU (%): 35.85 | TFLOPs: 354.55 | Global batch size: 128 | Global tokens/sec: 612516.69 | Global MFU (%): 35.85 | Global TFLOPs: 5672.78 | 
2025-05-19 11:06:20,410 - root - INFO - Step: 310 | Loss: 6.19 | Tokens per second: 38240.48 | Training tokens per second (%): 6.23 | MFU (%): 35.81 | TFLOPs: 354.16 | Global batch size: 128 | Global tokens/sec: 611847.62 | Global MFU (%): 35.81 | Global TFLOPs: 5666.58 | 
2025-05-19 11:06:24,698 - root - INFO - Step: 320 | Loss: 6.13 | Tokens per second: 38220.76 | Training tokens per second (%): 6.23 | MFU (%): 35.79 | TFLOPs: 353.98 | Global batch size: 128 | Global tokens/sec: 611532.18 | Global MFU (%): 35.79 | Global TFLOPs: 5663.66 | 
2025-05-19 11:06:28,979 - root - INFO - Step: 330 | Loss: 6.15 | Tokens per second: 38278.36 | Training tokens per second (%): 6.23 | MFU (%): 35.85 | TFLOPs: 354.51 | Global batch size: 128 | Global tokens/sec: 612453.72 | Global MFU (%): 35.85 | Global TFLOPs: 5672.20 | 
2025-05-19 11:06:33,263 - root - INFO - Step: 340 | Loss: 6.10 | Tokens per second: 38248.26 | Training tokens per second (%): 6.23 | MFU (%): 35.82 | TFLOPs: 354.23 | Global batch size: 128 | Global tokens/sec: 611972.13 | Global MFU (%): 35.82 | Global TFLOPs: 5667.74 | 
2025-05-19 11:06:37,549 - root - INFO - Step: 350 | Loss: 6.09 | Tokens per second: 38239.58 | Training tokens per second (%): 6.23 | MFU (%): 35.81 | TFLOPs: 354.15 | Global batch size: 128 | Global tokens/sec: 611833.24 | Global MFU (%): 35.81 | Global TFLOPs: 5666.45 | 
2025-05-19 11:06:41,832 - root - INFO - Step: 360 | Loss: 6.08 | Tokens per second: 38254.80 | Training tokens per second (%): 6.23 | MFU (%): 35.82 | TFLOPs: 354.29 | Global batch size: 128 | Global tokens/sec: 612076.76 | Global MFU (%): 35.82 | Global TFLOPs: 5668.71 | 
2025-05-19 11:06:46,118 - root - INFO - Step: 370 | Loss: 6.09 | Tokens per second: 38230.68 | Training tokens per second (%): 6.23 | MFU (%): 35.80 | TFLOPs: 354.07 | Global batch size: 128 | Global tokens/sec: 611690.95 | Global MFU (%): 35.80 | Global TFLOPs: 5665.13 | 
2025-05-19 11:06:50,407 - root - INFO - Step: 380 | Loss: 6.02 | Tokens per second: 38214.73 | Training tokens per second (%): 6.23 | MFU (%): 35.79 | TFLOPs: 353.92 | Global batch size: 128 | Global tokens/sec: 611435.64 | Global MFU (%): 35.79 | Global TFLOPs: 5662.77 | 
2025-05-19 11:06:54,700 - root - INFO - Step: 390 | Loss: 5.95 | Tokens per second: 38169.26 | Training tokens per second (%): 6.23 | MFU (%): 35.74 | TFLOPs: 353.50 | Global batch size: 128 | Global tokens/sec: 610708.23 | Global MFU (%): 35.74 | Global TFLOPs: 5656.03 | 
2025-05-19 11:06:58,980 - root - INFO - Step: 400 | Loss: 5.98 | Tokens per second: 38286.03 | Training tokens per second (%): 6.23 | MFU (%): 35.85 | TFLOPs: 354.58 | Global batch size: 128 | Global tokens/sec: 612576.50 | Global MFU (%): 35.85 | Global TFLOPs: 5673.33 | 
2025-05-19 11:07:03,264 - root - INFO - Step: 410 | Loss: 5.97 | Tokens per second: 38248.07 | Training tokens per second (%): 6.23 | MFU (%): 35.82 | TFLOPs: 354.23 | Global batch size: 128 | Global tokens/sec: 611969.17 | Global MFU (%): 35.82 | Global TFLOPs: 5667.71 | 
2025-05-19 11:07:07,556 - root - INFO - Step: 420 | Loss: 5.91 | Tokens per second: 38182.06 | Training tokens per second (%): 6.23 | MFU (%): 35.76 | TFLOPs: 353.62 | Global batch size: 128 | Global tokens/sec: 610912.99 | Global MFU (%): 35.76 | Global TFLOPs: 5657.93 | 
2025-05-19 11:07:11,844 - root - INFO - Step: 430 | Loss: 5.96 | Tokens per second: 38218.10 | Training tokens per second (%): 6.24 | MFU (%): 35.79 | TFLOPs: 353.95 | Global batch size: 128 | Global tokens/sec: 611489.67 | Global MFU (%): 35.79 | Global TFLOPs: 5663.27 | 
2025-05-19 11:07:16,127 - root - INFO - Step: 440 | Loss: 5.89 | Tokens per second: 38255.53 | Training tokens per second (%): 6.24 | MFU (%): 35.82 | TFLOPs: 354.30 | Global batch size: 128 | Global tokens/sec: 612088.49 | Global MFU (%): 35.82 | Global TFLOPs: 5668.81 | 
2025-05-19 11:07:20,411 - root - INFO - Step: 450 | Loss: 5.88 | Tokens per second: 38257.21 | Training tokens per second (%): 6.23 | MFU (%): 35.83 | TFLOPs: 354.32 | Global batch size: 128 | Global tokens/sec: 612115.31 | Global MFU (%): 35.83 | Global TFLOPs: 5669.06 | 
2025-05-19 11:07:24,699 - root - INFO - Step: 460 | Loss: 5.86 | Tokens per second: 38217.69 | Training tokens per second (%): 6.23 | MFU (%): 35.79 | TFLOPs: 353.95 | Global batch size: 128 | Global tokens/sec: 611482.99 | Global MFU (%): 35.79 | Global TFLOPs: 5663.21 | 
2025-05-19 11:07:28,986 - root - INFO - Step: 470 | Loss: 5.82 | Tokens per second: 38221.86 | Training tokens per second (%): 6.23 | MFU (%): 35.79 | TFLOPs: 353.99 | Global batch size: 128 | Global tokens/sec: 611549.70 | Global MFU (%): 35.79 | Global TFLOPs: 5663.82 | 
2025-05-19 11:07:33,270 - root - INFO - Step: 480 | Loss: 6.04 | Tokens per second: 38246.06 | Training tokens per second (%): 6.23 | MFU (%): 35.82 | TFLOPs: 354.21 | Global batch size: 128 | Global tokens/sec: 611936.98 | Global MFU (%): 35.82 | Global TFLOPs: 5667.41 | 
2025-05-19 11:07:37,559 - root - INFO - Step: 490 | Loss: 5.97 | Tokens per second: 38211.44 | Training tokens per second (%): 6.23 | MFU (%): 35.78 | TFLOPs: 353.89 | Global batch size: 128 | Global tokens/sec: 611383.09 | Global MFU (%): 35.78 | Global TFLOPs: 5662.28 | 
2025-05-19 11:07:41,851 - root - INFO - Step: 500 | Loss: 5.87 | Tokens per second: 38174.44 | Training tokens per second (%): 6.24 | MFU (%): 35.75 | TFLOPs: 353.55 | Global batch size: 128 | Global tokens/sec: 610791.02 | Global MFU (%): 35.75 | Global TFLOPs: 5656.80 | 
2025-05-19 11:07:46,138 - root - INFO - Step: 510 | Loss: 5.83 | Tokens per second: 38224.50 | Training tokens per second (%): 6.23 | MFU (%): 35.80 | TFLOPs: 354.01 | Global batch size: 128 | Global tokens/sec: 611591.96 | Global MFU (%): 35.80 | Global TFLOPs: 5664.22 | 
2025-05-19 11:07:50,415 - root - INFO - Step: 520 | Loss: 5.84 | Tokens per second: 38319.50 | Training tokens per second (%): 6.23 | MFU (%): 35.88 | TFLOPs: 354.89 | Global batch size: 128 | Global tokens/sec: 613111.98 | Global MFU (%): 35.88 | Global TFLOPs: 5678.29 | 
2025-05-19 11:07:54,697 - root - INFO - Step: 530 | Loss: 5.80 | Tokens per second: 38269.25 | Training tokens per second (%): 6.23 | MFU (%): 35.84 | TFLOPs: 354.43 | Global batch size: 128 | Global tokens/sec: 612308.08 | Global MFU (%): 35.84 | Global TFLOPs: 5670.85 | 
2025-05-19 11:07:58,978 - root - INFO - Step: 540 | Loss: 5.80 | Tokens per second: 38273.77 | Training tokens per second (%): 6.23 | MFU (%): 35.84 | TFLOPs: 354.47 | Global batch size: 128 | Global tokens/sec: 612380.31 | Global MFU (%): 35.84 | Global TFLOPs: 5671.52 | 
2025-05-19 11:08:03,265 - root - INFO - Step: 550 | Loss: 5.81 | Tokens per second: 38224.30 | Training tokens per second (%): 6.23 | MFU (%): 35.79 | TFLOPs: 354.01 | Global batch size: 128 | Global tokens/sec: 611588.87 | Global MFU (%): 35.79 | Global TFLOPs: 5664.19 | 
2025-05-19 11:08:07,550 - root - INFO - Step: 560 | Loss: 5.73 | Tokens per second: 38244.17 | Training tokens per second (%): 6.23 | MFU (%): 35.81 | TFLOPs: 354.20 | Global batch size: 128 | Global tokens/sec: 611906.68 | Global MFU (%): 35.81 | Global TFLOPs: 5667.13 | 
2025-05-19 11:08:11,832 - root - INFO - Step: 570 | Loss: 5.75 | Tokens per second: 38272.07 | Training tokens per second (%): 6.23 | MFU (%): 35.84 | TFLOPs: 354.45 | Global batch size: 128 | Global tokens/sec: 612353.12 | Global MFU (%): 35.84 | Global TFLOPs: 5671.26 | 
2025-05-19 11:08:16,115 - root - INFO - Step: 580 | Loss: 5.76 | Tokens per second: 38256.27 | Training tokens per second (%): 6.23 | MFU (%): 35.82 | TFLOPs: 354.31 | Global batch size: 128 | Global tokens/sec: 612100.38 | Global MFU (%): 35.82 | Global TFLOPs: 5668.92 | 
2025-05-19 11:08:20,401 - root - INFO - Step: 590 | Loss: 5.75 | Tokens per second: 38236.70 | Training tokens per second (%): 6.23 | MFU (%): 35.81 | TFLOPs: 354.13 | Global batch size: 128 | Global tokens/sec: 611787.27 | Global MFU (%): 35.81 | Global TFLOPs: 5666.02 | 
2025-05-19 11:08:24,688 - root - INFO - Step: 600 | Loss: 5.68 | Tokens per second: 38223.84 | Training tokens per second (%): 6.23 | MFU (%): 35.79 | TFLOPs: 354.01 | Global batch size: 128 | Global tokens/sec: 611581.44 | Global MFU (%): 35.79 | Global TFLOPs: 5664.12 | 
2025-05-19 11:08:28,970 - root - INFO - Step: 610 | Loss: 5.69 | Tokens per second: 38270.85 | Training tokens per second (%): 6.23 | MFU (%): 35.84 | TFLOPs: 354.44 | Global batch size: 128 | Global tokens/sec: 612333.60 | Global MFU (%): 35.84 | Global TFLOPs: 5671.08 | 
2025-05-19 11:08:33,252 - root - INFO - Step: 620 | Loss: 5.74 | Tokens per second: 38271.68 | Training tokens per second (%): 6.23 | MFU (%): 35.84 | TFLOPs: 354.45 | Global batch size: 128 | Global tokens/sec: 612346.83 | Global MFU (%): 35.84 | Global TFLOPs: 5671.21 | 
2025-05-19 11:08:37,535 - root - INFO - Step: 630 | Loss: 5.64 | Tokens per second: 38258.37 | Training tokens per second (%): 6.24 | MFU (%): 35.83 | TFLOPs: 354.33 | Global batch size: 128 | Global tokens/sec: 612133.89 | Global MFU (%): 35.83 | Global TFLOPs: 5669.23 | 
2025-05-19 11:08:41,818 - root - INFO - Step: 640 | Loss: 5.64 | Tokens per second: 38262.08 | Training tokens per second (%): 6.23 | MFU (%): 35.83 | TFLOPs: 354.36 | Global batch size: 128 | Global tokens/sec: 612193.28 | Global MFU (%): 35.83 | Global TFLOPs: 5669.78 | 
2025-05-19 11:08:46,107 - root - INFO - Step: 650 | Loss: 5.62 | Tokens per second: 38204.27 | Training tokens per second (%): 6.23 | MFU (%): 35.78 | TFLOPs: 353.83 | Global batch size: 128 | Global tokens/sec: 611268.27 | Global MFU (%): 35.78 | Global TFLOPs: 5661.22 | 
2025-05-19 11:08:50,387 - root - INFO - Step: 660 | Loss: 5.63 | Tokens per second: 38288.39 | Training tokens per second (%): 6.23 | MFU (%): 35.85 | TFLOPs: 354.61 | Global batch size: 128 | Global tokens/sec: 612614.31 | Global MFU (%): 35.85 | Global TFLOPs: 5673.68 | 
2025-05-19 11:08:54,674 - root - INFO - Step: 670 | Loss: 5.69 | Tokens per second: 38219.33 | Training tokens per second (%): 6.23 | MFU (%): 35.79 | TFLOPs: 353.97 | Global batch size: 128 | Global tokens/sec: 611509.33 | Global MFU (%): 35.79 | Global TFLOPs: 5663.45 | 
2025-05-19 11:08:58,965 - root - INFO - Step: 680 | Loss: 5.63 | Tokens per second: 38191.27 | Training tokens per second (%): 6.23 | MFU (%): 35.76 | TFLOPs: 353.71 | Global batch size: 128 | Global tokens/sec: 611060.27 | Global MFU (%): 35.76 | Global TFLOPs: 5659.29 | 
2025-05-19 11:09:03,248 - root - INFO - Step: 690 | Loss: 5.62 | Tokens per second: 38262.53 | Training tokens per second (%): 6.23 | MFU (%): 35.83 | TFLOPs: 354.37 | Global batch size: 128 | Global tokens/sec: 612200.41 | Global MFU (%): 35.83 | Global TFLOPs: 5669.85 | 
2025-05-19 11:09:07,534 - root - INFO - Step: 700 | Loss: 5.52 | Tokens per second: 38228.93 | Training tokens per second (%): 6.23 | MFU (%): 35.80 | TFLOPs: 354.05 | Global batch size: 128 | Global tokens/sec: 611662.80 | Global MFU (%): 35.80 | Global TFLOPs: 5664.87 | 
2025-05-19 11:09:11,825 - root - INFO - Step: 710 | Loss: 5.61 | Tokens per second: 38192.00 | Training tokens per second (%): 6.24 | MFU (%): 35.76 | TFLOPs: 353.71 | Global batch size: 128 | Global tokens/sec: 611072.03 | Global MFU (%): 35.76 | Global TFLOPs: 5659.40 | 
2025-05-19 11:09:16,116 - root - INFO - Step: 720 | Loss: 5.61 | Tokens per second: 38190.97 | Training tokens per second (%): 6.23 | MFU (%): 35.76 | TFLOPs: 353.70 | Global batch size: 128 | Global tokens/sec: 611055.51 | Global MFU (%): 35.76 | Global TFLOPs: 5659.25 | 
2025-05-19 11:09:20,403 - root - INFO - Step: 730 | Loss: 5.58 | Tokens per second: 38222.49 | Training tokens per second (%): 6.23 | MFU (%): 35.79 | TFLOPs: 353.99 | Global batch size: 128 | Global tokens/sec: 611559.79 | Global MFU (%): 35.79 | Global TFLOPs: 5663.92 | 
2025-05-19 11:09:24,693 - root - INFO - Step: 740 | Loss: 5.57 | Tokens per second: 38201.97 | Training tokens per second (%): 6.23 | MFU (%): 35.77 | TFLOPs: 353.80 | Global batch size: 128 | Global tokens/sec: 611231.49 | Global MFU (%): 35.77 | Global TFLOPs: 5660.88 | 
2025-05-19 11:09:28,981 - root - INFO - Step: 750 | Loss: 5.52 | Tokens per second: 38211.01 | Training tokens per second (%): 6.23 | MFU (%): 35.78 | TFLOPs: 353.89 | Global batch size: 128 | Global tokens/sec: 611376.11 | Global MFU (%): 35.78 | Global TFLOPs: 5662.22 | 
2025-05-19 11:09:33,271 - root - INFO - Step: 760 | Loss: 5.51 | Tokens per second: 38198.82 | Training tokens per second (%): 6.23 | MFU (%): 35.77 | TFLOPs: 353.78 | Global batch size: 128 | Global tokens/sec: 611181.12 | Global MFU (%): 35.77 | Global TFLOPs: 5660.41 | 
2025-05-19 11:09:37,558 - root - INFO - Step: 770 | Loss: 5.55 | Tokens per second: 38226.47 | Training tokens per second (%): 6.23 | MFU (%): 35.80 | TFLOPs: 354.03 | Global batch size: 128 | Global tokens/sec: 611623.45 | Global MFU (%): 35.80 | Global TFLOPs: 5664.51 | 
2025-05-19 11:09:41,847 - root - INFO - Step: 780 | Loss: 5.53 | Tokens per second: 38208.33 | Training tokens per second (%): 6.23 | MFU (%): 35.78 | TFLOPs: 353.86 | Global batch size: 128 | Global tokens/sec: 611333.23 | Global MFU (%): 35.78 | Global TFLOPs: 5661.82 | 
2025-05-19 11:09:46,130 - root - INFO - Step: 790 | Loss: 5.51 | Tokens per second: 38261.34 | Training tokens per second (%): 6.23 | MFU (%): 35.83 | TFLOPs: 354.35 | Global batch size: 128 | Global tokens/sec: 612181.43 | Global MFU (%): 35.83 | Global TFLOPs: 5669.67 | 
2025-05-19 11:09:50,416 - root - INFO - Step: 800 | Loss: 5.81 | Tokens per second: 38228.94 | Training tokens per second (%): 6.23 | MFU (%): 35.80 | TFLOPs: 354.05 | Global batch size: 128 | Global tokens/sec: 611663.10 | Global MFU (%): 35.80 | Global TFLOPs: 5664.87 | 
2025-05-19 11:09:54,697 - root - INFO - Step: 810 | Loss: 5.67 | Tokens per second: 38279.87 | Training tokens per second (%): 6.23 | MFU (%): 35.85 | TFLOPs: 354.53 | Global batch size: 128 | Global tokens/sec: 612477.90 | Global MFU (%): 35.85 | Global TFLOPs: 5672.42 | 
2025-05-19 11:09:58,987 - root - INFO - Step: 820 | Loss: 5.60 | Tokens per second: 38198.68 | Training tokens per second (%): 6.23 | MFU (%): 35.77 | TFLOPs: 353.77 | Global batch size: 128 | Global tokens/sec: 611178.82 | Global MFU (%): 35.77 | Global TFLOPs: 5660.39 | 
2025-05-19 11:10:03,276 - root - INFO - Step: 830 | Loss: 5.55 | Tokens per second: 38206.39 | Training tokens per second (%): 6.24 | MFU (%): 35.78 | TFLOPs: 353.85 | Global batch size: 128 | Global tokens/sec: 611302.24 | Global MFU (%): 35.78 | Global TFLOPs: 5661.53 | 
2025-05-19 11:10:07,562 - root - INFO - Step: 840 | Loss: 5.53 | Tokens per second: 38235.89 | Training tokens per second (%): 6.24 | MFU (%): 35.81 | TFLOPs: 354.12 | Global batch size: 128 | Global tokens/sec: 611774.23 | Global MFU (%): 35.81 | Global TFLOPs: 5665.90 | 
2025-05-19 11:10:11,850 - root - INFO - Step: 850 | Loss: 5.51 | Tokens per second: 38213.65 | Training tokens per second (%): 6.23 | MFU (%): 35.78 | TFLOPs: 353.91 | Global batch size: 128 | Global tokens/sec: 611418.35 | Global MFU (%): 35.78 | Global TFLOPs: 5662.61 | 
2025-05-19 11:10:16,131 - root - INFO - Step: 860 | Loss: 5.47 | Tokens per second: 38280.79 | Training tokens per second (%): 6.23 | MFU (%): 35.85 | TFLOPs: 354.53 | Global batch size: 128 | Global tokens/sec: 612492.71 | Global MFU (%): 35.85 | Global TFLOPs: 5672.56 | 
2025-05-19 11:10:20,420 - root - INFO - Step: 870 | Loss: 5.49 | Tokens per second: 38204.01 | Training tokens per second (%): 6.24 | MFU (%): 35.78 | TFLOPs: 353.82 | Global batch size: 128 | Global tokens/sec: 611264.13 | Global MFU (%): 35.78 | Global TFLOPs: 5661.18 | 
2025-05-19 11:10:24,704 - root - INFO - Step: 880 | Loss: 5.46 | Tokens per second: 38247.57 | Training tokens per second (%): 6.23 | MFU (%): 35.82 | TFLOPs: 354.23 | Global batch size: 128 | Global tokens/sec: 611961.11 | Global MFU (%): 35.82 | Global TFLOPs: 5667.63 | 
2025-05-19 11:10:29,000 - root - INFO - Step: 890 | Loss: 5.43 | Tokens per second: 38143.90 | Training tokens per second (%): 6.23 | MFU (%): 35.72 | TFLOPs: 353.27 | Global batch size: 128 | Global tokens/sec: 610302.35 | Global MFU (%): 35.72 | Global TFLOPs: 5652.27 | 
2025-05-19 11:10:33,293 - root - INFO - Step: 900 | Loss: 5.46 | Tokens per second: 38178.30 | Training tokens per second (%): 6.23 | MFU (%): 35.75 | TFLOPs: 353.59 | Global batch size: 128 | Global tokens/sec: 610852.83 | Global MFU (%): 35.75 | Global TFLOPs: 5657.37 | 
2025-05-19 11:10:37,572 - root - INFO - Step: 910 | Loss: 5.44 | Tokens per second: 38292.99 | Training tokens per second (%): 6.23 | MFU (%): 35.86 | TFLOPs: 354.65 | Global batch size: 128 | Global tokens/sec: 612687.83 | Global MFU (%): 35.86 | Global TFLOPs: 5674.36 | 
2025-05-19 11:10:41,864 - root - INFO - Step: 920 | Loss: 5.44 | Tokens per second: 38179.08 | Training tokens per second (%): 6.23 | MFU (%): 35.75 | TFLOPs: 353.59 | Global batch size: 128 | Global tokens/sec: 610865.27 | Global MFU (%): 35.75 | Global TFLOPs: 5657.49 | 
2025-05-19 11:10:46,141 - root - INFO - Step: 930 | Loss: 5.46 | Tokens per second: 38316.22 | Training tokens per second (%): 6.24 | MFU (%): 35.88 | TFLOPs: 354.86 | Global batch size: 128 | Global tokens/sec: 613059.56 | Global MFU (%): 35.88 | Global TFLOPs: 5677.81 | 
2025-05-19 11:10:50,436 - root - INFO - Step: 940 | Loss: 5.45 | Tokens per second: 38150.77 | Training tokens per second (%): 6.23 | MFU (%): 35.73 | TFLOPs: 353.33 | Global batch size: 128 | Global tokens/sec: 610412.28 | Global MFU (%): 35.73 | Global TFLOPs: 5653.29 | 
2025-05-19 11:10:54,724 - root - INFO - Step: 950 | Loss: 5.47 | Tokens per second: 38217.49 | Training tokens per second (%): 6.23 | MFU (%): 35.79 | TFLOPs: 353.95 | Global batch size: 128 | Global tokens/sec: 611479.82 | Global MFU (%): 35.79 | Global TFLOPs: 5663.18 | 
2025-05-19 11:10:59,006 - root - INFO - Step: 960 | Loss: 5.39 | Tokens per second: 38268.40 | Training tokens per second (%): 6.23 | MFU (%): 35.84 | TFLOPs: 354.42 | Global batch size: 128 | Global tokens/sec: 612294.40 | Global MFU (%): 35.84 | Global TFLOPs: 5670.72 | 
2025-05-19 11:11:03,297 - root - INFO - Step: 970 | Loss: 5.37 | Tokens per second: 38193.27 | Training tokens per second (%): 6.23 | MFU (%): 35.77 | TFLOPs: 353.72 | Global batch size: 128 | Global tokens/sec: 611092.29 | Global MFU (%): 35.77 | Global TFLOPs: 5659.59 | 
2025-05-19 11:11:07,581 - root - INFO - Step: 980 | Loss: 5.36 | Tokens per second: 38246.43 | Training tokens per second (%): 6.23 | MFU (%): 35.82 | TFLOPs: 354.22 | Global batch size: 128 | Global tokens/sec: 611942.83 | Global MFU (%): 35.82 | Global TFLOPs: 5667.46 | 
2025-05-19 11:11:11,876 - root - INFO - Step: 990 | Loss: 5.36 | Tokens per second: 38155.26 | Training tokens per second (%): 6.23 | MFU (%): 35.73 | TFLOPs: 353.37 | Global batch size: 128 | Global tokens/sec: 610484.23 | Global MFU (%): 35.73 | Global TFLOPs: 5653.96 | 
2025-05-19 11:11:16,167 - root - INFO - Step: 1000 | Loss: 5.37 | Tokens per second: 38186.58 | Training tokens per second (%): 6.23 | MFU (%): 35.76 | TFLOPs: 353.66 | Global batch size: 128 | Global tokens/sec: 610985.26 | Global MFU (%): 35.76 | Global TFLOPs: 5658.60 | 
2025-05-19 11:11:16,167 - root - INFO - Training completed
[sbatch-master] task finished
