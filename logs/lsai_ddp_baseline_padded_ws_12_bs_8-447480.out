[sbatch-master] running on nid006931
[sbatch-master] SLURM_NODELIST: nid[006931,006995-006996]
[sbatch-master] SLURM_NNODES: 3
[sbatch-master] SLURM_NODEID: 0
[Master] Master node: nid006931
[Master] World size: 12
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
[srun] rank=0 host=nid006931 noderank=0 localrank=0
[srun] rank=2 host=nid006996 noderank=2 localrank=0
[srun] rank=1 host=nid006995 noderank=1 localrank=0
W0518 22:35:39.921000 119519 torch/distributed/run.py:792] 
W0518 22:35:39.921000 119519 torch/distributed/run.py:792] *****************************************
W0518 22:35:39.921000 119519 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0518 22:35:39.921000 119519 torch/distributed/run.py:792] *****************************************
W0518 22:35:39.921000 170007 torch/distributed/run.py:792] 
W0518 22:35:39.921000 170007 torch/distributed/run.py:792] *****************************************
W0518 22:35:39.921000 170007 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0518 22:35:39.921000 170007 torch/distributed/run.py:792] *****************************************
W0518 22:35:40.884000 107447 torch/distributed/run.py:792] 
W0518 22:35:40.884000 107447 torch/distributed/run.py:792] *****************************************
W0518 22:35:40.884000 107447 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0518 22:35:40.884000 107447 torch/distributed/run.py:792] *****************************************
2025-05-18 22:36:00,275 - root - INFO - [Distributed Init] Rank 4 initialized on node 1 on GPU 0.
2025-05-18 22:36:00,408 - root - INFO - [Distributed Init] Rank 8 initialized on node 2 on GPU 0.
2025-05-18 22:36:00,417 - root - INFO - [Distributed Init] Rank 0 initialized on node 0 on GPU 0.
[rank4]:[W518 22:36:00.124492384 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank8]:[W518 22:36:00.997130510 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 8]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 22:36:00,827 - root - INFO - [Distributed Init] Rank 7 initialized on node 1 on GPU 3.
[rank7]:[W518 22:36:00.232565839 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 7]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank0]:[W518 22:36:00.600470197 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 22:36:00,928 - root - INFO - [Distributed Init] Rank 6 initialized on node 1 on GPU 2.
[rank6]:[W518 22:36:00.332905291 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 6]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 22:36:00,938 - root - INFO - [Distributed Init] Rank 5 initialized on node 1 on GPU 1.
[rank5]:[W518 22:36:00.342725510 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 5]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 22:36:00,938 - root - INFO - [Distributed Init] Rank 9 initialized on node 2 on GPU 1.
[rank9]:[W518 22:36:00.113135674 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 9]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 22:36:00,988 - root - INFO - [Distributed Init] Rank 11 initialized on node 2 on GPU 3.
[rank11]:[W518 22:36:00.162511983 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 11]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 22:36:00,991 - root - INFO - [Distributed Init] Rank 2 initialized on node 0 on GPU 2.
[rank2]:[W518 22:36:00.722407250 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 22:36:00,997 - root - INFO - [Distributed Init] Rank 10 initialized on node 2 on GPU 2.
[rank10]:[W518 22:36:00.172220428 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 10]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 22:36:01,011 - root - INFO - [Distributed Init] Rank 1 initialized on node 0 on GPU 1.
[rank1]:[W518 22:36:01.741905691 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 22:36:01,031 - root - INFO - [Distributed Init] Rank 3 initialized on node 0 on GPU 3.
[rank3]:[W518 22:36:01.761870969 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 22:36:06,043 - root - INFO - [Rank 0] All ranks ready!
2025-05-18 22:36:06,043 - root - INFO - Distributed training enabled: 12 processes
2025-05-18 22:36:06,043 - root - INFO - Master process: 0 on cuda:0
2025-05-18 22:36:06,043 - root - INFO - Experiment args: Namespace(dataset='/capstor/scratch/cscs/kasparr/project/train_data.parquet', dataset_type='padded', pretokenized=False, tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=10, profile=False, profile_step_start=10, profile_step_end=10, grad_max_norm=1, model_dtype='bf16', compile=False, distributed=True, backend='nccl', find_unused_parameters=False)
2025-05-18 22:36:06,043 - root - INFO - Setting up Tokenizer...
2025-05-18 22:36:06,836 - root - INFO - Setting up DataLoaders...
2025-05-18 22:36:06,836 - root - INFO - Using padded ParquetDataset with on-the-fly tokenization
2025-05-18 22:36:14,950 - root - INFO - Setting up Model...
2025-05-18 22:36:23,376 - root - INFO - Model wrapped with DistributedDataParallel
2025-05-18 22:36:23,377 - root - INFO - Global batch size: 96 (local: 8 Ã— 12 processes)
2025-05-18 22:36:23,377 - root - INFO - Starting training!
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
2025-05-18 22:36:24,928 - root - INFO - Step: 1 | Loss: 11.95 | Tokens per second: 10571.58 | Training tokens per second (%): 3.60 | MFU (%): 9.90 | TFLOPs: 97.91 | Global batch size: 96 | Global tokens/sec: 126858.96 | Global MFU (%): 9.90 | Global TFLOPs: 1174.90 | 
2025-05-18 22:36:28,917 - root - INFO - Step: 10 | Loss: 11.91 | Tokens per second: 36965.04 | Training tokens per second (%): 2.96 | MFU (%): 34.62 | TFLOPs: 342.35 | Global batch size: 96 | Global tokens/sec: 443580.45 | Global MFU (%): 34.62 | Global TFLOPs: 4108.19 | 
2025-05-18 22:36:33,705 - root - INFO - Step: 20 | Loss: 11.68 | Tokens per second: 34228.65 | Training tokens per second (%): 2.51 | MFU (%): 32.05 | TFLOPs: 317.01 | Global batch size: 96 | Global tokens/sec: 410743.76 | Global MFU (%): 32.05 | Global TFLOPs: 3804.07 | 
2025-05-18 22:36:37,925 - root - INFO - Step: 30 | Loss: 11.00 | Tokens per second: 38831.24 | Training tokens per second (%): 3.23 | MFU (%): 36.36 | TFLOPs: 359.63 | Global batch size: 96 | Global tokens/sec: 465974.89 | Global MFU (%): 36.36 | Global TFLOPs: 4315.59 | 
2025-05-18 22:36:42,271 - root - INFO - Step: 40 | Loss: 10.02 | Tokens per second: 37705.23 | Training tokens per second (%): 3.00 | MFU (%): 35.31 | TFLOPs: 349.20 | Global batch size: 96 | Global tokens/sec: 452462.80 | Global MFU (%): 35.31 | Global TFLOPs: 4190.45 | 
2025-05-18 22:36:46,717 - root - INFO - Step: 50 | Loss: 9.37 | Tokens per second: 36856.29 | Training tokens per second (%): 3.23 | MFU (%): 34.51 | TFLOPs: 341.34 | Global batch size: 96 | Global tokens/sec: 442275.51 | Global MFU (%): 34.51 | Global TFLOPs: 4096.10 | 
2025-05-18 22:36:50,961 - root - INFO - Step: 60 | Loss: 8.80 | Tokens per second: 38607.49 | Training tokens per second (%): 3.30 | MFU (%): 36.15 | TFLOPs: 357.56 | Global batch size: 96 | Global tokens/sec: 463289.85 | Global MFU (%): 36.15 | Global TFLOPs: 4290.73 | 
2025-05-18 22:36:55,366 - root - INFO - Step: 70 | Loss: 8.32 | Tokens per second: 37205.36 | Training tokens per second (%): 3.23 | MFU (%): 34.84 | TFLOPs: 344.57 | Global batch size: 96 | Global tokens/sec: 446464.34 | Global MFU (%): 34.84 | Global TFLOPs: 4134.90 | 
2025-05-18 22:36:59,802 - root - INFO - Step: 80 | Loss: 7.90 | Tokens per second: 36935.86 | Training tokens per second (%): 3.46 | MFU (%): 34.59 | TFLOPs: 342.08 | Global batch size: 96 | Global tokens/sec: 443230.33 | Global MFU (%): 34.59 | Global TFLOPs: 4104.95 | 
2025-05-18 22:37:03,868 - root - INFO - Step: 90 | Loss: 7.60 | Tokens per second: 40302.10 | Training tokens per second (%): 3.08 | MFU (%): 37.74 | TFLOPs: 373.26 | Global batch size: 96 | Global tokens/sec: 483625.26 | Global MFU (%): 37.74 | Global TFLOPs: 4479.06 | 
2025-05-18 22:37:08,135 - root - INFO - Step: 100 | Loss: 7.39 | Tokens per second: 38401.21 | Training tokens per second (%): 3.32 | MFU (%): 35.96 | TFLOPs: 355.65 | Global batch size: 96 | Global tokens/sec: 460814.57 | Global MFU (%): 35.96 | Global TFLOPs: 4267.80 | 
2025-05-18 22:37:12,352 - root - INFO - Step: 110 | Loss: 7.29 | Tokens per second: 38861.67 | Training tokens per second (%): 3.47 | MFU (%): 36.39 | TFLOPs: 359.91 | Global batch size: 96 | Global tokens/sec: 466340.01 | Global MFU (%): 36.39 | Global TFLOPs: 4318.97 | 
2025-05-18 22:37:16,669 - root - INFO - Step: 120 | Loss: 7.23 | Tokens per second: 37954.43 | Training tokens per second (%): 3.13 | MFU (%): 35.54 | TFLOPs: 351.51 | Global batch size: 96 | Global tokens/sec: 455453.10 | Global MFU (%): 35.54 | Global TFLOPs: 4218.15 | 
2025-05-18 22:37:20,955 - root - INFO - Step: 130 | Loss: 7.07 | Tokens per second: 38235.18 | Training tokens per second (%): 3.38 | MFU (%): 35.81 | TFLOPs: 354.11 | Global batch size: 96 | Global tokens/sec: 458822.15 | Global MFU (%): 35.81 | Global TFLOPs: 4249.35 | 
2025-05-18 22:37:25,262 - root - INFO - Step: 140 | Loss: 6.98 | Tokens per second: 38049.58 | Training tokens per second (%): 2.81 | MFU (%): 35.63 | TFLOPs: 352.39 | Global batch size: 96 | Global tokens/sec: 456594.99 | Global MFU (%): 35.63 | Global TFLOPs: 4228.72 | 
2025-05-18 22:37:29,606 - root - INFO - Step: 150 | Loss: 6.97 | Tokens per second: 37722.66 | Training tokens per second (%): 3.40 | MFU (%): 35.33 | TFLOPs: 349.37 | Global batch size: 96 | Global tokens/sec: 452671.87 | Global MFU (%): 35.33 | Global TFLOPs: 4192.39 | 
2025-05-18 22:37:33,848 - root - INFO - Step: 160 | Loss: 6.92 | Tokens per second: 38625.26 | Training tokens per second (%): 3.03 | MFU (%): 36.17 | TFLOPs: 357.73 | Global batch size: 96 | Global tokens/sec: 463503.17 | Global MFU (%): 36.17 | Global TFLOPs: 4292.70 | 
2025-05-18 22:37:38,244 - root - INFO - Step: 170 | Loss: 6.86 | Tokens per second: 37279.35 | Training tokens per second (%): 2.70 | MFU (%): 34.91 | TFLOPs: 345.26 | Global batch size: 96 | Global tokens/sec: 447352.19 | Global MFU (%): 34.91 | Global TFLOPs: 4143.12 | 
2025-05-18 22:37:42,449 - root - INFO - Step: 180 | Loss: 6.81 | Tokens per second: 38969.04 | Training tokens per second (%): 3.17 | MFU (%): 36.49 | TFLOPs: 360.91 | Global batch size: 96 | Global tokens/sec: 467628.52 | Global MFU (%): 36.49 | Global TFLOPs: 4330.91 | 
2025-05-18 22:37:46,749 - root - INFO - Step: 190 | Loss: 6.83 | Tokens per second: 38108.29 | Training tokens per second (%): 3.05 | MFU (%): 35.69 | TFLOPs: 352.94 | Global batch size: 96 | Global tokens/sec: 457299.54 | Global MFU (%): 35.69 | Global TFLOPs: 4235.25 | 
2025-05-18 22:37:51,041 - root - INFO - Step: 200 | Loss: 6.75 | Tokens per second: 38179.80 | Training tokens per second (%): 2.94 | MFU (%): 35.75 | TFLOPs: 353.60 | Global batch size: 96 | Global tokens/sec: 458157.65 | Global MFU (%): 35.75 | Global TFLOPs: 4243.19 | 
2025-05-18 22:37:55,319 - root - INFO - Step: 210 | Loss: 6.72 | Tokens per second: 38300.52 | Training tokens per second (%): 3.18 | MFU (%): 35.87 | TFLOPs: 354.72 | Global batch size: 96 | Global tokens/sec: 459606.19 | Global MFU (%): 35.87 | Global TFLOPs: 4256.61 | 
2025-05-18 22:37:59,752 - root - INFO - Step: 220 | Loss: 6.66 | Tokens per second: 36971.43 | Training tokens per second (%): 2.91 | MFU (%): 34.62 | TFLOPs: 342.41 | Global batch size: 96 | Global tokens/sec: 443657.20 | Global MFU (%): 34.62 | Global TFLOPs: 4108.90 | 
2025-05-18 22:38:04,019 - root - INFO - Step: 230 | Loss: 6.62 | Tokens per second: 38397.33 | Training tokens per second (%): 2.95 | MFU (%): 35.96 | TFLOPs: 355.61 | Global batch size: 96 | Global tokens/sec: 460767.95 | Global MFU (%): 35.96 | Global TFLOPs: 4267.37 | 
2025-05-18 22:38:08,267 - root - INFO - Step: 240 | Loss: 6.54 | Tokens per second: 38572.64 | Training tokens per second (%): 3.00 | MFU (%): 36.12 | TFLOPs: 357.24 | Global batch size: 96 | Global tokens/sec: 462871.62 | Global MFU (%): 36.12 | Global TFLOPs: 4286.85 | 
2025-05-18 22:38:12,711 - root - INFO - Step: 250 | Loss: 6.48 | Tokens per second: 36873.71 | Training tokens per second (%): 2.90 | MFU (%): 34.53 | TFLOPs: 341.50 | Global batch size: 96 | Global tokens/sec: 442484.51 | Global MFU (%): 34.53 | Global TFLOPs: 4098.04 | 
2025-05-18 22:38:16,743 - root - INFO - Step: 260 | Loss: 6.46 | Tokens per second: 40645.15 | Training tokens per second (%): 3.16 | MFU (%): 38.06 | TFLOPs: 376.43 | Global batch size: 96 | Global tokens/sec: 487741.84 | Global MFU (%): 38.06 | Global TFLOPs: 4517.19 | 
2025-05-18 22:38:21,048 - root - INFO - Step: 270 | Loss: 6.46 | Tokens per second: 38067.89 | Training tokens per second (%): 3.01 | MFU (%): 35.65 | TFLOPs: 352.56 | Global batch size: 96 | Global tokens/sec: 456814.65 | Global MFU (%): 35.65 | Global TFLOPs: 4230.76 | 
2025-05-18 22:38:25,494 - root - INFO - Step: 280 | Loss: 6.42 | Tokens per second: 36854.50 | Training tokens per second (%): 2.71 | MFU (%): 34.51 | TFLOPs: 341.33 | Global batch size: 96 | Global tokens/sec: 442253.96 | Global MFU (%): 34.51 | Global TFLOPs: 4095.90 | 
2025-05-18 22:38:29,757 - root - INFO - Step: 290 | Loss: 6.36 | Tokens per second: 38440.63 | Training tokens per second (%): 2.91 | MFU (%): 36.00 | TFLOPs: 356.02 | Global batch size: 96 | Global tokens/sec: 461287.62 | Global MFU (%): 36.00 | Global TFLOPs: 4272.18 | 
2025-05-18 22:38:34,068 - root - INFO - Step: 300 | Loss: 6.36 | Tokens per second: 38005.43 | Training tokens per second (%): 3.43 | MFU (%): 35.59 | TFLOPs: 351.98 | Global batch size: 96 | Global tokens/sec: 456065.10 | Global MFU (%): 35.59 | Global TFLOPs: 4223.81 | 
2025-05-18 22:38:38,361 - root - INFO - Step: 310 | Loss: 6.33 | Tokens per second: 38172.25 | Training tokens per second (%): 3.26 | MFU (%): 35.75 | TFLOPs: 353.53 | Global batch size: 96 | Global tokens/sec: 458067.06 | Global MFU (%): 35.75 | Global TFLOPs: 4242.36 | 
2025-05-18 22:38:42,632 - root - INFO - Step: 320 | Loss: 6.24 | Tokens per second: 38368.86 | Training tokens per second (%): 2.97 | MFU (%): 35.93 | TFLOPs: 355.35 | Global batch size: 96 | Global tokens/sec: 460426.36 | Global MFU (%): 35.93 | Global TFLOPs: 4264.21 | 
2025-05-18 22:38:46,961 - root - INFO - Step: 330 | Loss: 6.28 | Tokens per second: 37856.24 | Training tokens per second (%): 2.71 | MFU (%): 35.45 | TFLOPs: 350.60 | Global batch size: 96 | Global tokens/sec: 454274.83 | Global MFU (%): 35.45 | Global TFLOPs: 4207.23 | 
2025-05-18 22:38:51,199 - root - INFO - Step: 340 | Loss: 6.20 | Tokens per second: 38662.49 | Training tokens per second (%): 3.15 | MFU (%): 36.21 | TFLOPs: 358.07 | Global batch size: 96 | Global tokens/sec: 463949.93 | Global MFU (%): 36.21 | Global TFLOPs: 4296.84 | 
2025-05-18 22:38:55,547 - root - INFO - Step: 350 | Loss: 6.20 | Tokens per second: 37689.21 | Training tokens per second (%): 2.74 | MFU (%): 35.29 | TFLOPs: 349.06 | Global batch size: 96 | Global tokens/sec: 452270.55 | Global MFU (%): 35.29 | Global TFLOPs: 4188.67 | 
2025-05-18 22:38:59,819 - root - INFO - Step: 360 | Loss: 6.17 | Tokens per second: 38353.61 | Training tokens per second (%): 2.86 | MFU (%): 35.92 | TFLOPs: 355.21 | Global batch size: 96 | Global tokens/sec: 460243.26 | Global MFU (%): 35.92 | Global TFLOPs: 4262.51 | 
2025-05-18 22:39:04,055 - root - INFO - Step: 370 | Loss: 6.10 | Tokens per second: 38687.14 | Training tokens per second (%): 3.51 | MFU (%): 36.23 | TFLOPs: 358.30 | Global batch size: 96 | Global tokens/sec: 464245.70 | Global MFU (%): 36.23 | Global TFLOPs: 4299.58 | 
2025-05-18 22:39:08,355 - root - INFO - Step: 380 | Loss: 6.17 | Tokens per second: 38105.87 | Training tokens per second (%): 3.51 | MFU (%): 35.68 | TFLOPs: 352.91 | Global batch size: 96 | Global tokens/sec: 457270.47 | Global MFU (%): 35.68 | Global TFLOPs: 4234.98 | 
2025-05-18 22:39:12,722 - root - INFO - Step: 390 | Loss: 6.02 | Tokens per second: 37529.99 | Training tokens per second (%): 3.21 | MFU (%): 35.14 | TFLOPs: 347.58 | Global batch size: 96 | Global tokens/sec: 450359.88 | Global MFU (%): 35.14 | Global TFLOPs: 4170.98 | 
2025-05-18 22:39:17,011 - root - INFO - Step: 400 | Loss: 6.14 | Tokens per second: 38206.09 | Training tokens per second (%): 3.39 | MFU (%): 35.78 | TFLOPs: 353.84 | Global batch size: 96 | Global tokens/sec: 458473.02 | Global MFU (%): 35.78 | Global TFLOPs: 4246.12 | 
2025-05-18 22:39:21,284 - root - INFO - Step: 410 | Loss: 6.14 | Tokens per second: 38349.33 | Training tokens per second (%): 2.85 | MFU (%): 35.91 | TFLOPs: 355.17 | Global batch size: 96 | Global tokens/sec: 460192.01 | Global MFU (%): 35.91 | Global TFLOPs: 4262.04 | 
2025-05-18 22:39:25,517 - root - INFO - Step: 420 | Loss: 5.98 | Tokens per second: 38712.68 | Training tokens per second (%): 3.41 | MFU (%): 36.25 | TFLOPs: 358.53 | Global batch size: 96 | Global tokens/sec: 464552.17 | Global MFU (%): 36.25 | Global TFLOPs: 4302.42 | 
2025-05-18 22:39:29,900 - root - INFO - Step: 430 | Loss: 6.00 | Tokens per second: 37385.64 | Training tokens per second (%): 2.93 | MFU (%): 35.01 | TFLOPs: 346.24 | Global batch size: 96 | Global tokens/sec: 448627.69 | Global MFU (%): 35.01 | Global TFLOPs: 4154.93 | 
2025-05-18 22:39:34,149 - root - INFO - Step: 440 | Loss: 5.97 | Tokens per second: 38565.54 | Training tokens per second (%): 3.35 | MFU (%): 36.11 | TFLOPs: 357.17 | Global batch size: 96 | Global tokens/sec: 462786.45 | Global MFU (%): 36.11 | Global TFLOPs: 4286.06 | 
2025-05-18 22:39:38,416 - root - INFO - Step: 450 | Loss: 6.00 | Tokens per second: 38397.41 | Training tokens per second (%): 3.16 | MFU (%): 35.96 | TFLOPs: 355.61 | Global batch size: 96 | Global tokens/sec: 460768.89 | Global MFU (%): 35.96 | Global TFLOPs: 4267.38 | 
2025-05-18 22:39:42,711 - root - INFO - Step: 460 | Loss: 5.91 | Tokens per second: 38154.32 | Training tokens per second (%): 3.31 | MFU (%): 35.73 | TFLOPs: 353.36 | Global batch size: 96 | Global tokens/sec: 457851.86 | Global MFU (%): 35.73 | Global TFLOPs: 4240.36 | 
2025-05-18 22:39:46,910 - root - INFO - Step: 470 | Loss: 5.89 | Tokens per second: 39028.75 | Training tokens per second (%): 3.33 | MFU (%): 36.55 | TFLOPs: 361.46 | Global batch size: 96 | Global tokens/sec: 468344.99 | Global MFU (%): 36.55 | Global TFLOPs: 4337.54 | 
2025-05-18 22:39:51,188 - root - INFO - Step: 480 | Loss: 5.96 | Tokens per second: 38300.61 | Training tokens per second (%): 3.50 | MFU (%): 35.87 | TFLOPs: 354.72 | Global batch size: 96 | Global tokens/sec: 459607.26 | Global MFU (%): 35.87 | Global TFLOPs: 4256.62 | 
2025-05-18 22:39:55,538 - root - INFO - Step: 490 | Loss: 5.93 | Tokens per second: 37672.68 | Training tokens per second (%): 2.86 | MFU (%): 35.28 | TFLOPs: 348.90 | Global batch size: 96 | Global tokens/sec: 452072.21 | Global MFU (%): 35.28 | Global TFLOPs: 4186.83 | 
2025-05-18 22:40:00,013 - root - INFO - Step: 500 | Loss: 5.99 | Tokens per second: 36616.58 | Training tokens per second (%): 2.90 | MFU (%): 34.29 | TFLOPs: 339.12 | Global batch size: 96 | Global tokens/sec: 439398.98 | Global MFU (%): 34.29 | Global TFLOPs: 4069.46 | 
2025-05-18 22:40:04,098 - root - INFO - Step: 510 | Loss: 5.81 | Tokens per second: 40111.84 | Training tokens per second (%): 2.95 | MFU (%): 37.56 | TFLOPs: 371.49 | Global batch size: 96 | Global tokens/sec: 481342.09 | Global MFU (%): 37.56 | Global TFLOPs: 4457.92 | 
2025-05-18 22:40:08,382 - root - INFO - Step: 520 | Loss: 5.81 | Tokens per second: 38256.64 | Training tokens per second (%): 2.88 | MFU (%): 35.83 | TFLOPs: 354.31 | Global batch size: 96 | Global tokens/sec: 459079.72 | Global MFU (%): 35.83 | Global TFLOPs: 4251.73 | 
2025-05-18 22:40:12,818 - root - INFO - Step: 530 | Loss: 5.73 | Tokens per second: 36936.09 | Training tokens per second (%): 3.48 | MFU (%): 34.59 | TFLOPs: 342.08 | Global batch size: 96 | Global tokens/sec: 443233.05 | Global MFU (%): 34.59 | Global TFLOPs: 4104.97 | 
2025-05-18 22:40:17,126 - root - INFO - Step: 540 | Loss: 5.81 | Tokens per second: 38043.96 | Training tokens per second (%): 3.49 | MFU (%): 35.63 | TFLOPs: 352.34 | Global batch size: 96 | Global tokens/sec: 456527.51 | Global MFU (%): 35.63 | Global TFLOPs: 4228.10 | 
2025-05-18 22:40:21,401 - root - INFO - Step: 550 | Loss: 5.73 | Tokens per second: 38329.20 | Training tokens per second (%): 3.07 | MFU (%): 35.89 | TFLOPs: 354.98 | Global batch size: 96 | Global tokens/sec: 459950.34 | Global MFU (%): 35.89 | Global TFLOPs: 4259.80 | 
2025-05-18 22:40:25,646 - root - INFO - Step: 560 | Loss: 5.71 | Tokens per second: 38600.81 | Training tokens per second (%): 3.55 | MFU (%): 36.15 | TFLOPs: 357.50 | Global batch size: 96 | Global tokens/sec: 463209.68 | Global MFU (%): 36.15 | Global TFLOPs: 4289.98 | 
2025-05-18 22:40:29,939 - root - INFO - Step: 570 | Loss: 5.75 | Tokens per second: 38168.66 | Training tokens per second (%): 3.34 | MFU (%): 35.74 | TFLOPs: 353.50 | Global batch size: 96 | Global tokens/sec: 458023.92 | Global MFU (%): 35.74 | Global TFLOPs: 4241.96 | 
2025-05-18 22:40:34,495 - root - INFO - Step: 580 | Loss: 5.66 | Tokens per second: 35969.92 | Training tokens per second (%): 3.39 | MFU (%): 33.68 | TFLOPs: 333.13 | Global batch size: 96 | Global tokens/sec: 431639.02 | Global MFU (%): 33.68 | Global TFLOPs: 3997.59 | 
2025-05-18 22:40:38,545 - root - INFO - Step: 590 | Loss: 5.70 | Tokens per second: 40455.02 | Training tokens per second (%): 3.27 | MFU (%): 37.88 | TFLOPs: 374.67 | Global batch size: 96 | Global tokens/sec: 485460.21 | Global MFU (%): 37.88 | Global TFLOPs: 4496.06 | 
2025-05-18 22:40:42,810 - root - INFO - Step: 600 | Loss: 5.69 | Tokens per second: 38421.76 | Training tokens per second (%): 2.96 | MFU (%): 35.98 | TFLOPs: 355.84 | Global batch size: 96 | Global tokens/sec: 461061.09 | Global MFU (%): 35.98 | Global TFLOPs: 4270.08 | 
2025-05-18 22:40:47,295 - root - INFO - Step: 610 | Loss: 5.74 | Tokens per second: 36538.67 | Training tokens per second (%): 2.97 | MFU (%): 34.22 | TFLOPs: 338.40 | Global batch size: 96 | Global tokens/sec: 438464.04 | Global MFU (%): 34.22 | Global TFLOPs: 4060.80 | 
2025-05-18 22:40:51,656 - root - INFO - Step: 620 | Loss: 5.59 | Tokens per second: 37577.35 | Training tokens per second (%): 3.02 | MFU (%): 35.19 | TFLOPs: 348.02 | Global batch size: 96 | Global tokens/sec: 450928.16 | Global MFU (%): 35.19 | Global TFLOPs: 4176.24 | 
2025-05-18 22:40:55,920 - root - INFO - Step: 630 | Loss: 5.65 | Tokens per second: 38433.41 | Training tokens per second (%): 2.98 | MFU (%): 35.99 | TFLOPs: 355.95 | Global batch size: 96 | Global tokens/sec: 461200.87 | Global MFU (%): 35.99 | Global TFLOPs: 4271.38 | 
2025-05-18 22:41:00,305 - root - INFO - Step: 640 | Loss: 5.63 | Tokens per second: 37363.92 | Training tokens per second (%): 3.12 | MFU (%): 34.99 | TFLOPs: 346.04 | Global batch size: 96 | Global tokens/sec: 448366.99 | Global MFU (%): 34.99 | Global TFLOPs: 4152.52 | 
2025-05-18 22:41:04,568 - root - INFO - Step: 650 | Loss: 5.65 | Tokens per second: 38441.30 | Training tokens per second (%): 2.89 | MFU (%): 36.00 | TFLOPs: 356.02 | Global batch size: 96 | Global tokens/sec: 461295.57 | Global MFU (%): 36.00 | Global TFLOPs: 4272.26 | 
2025-05-18 22:41:08,844 - root - INFO - Step: 660 | Loss: 5.65 | Tokens per second: 38322.96 | Training tokens per second (%): 3.03 | MFU (%): 35.89 | TFLOPs: 354.93 | Global batch size: 96 | Global tokens/sec: 459875.57 | Global MFU (%): 35.89 | Global TFLOPs: 4259.10 | 
2025-05-18 22:41:13,148 - root - INFO - Step: 670 | Loss: 5.58 | Tokens per second: 38073.09 | Training tokens per second (%): 3.41 | MFU (%): 35.65 | TFLOPs: 352.61 | Global batch size: 96 | Global tokens/sec: 456877.10 | Global MFU (%): 35.65 | Global TFLOPs: 4231.33 | 
2025-05-18 22:41:17,163 - root - INFO - Step: 680 | Loss: 5.54 | Tokens per second: 40816.42 | Training tokens per second (%): 2.95 | MFU (%): 38.22 | TFLOPs: 378.02 | Global batch size: 96 | Global tokens/sec: 489797.02 | Global MFU (%): 38.22 | Global TFLOPs: 4536.22 | 
2025-05-18 22:41:21,501 - root - INFO - Step: 690 | Loss: 5.55 | Tokens per second: 37768.90 | Training tokens per second (%): 3.24 | MFU (%): 35.37 | TFLOPs: 349.79 | Global batch size: 96 | Global tokens/sec: 453226.74 | Global MFU (%): 35.37 | Global TFLOPs: 4197.53 | 
2025-05-18 22:41:26,018 - root - INFO - Step: 700 | Loss: 5.48 | Tokens per second: 36280.38 | Training tokens per second (%): 3.37 | MFU (%): 33.97 | TFLOPs: 336.01 | Global batch size: 96 | Global tokens/sec: 435364.61 | Global MFU (%): 33.97 | Global TFLOPs: 4032.10 | 
2025-05-18 22:41:30,374 - root - INFO - Step: 710 | Loss: 5.46 | Tokens per second: 37620.68 | Training tokens per second (%): 2.99 | MFU (%): 35.23 | TFLOPs: 348.42 | Global batch size: 96 | Global tokens/sec: 451448.12 | Global MFU (%): 35.23 | Global TFLOPs: 4181.05 | 
2025-05-18 22:41:34,752 - root - INFO - Step: 720 | Loss: 5.46 | Tokens per second: 37427.57 | Training tokens per second (%): 3.19 | MFU (%): 35.05 | TFLOPs: 346.63 | Global batch size: 96 | Global tokens/sec: 449130.83 | Global MFU (%): 35.05 | Global TFLOPs: 4159.59 | 
2025-05-18 22:41:39,095 - root - INFO - Step: 730 | Loss: 5.45 | Tokens per second: 37729.85 | Training tokens per second (%): 3.33 | MFU (%): 35.33 | TFLOPs: 349.43 | Global batch size: 96 | Global tokens/sec: 452758.19 | Global MFU (%): 35.33 | Global TFLOPs: 4193.19 | 
2025-05-18 22:41:43,441 - root - INFO - Step: 740 | Loss: 5.61 | Tokens per second: 37703.04 | Training tokens per second (%): 2.90 | MFU (%): 35.31 | TFLOPs: 349.18 | Global batch size: 96 | Global tokens/sec: 452436.50 | Global MFU (%): 35.31 | Global TFLOPs: 4190.21 | 
2025-05-18 22:41:47,662 - root - INFO - Step: 750 | Loss: 5.51 | Tokens per second: 38827.62 | Training tokens per second (%): 3.23 | MFU (%): 36.36 | TFLOPs: 359.60 | Global batch size: 96 | Global tokens/sec: 465931.47 | Global MFU (%): 36.36 | Global TFLOPs: 4315.19 | 
2025-05-18 22:41:51,684 - root - INFO - Step: 760 | Loss: 5.45 | Tokens per second: 40742.98 | Training tokens per second (%): 2.96 | MFU (%): 38.15 | TFLOPs: 377.34 | Global batch size: 96 | Global tokens/sec: 488915.72 | Global MFU (%): 38.15 | Global TFLOPs: 4528.06 | 
2025-05-18 22:41:56,066 - root - INFO - Step: 770 | Loss: 5.36 | Tokens per second: 37388.44 | Training tokens per second (%): 2.92 | MFU (%): 35.01 | TFLOPs: 346.27 | Global batch size: 96 | Global tokens/sec: 448661.32 | Global MFU (%): 35.01 | Global TFLOPs: 4155.24 | 
2025-05-18 22:42:00,560 - root - INFO - Step: 780 | Loss: 5.48 | Tokens per second: 36465.95 | Training tokens per second (%): 3.39 | MFU (%): 34.15 | TFLOPs: 337.73 | Global batch size: 96 | Global tokens/sec: 437591.38 | Global MFU (%): 34.15 | Global TFLOPs: 4052.72 | 
2025-05-18 22:42:04,855 - root - INFO - Step: 790 | Loss: 5.47 | Tokens per second: 38154.91 | Training tokens per second (%): 2.81 | MFU (%): 35.73 | TFLOPs: 353.37 | Global batch size: 96 | Global tokens/sec: 457858.94 | Global MFU (%): 35.73 | Global TFLOPs: 4240.43 | 
2025-05-18 22:42:09,070 - root - INFO - Step: 800 | Loss: 5.35 | Tokens per second: 38872.44 | Training tokens per second (%): 3.46 | MFU (%): 36.40 | TFLOPs: 360.01 | Global batch size: 96 | Global tokens/sec: 466469.22 | Global MFU (%): 36.40 | Global TFLOPs: 4320.17 | 
2025-05-18 22:42:13,366 - root - INFO - Step: 810 | Loss: 5.49 | Tokens per second: 38143.07 | Training tokens per second (%): 3.09 | MFU (%): 35.72 | TFLOPs: 353.26 | Global batch size: 96 | Global tokens/sec: 457716.88 | Global MFU (%): 35.72 | Global TFLOPs: 4239.11 | 
2025-05-18 22:42:17,679 - root - INFO - Step: 820 | Loss: 5.51 | Tokens per second: 38000.42 | Training tokens per second (%): 3.09 | MFU (%): 35.59 | TFLOPs: 351.94 | Global batch size: 96 | Global tokens/sec: 456005.08 | Global MFU (%): 35.59 | Global TFLOPs: 4223.26 | 
2025-05-18 22:42:22,193 - root - INFO - Step: 830 | Loss: 5.41 | Tokens per second: 36296.19 | Training tokens per second (%): 3.19 | MFU (%): 33.99 | TFLOPs: 336.15 | Global batch size: 96 | Global tokens/sec: 435554.26 | Global MFU (%): 33.99 | Global TFLOPs: 4033.85 | 
2025-05-18 22:42:26,199 - root - INFO - Step: 840 | Loss: 5.46 | Tokens per second: 40909.55 | Training tokens per second (%): 3.19 | MFU (%): 38.31 | TFLOPs: 378.88 | Global batch size: 96 | Global tokens/sec: 490914.63 | Global MFU (%): 38.31 | Global TFLOPs: 4546.57 | 
2025-05-18 22:42:30,561 - root - INFO - Step: 850 | Loss: 5.59 | Tokens per second: 37565.97 | Training tokens per second (%): 3.48 | MFU (%): 35.18 | TFLOPs: 347.91 | Global batch size: 96 | Global tokens/sec: 450791.66 | Global MFU (%): 35.18 | Global TFLOPs: 4174.97 | 
2025-05-18 22:42:34,851 - root - INFO - Step: 860 | Loss: 5.41 | Tokens per second: 38196.61 | Training tokens per second (%): 3.51 | MFU (%): 35.77 | TFLOPs: 353.76 | Global batch size: 96 | Global tokens/sec: 458359.33 | Global MFU (%): 35.77 | Global TFLOPs: 4245.06 | 
2025-05-18 22:42:39,085 - root - INFO - Step: 870 | Loss: 5.40 | Tokens per second: 38703.63 | Training tokens per second (%): 3.34 | MFU (%): 36.24 | TFLOPs: 358.45 | Global batch size: 96 | Global tokens/sec: 464443.51 | Global MFU (%): 36.24 | Global TFLOPs: 4301.41 | 
2025-05-18 22:42:43,419 - root - INFO - Step: 880 | Loss: 5.34 | Tokens per second: 37811.32 | Training tokens per second (%): 3.67 | MFU (%): 35.41 | TFLOPs: 350.19 | Global batch size: 96 | Global tokens/sec: 453735.86 | Global MFU (%): 35.41 | Global TFLOPs: 4202.24 | 
2025-05-18 22:42:47,650 - root - INFO - Step: 890 | Loss: 5.41 | Tokens per second: 38732.00 | Training tokens per second (%): 3.72 | MFU (%): 36.27 | TFLOPs: 358.71 | Global batch size: 96 | Global tokens/sec: 464783.97 | Global MFU (%): 36.27 | Global TFLOPs: 4304.56 | 
2025-05-18 22:42:51,960 - root - INFO - Step: 900 | Loss: 5.32 | Tokens per second: 38013.56 | Training tokens per second (%): 2.90 | MFU (%): 35.60 | TFLOPs: 352.06 | Global batch size: 96 | Global tokens/sec: 456162.74 | Global MFU (%): 35.60 | Global TFLOPs: 4224.72 | 
2025-05-18 22:42:56,516 - root - INFO - Step: 910 | Loss: 5.32 | Tokens per second: 35968.43 | Training tokens per second (%): 3.23 | MFU (%): 33.68 | TFLOPs: 333.12 | Global batch size: 96 | Global tokens/sec: 431621.14 | Global MFU (%): 33.68 | Global TFLOPs: 3997.43 | 
2025-05-18 22:43:00,919 - root - INFO - Step: 920 | Loss: 5.41 | Tokens per second: 37220.24 | Training tokens per second (%): 2.96 | MFU (%): 34.85 | TFLOPs: 344.71 | Global batch size: 96 | Global tokens/sec: 446642.90 | Global MFU (%): 34.85 | Global TFLOPs: 4136.55 | 
2025-05-18 22:43:04,991 - root - INFO - Step: 930 | Loss: 5.26 | Tokens per second: 40239.83 | Training tokens per second (%): 3.16 | MFU (%): 37.68 | TFLOPs: 372.68 | Global batch size: 96 | Global tokens/sec: 482877.99 | Global MFU (%): 37.68 | Global TFLOPs: 4472.14 | 
2025-05-18 22:43:09,360 - root - INFO - Step: 940 | Loss: 5.26 | Tokens per second: 37505.15 | Training tokens per second (%): 2.76 | MFU (%): 35.12 | TFLOPs: 347.35 | Global batch size: 96 | Global tokens/sec: 450061.84 | Global MFU (%): 35.12 | Global TFLOPs: 4168.22 | 
2025-05-18 22:43:13,684 - root - INFO - Step: 950 | Loss: 5.30 | Tokens per second: 37896.53 | Training tokens per second (%): 2.66 | MFU (%): 35.49 | TFLOPs: 350.98 | Global batch size: 96 | Global tokens/sec: 454758.33 | Global MFU (%): 35.49 | Global TFLOPs: 4211.71 | 
2025-05-18 22:43:18,002 - root - INFO - Step: 960 | Loss: 5.28 | Tokens per second: 37949.96 | Training tokens per second (%): 3.13 | MFU (%): 35.54 | TFLOPs: 351.47 | Global batch size: 96 | Global tokens/sec: 455399.57 | Global MFU (%): 35.54 | Global TFLOPs: 4217.65 | 
2025-05-18 22:43:22,312 - root - INFO - Step: 970 | Loss: 5.30 | Tokens per second: 38020.91 | Training tokens per second (%): 3.24 | MFU (%): 35.60 | TFLOPs: 352.13 | Global batch size: 96 | Global tokens/sec: 456250.96 | Global MFU (%): 35.60 | Global TFLOPs: 4225.54 | 
2025-05-18 22:43:26,547 - root - INFO - Step: 980 | Loss: 5.26 | Tokens per second: 38691.68 | Training tokens per second (%): 3.34 | MFU (%): 36.23 | TFLOPs: 358.34 | Global batch size: 96 | Global tokens/sec: 464300.18 | Global MFU (%): 36.23 | Global TFLOPs: 4300.08 | 
2025-05-18 22:43:30,935 - root - INFO - Step: 990 | Loss: 5.25 | Tokens per second: 37346.12 | Training tokens per second (%): 3.42 | MFU (%): 34.97 | TFLOPs: 345.88 | Global batch size: 96 | Global tokens/sec: 448153.40 | Global MFU (%): 34.97 | Global TFLOPs: 4150.54 | 
2025-05-18 22:43:35,285 - root - INFO - Step: 1000 | Loss: 5.27 | Tokens per second: 37673.47 | Training tokens per second (%): 3.36 | MFU (%): 35.28 | TFLOPs: 348.91 | Global batch size: 96 | Global tokens/sec: 452081.61 | Global MFU (%): 35.28 | Global TFLOPs: 4186.92 | 
2025-05-18 22:43:35,285 - root - INFO - Training completed
[sbatch-master] task finished
