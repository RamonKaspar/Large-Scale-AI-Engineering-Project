[sbatch-master] running on nid006751
[sbatch-master] SLURM_NODELIST: nid[006751,006755,006766,006768]
[sbatch-master] SLURM_NNODES: 4
[sbatch-master] SLURM_NODEID: 0
[Master] Master node: nid006751
[Master] World size: 16
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
[srun] rank=0 host=nid006751 noderank=0 localrank=0
[srun] rank=1 host=nid006755 noderank=1 localrank=0
[srun] rank=2 host=nid006766 noderank=2 localrank=0
[srun] rank=3 host=nid006768 noderank=3 localrank=0
W0519 11:41:49.347000 93185 torch/distributed/run.py:792] 
W0519 11:41:49.347000 93185 torch/distributed/run.py:792] *****************************************
W0519 11:41:49.347000 93185 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0519 11:41:49.347000 93185 torch/distributed/run.py:792] *****************************************
W0519 11:41:49.866000 17991 torch/distributed/run.py:792] 
W0519 11:41:49.866000 17991 torch/distributed/run.py:792] *****************************************
W0519 11:41:49.866000 17991 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0519 11:41:49.866000 17991 torch/distributed/run.py:792] *****************************************
W0519 11:41:49.942000 56108 torch/distributed/run.py:792] 
W0519 11:41:49.942000 56108 torch/distributed/run.py:792] *****************************************
W0519 11:41:49.942000 56108 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0519 11:41:49.942000 56108 torch/distributed/run.py:792] *****************************************
W0519 11:41:50.271000 186908 torch/distributed/run.py:792] 
W0519 11:41:50.271000 186908 torch/distributed/run.py:792] *****************************************
W0519 11:41:50.271000 186908 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0519 11:41:50.271000 186908 torch/distributed/run.py:792] *****************************************
2025-05-19 11:42:02,169 - root - INFO - [Distributed Init] Rank 8 initialized on node 2 on GPU 0.
2025-05-19 11:42:02,418 - root - INFO - [Distributed Init] Rank 0 initialized on node 0 on GPU 0.
2025-05-19 11:42:02,563 - root - INFO - [Distributed Init] Rank 4 initialized on node 1 on GPU 0.
[rank8]:[W519 11:42:02.256856450 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 8]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:42:02,800 - root - INFO - [Distributed Init] Rank 10 initialized on node 2 on GPU 2.
2025-05-19 11:42:02,800 - root - INFO - [Distributed Init] Rank 11 initialized on node 2 on GPU 3.
[rank11]:[W519 11:42:02.423138817 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 11]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank10]:[W519 11:42:02.423139137 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 10]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:42:02,809 - root - INFO - [Distributed Init] Rank 9 initialized on node 2 on GPU 1.
[rank9]:[W519 11:42:02.432026762 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 9]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank0]:[W519 11:42:02.669727795 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W519 11:42:02.447512101 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:42:03,011 - root - INFO - [Distributed Init] Rank 1 initialized on node 0 on GPU 1.
2025-05-19 11:42:03,011 - root - INFO - [Distributed Init] Rank 2 initialized on node 0 on GPU 2.
[rank1]:[W519 11:42:03.797192991 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W519 11:42:03.797193823 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:42:03,061 - root - INFO - [Distributed Init] Rank 3 initialized on node 0 on GPU 3.
[rank3]:[W519 11:42:03.842410070 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:42:03,067 - root - INFO - [Distributed Init] Rank 7 initialized on node 1 on GPU 3.
[rank7]:[W519 11:42:03.512394765 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 7]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:42:03,127 - root - INFO - [Distributed Init] Rank 6 initialized on node 1 on GPU 2.
[rank6]:[W519 11:42:03.572968380 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 6]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:42:03,185 - root - INFO - [Distributed Init] Rank 12 initialized on node 3 on GPU 0.
2025-05-19 11:42:03,187 - root - INFO - [Distributed Init] Rank 5 initialized on node 1 on GPU 1.
[rank5]:[W519 11:42:03.633422735 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 5]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank12]:[W519 11:42:03.762757569 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 12]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:42:03,747 - root - INFO - [Distributed Init] Rank 15 initialized on node 3 on GPU 3.
2025-05-19 11:42:03,747 - root - INFO - [Distributed Init] Rank 14 initialized on node 3 on GPU 2.
2025-05-19 11:42:03,747 - root - INFO - [Distributed Init] Rank 13 initialized on node 3 on GPU 1.
[rank14]:[W519 11:42:03.865309854 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 14]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank13]:[W519 11:42:03.865310654 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 13]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank15]:[W519 11:42:03.866010120 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 15]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 11:42:08,631 - root - INFO - [Rank 0] All ranks ready!
2025-05-19 11:42:08,631 - root - INFO - Distributed training enabled: 16 processes
2025-05-19 11:42:08,631 - root - INFO - Master process: 0 on cuda:0
2025-05-19 11:42:08,631 - root - INFO - Experiment args: Namespace(dataset='/capstor/scratch/cscs/kasparr/project/train_data.parquet', dataset_type='padding-free', pretokenized=False, tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=10, profile=False, profile_step_start=10, profile_step_end=10, grad_max_norm=1, model_dtype='bf16', compile=False, distributed=True, backend='nccl', find_unused_parameters=False)
2025-05-19 11:42:08,631 - root - INFO - Setting up Tokenizer...
2025-05-19 11:42:09,175 - root - INFO - Setting up DataLoaders...
2025-05-19 11:42:09,175 - root - INFO - Using padding-free IterableParquetDataset with on-the-fly tokenization
DDP sharding: rank 0/16DDP sharding: rank 2/16DDP sharding: rank 1/16


Rank 2/16 processing documents 98238 to 147356Rank 1/16 processing documents 49119 to 98237Rank 0/16 processing documents 0 to 49118


2025-05-19 11:42:15,811 - root - INFO - Setting up Model...
DDP sharding: rank 3/16
Rank 3/16 processing documents 147357 to 196475
DDP sharding: rank 8/16DDP sharding: rank 11/16

Rank 11/16 processing documents 540309 to 589427Rank 8/16 processing documents 392952 to 442070

DDP sharding: rank 10/16DDP sharding: rank 9/16

Rank 10/16 processing documents 491190 to 540308Rank 9/16 processing documents 442071 to 491189

DDP sharding: rank 13/16DDP sharding: rank 15/16DDP sharding: rank 12/16


Rank 15/16 processing documents 736785 to 785905Rank 13/16 processing documents 638547 to 687665Rank 12/16 processing documents 589428 to 638546


DDP sharding: rank 14/16
Rank 14/16 processing documents 687666 to 736784
DDP sharding: rank 7/16DDP sharding: rank 4/16DDP sharding: rank 6/16


Rank 6/16 processing documents 294714 to 343832Rank 7/16 processing documents 343833 to 392951Rank 4/16 processing documents 196476 to 245594


DDP sharding: rank 5/16
Rank 5/16 processing documents 245595 to 294713
2025-05-19 11:42:24,956 - root - INFO - Model wrapped with DistributedDataParallel
2025-05-19 11:42:24,957 - root - INFO - Global batch size: 128 (local: 8 × 16 processes)
2025-05-19 11:42:24,957 - root - INFO - Starting training!
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
2025-05-19 11:42:26,597 - root - INFO - Step: 1 | Loss: 11.95 | Tokens per second: 9995.14 | Training tokens per second (%): 6.24 | MFU (%): 9.36 | TFLOPs: 92.57 | Global batch size: 128 | Global tokens/sec: 159922.25 | Global MFU (%): 9.36 | Global TFLOPs: 1481.11 | 
2025-05-19 11:42:31,016 - root - INFO - Step: 10 | Loss: 11.92 | Tokens per second: 33375.51 | Training tokens per second (%): 6.24 | MFU (%): 31.25 | TFLOPs: 309.10 | Global batch size: 128 | Global tokens/sec: 534008.18 | Global MFU (%): 31.25 | Global TFLOPs: 4945.68 | 
2025-05-19 11:42:36,402 - root - INFO - Step: 20 | Loss: 11.64 | Tokens per second: 30426.01 | Training tokens per second (%): 6.24 | MFU (%): 28.49 | TFLOPs: 281.79 | Global batch size: 128 | Global tokens/sec: 486816.20 | Global MFU (%): 28.49 | Global TFLOPs: 4508.61 | 
2025-05-19 11:42:41,045 - root - INFO - Step: 30 | Loss: 11.04 | Tokens per second: 35293.41 | Training tokens per second (%): 6.24 | MFU (%): 33.05 | TFLOPs: 326.87 | Global batch size: 128 | Global tokens/sec: 564694.62 | Global MFU (%): 33.05 | Global TFLOPs: 5229.88 | 
2025-05-19 11:42:45,696 - root - INFO - Step: 40 | Loss: 9.96 | Tokens per second: 35228.77 | Training tokens per second (%): 6.24 | MFU (%): 32.99 | TFLOPs: 326.27 | Global batch size: 128 | Global tokens/sec: 563660.34 | Global MFU (%): 32.99 | Global TFLOPs: 5220.30 | 
2025-05-19 11:42:50,240 - root - INFO - Step: 50 | Loss: 9.31 | Tokens per second: 36058.96 | Training tokens per second (%): 6.24 | MFU (%): 33.77 | TFLOPs: 333.96 | Global batch size: 128 | Global tokens/sec: 576943.40 | Global MFU (%): 33.77 | Global TFLOPs: 5343.32 | 
2025-05-19 11:42:55,027 - root - INFO - Step: 60 | Loss: 8.75 | Tokens per second: 34232.73 | Training tokens per second (%): 6.24 | MFU (%): 32.06 | TFLOPs: 317.04 | Global batch size: 128 | Global tokens/sec: 547723.65 | Global MFU (%): 32.06 | Global TFLOPs: 5072.70 | 
2025-05-19 11:42:59,634 - root - INFO - Step: 70 | Loss: 8.17 | Tokens per second: 35572.93 | Training tokens per second (%): 6.24 | MFU (%): 33.31 | TFLOPs: 329.46 | Global batch size: 128 | Global tokens/sec: 569166.93 | Global MFU (%): 33.31 | Global TFLOPs: 5271.30 | 
2025-05-19 11:43:04,376 - root - INFO - Step: 80 | Loss: 7.76 | Tokens per second: 34551.46 | Training tokens per second (%): 6.24 | MFU (%): 32.36 | TFLOPs: 320.00 | Global batch size: 128 | Global tokens/sec: 552823.31 | Global MFU (%): 32.36 | Global TFLOPs: 5119.93 | 
2025-05-19 11:43:09,210 - root - INFO - Step: 90 | Loss: 7.47 | Tokens per second: 33900.75 | Training tokens per second (%): 6.24 | MFU (%): 31.75 | TFLOPs: 313.97 | Global batch size: 128 | Global tokens/sec: 542411.93 | Global MFU (%): 31.75 | Global TFLOPs: 5023.51 | 
2025-05-19 11:43:13,971 - root - INFO - Step: 100 | Loss: 7.29 | Tokens per second: 34418.60 | Training tokens per second (%): 6.24 | MFU (%): 32.23 | TFLOPs: 318.77 | Global batch size: 128 | Global tokens/sec: 550697.65 | Global MFU (%): 32.23 | Global TFLOPs: 5100.25 | 
2025-05-19 11:43:18,678 - root - INFO - Step: 110 | Loss: 7.23 | Tokens per second: 34811.25 | Training tokens per second (%): 6.24 | MFU (%): 32.60 | TFLOPs: 322.40 | Global batch size: 128 | Global tokens/sec: 556980.07 | Global MFU (%): 32.60 | Global TFLOPs: 5158.43 | 
2025-05-19 11:43:23,303 - root - INFO - Step: 120 | Loss: 7.22 | Tokens per second: 35429.62 | Training tokens per second (%): 6.24 | MFU (%): 33.18 | TFLOPs: 328.13 | Global batch size: 128 | Global tokens/sec: 566873.96 | Global MFU (%): 33.18 | Global TFLOPs: 5250.06 | 
2025-05-19 11:43:27,955 - root - INFO - Step: 130 | Loss: 7.08 | Tokens per second: 35220.33 | Training tokens per second (%): 6.24 | MFU (%): 32.98 | TFLOPs: 326.19 | Global batch size: 128 | Global tokens/sec: 563525.29 | Global MFU (%): 32.98 | Global TFLOPs: 5219.05 | 
2025-05-19 11:43:32,738 - root - INFO - Step: 140 | Loss: 6.96 | Tokens per second: 34261.71 | Training tokens per second (%): 6.25 | MFU (%): 32.08 | TFLOPs: 317.31 | Global batch size: 128 | Global tokens/sec: 548187.34 | Global MFU (%): 32.08 | Global TFLOPs: 5077.00 | 
2025-05-19 11:43:37,302 - root - INFO - Step: 150 | Loss: 6.88 | Tokens per second: 35905.74 | Training tokens per second (%): 6.24 | MFU (%): 33.62 | TFLOPs: 332.54 | Global batch size: 128 | Global tokens/sec: 574491.90 | Global MFU (%): 33.62 | Global TFLOPs: 5320.62 | 
2025-05-19 11:43:41,842 - root - INFO - Step: 160 | Loss: 6.85 | Tokens per second: 36091.78 | Training tokens per second (%): 6.24 | MFU (%): 33.80 | TFLOPs: 334.26 | Global batch size: 128 | Global tokens/sec: 577468.55 | Global MFU (%): 33.80 | Global TFLOPs: 5348.18 | 
2025-05-19 11:43:46,433 - root - INFO - Step: 170 | Loss: 6.83 | Tokens per second: 35690.15 | Training tokens per second (%): 6.23 | MFU (%): 33.42 | TFLOPs: 330.54 | Global batch size: 128 | Global tokens/sec: 571042.37 | Global MFU (%): 33.42 | Global TFLOPs: 5288.67 | 
2025-05-19 11:43:51,043 - root - INFO - Step: 180 | Loss: 6.66 | Tokens per second: 35552.35 | Training tokens per second (%): 6.24 | MFU (%): 33.29 | TFLOPs: 329.27 | Global batch size: 128 | Global tokens/sec: 568837.59 | Global MFU (%): 33.29 | Global TFLOPs: 5268.25 | 
2025-05-19 11:43:55,784 - root - INFO - Step: 190 | Loss: 6.62 | Tokens per second: 34557.64 | Training tokens per second (%): 6.24 | MFU (%): 32.36 | TFLOPs: 320.05 | Global batch size: 128 | Global tokens/sec: 552922.28 | Global MFU (%): 32.36 | Global TFLOPs: 5120.85 | 
2025-05-19 11:44:00,413 - root - INFO - Step: 200 | Loss: 6.65 | Tokens per second: 35403.19 | Training tokens per second (%): 6.24 | MFU (%): 33.15 | TFLOPs: 327.88 | Global batch size: 128 | Global tokens/sec: 566451.05 | Global MFU (%): 33.15 | Global TFLOPs: 5246.15 | 
2025-05-19 11:44:05,023 - root - INFO - Step: 210 | Loss: 6.57 | Tokens per second: 35546.73 | Training tokens per second (%): 6.24 | MFU (%): 33.29 | TFLOPs: 329.21 | Global batch size: 128 | Global tokens/sec: 568747.68 | Global MFU (%): 33.29 | Global TFLOPs: 5267.42 | 
2025-05-19 11:44:09,606 - root - INFO - Step: 220 | Loss: 6.61 | Tokens per second: 35752.95 | Training tokens per second (%): 6.24 | MFU (%): 33.48 | TFLOPs: 331.12 | Global batch size: 128 | Global tokens/sec: 572047.22 | Global MFU (%): 33.48 | Global TFLOPs: 5297.97 | 
2025-05-19 11:44:14,301 - root - INFO - Step: 230 | Loss: 6.43 | Tokens per second: 34897.07 | Training tokens per second (%): 6.24 | MFU (%): 32.68 | TFLOPs: 323.20 | Global batch size: 128 | Global tokens/sec: 558353.06 | Global MFU (%): 32.68 | Global TFLOPs: 5171.15 | 
2025-05-19 11:44:18,942 - root - INFO - Step: 240 | Loss: 6.40 | Tokens per second: 35307.94 | Training tokens per second (%): 6.24 | MFU (%): 33.06 | TFLOPs: 327.00 | Global batch size: 128 | Global tokens/sec: 564927.11 | Global MFU (%): 33.06 | Global TFLOPs: 5232.03 | 
2025-05-19 11:44:23,582 - root - INFO - Step: 250 | Loss: 6.42 | Tokens per second: 35320.62 | Training tokens per second (%): 6.24 | MFU (%): 33.08 | TFLOPs: 327.12 | Global batch size: 128 | Global tokens/sec: 565129.91 | Global MFU (%): 33.08 | Global TFLOPs: 5233.91 | 
2025-05-19 11:44:28,180 - root - INFO - Step: 260 | Loss: 6.33 | Tokens per second: 35638.24 | Training tokens per second (%): 6.24 | MFU (%): 33.37 | TFLOPs: 330.06 | Global batch size: 128 | Global tokens/sec: 570211.82 | Global MFU (%): 33.37 | Global TFLOPs: 5280.98 | 
2025-05-19 11:44:32,935 - root - INFO - Step: 270 | Loss: 6.31 | Tokens per second: 34461.04 | Training tokens per second (%): 6.24 | MFU (%): 32.27 | TFLOPs: 319.16 | Global batch size: 128 | Global tokens/sec: 551376.66 | Global MFU (%): 32.27 | Global TFLOPs: 5106.54 | 
2025-05-19 11:44:37,607 - root - INFO - Step: 280 | Loss: 6.38 | Tokens per second: 35073.81 | Training tokens per second (%): 6.24 | MFU (%): 32.84 | TFLOPs: 324.83 | Global batch size: 128 | Global tokens/sec: 561180.92 | Global MFU (%): 32.84 | Global TFLOPs: 5197.34 | 
2025-05-19 11:44:42,242 - root - INFO - Step: 290 | Loss: 6.25 | Tokens per second: 35353.97 | Training tokens per second (%): 6.24 | MFU (%): 33.11 | TFLOPs: 327.43 | Global batch size: 128 | Global tokens/sec: 565663.47 | Global MFU (%): 33.11 | Global TFLOPs: 5238.85 | 
2025-05-19 11:44:46,813 - root - INFO - Step: 300 | Loss: 6.29 | Tokens per second: 35846.91 | Training tokens per second (%): 6.24 | MFU (%): 33.57 | TFLOPs: 331.99 | Global batch size: 128 | Global tokens/sec: 573550.59 | Global MFU (%): 33.57 | Global TFLOPs: 5311.90 | 
2025-05-19 11:44:51,442 - root - INFO - Step: 310 | Loss: 6.14 | Tokens per second: 35397.32 | Training tokens per second (%): 6.24 | MFU (%): 33.15 | TFLOPs: 327.83 | Global batch size: 128 | Global tokens/sec: 566357.05 | Global MFU (%): 33.15 | Global TFLOPs: 5245.28 | 
2025-05-19 11:44:56,025 - root - INFO - Step: 320 | Loss: 6.54 | Tokens per second: 35754.35 | Training tokens per second (%): 6.24 | MFU (%): 33.48 | TFLOPs: 331.14 | Global batch size: 128 | Global tokens/sec: 572069.59 | Global MFU (%): 33.48 | Global TFLOPs: 5298.18 | 
2025-05-19 11:45:00,622 - root - INFO - Step: 330 | Loss: 6.21 | Tokens per second: 35646.02 | Training tokens per second (%): 6.23 | MFU (%): 33.38 | TFLOPs: 330.13 | Global batch size: 128 | Global tokens/sec: 570336.24 | Global MFU (%): 33.38 | Global TFLOPs: 5282.13 | 
2025-05-19 11:45:05,347 - root - INFO - Step: 340 | Loss: 6.25 | Tokens per second: 34682.54 | Training tokens per second (%): 6.24 | MFU (%): 32.48 | TFLOPs: 321.21 | Global batch size: 128 | Global tokens/sec: 554920.57 | Global MFU (%): 32.48 | Global TFLOPs: 5139.36 | 
2025-05-19 11:45:10,036 - root - INFO - Step: 350 | Loss: 6.14 | Tokens per second: 34944.64 | Training tokens per second (%): 6.24 | MFU (%): 32.72 | TFLOPs: 323.64 | Global batch size: 128 | Global tokens/sec: 559114.29 | Global MFU (%): 32.72 | Global TFLOPs: 5178.20 | 
2025-05-19 11:45:14,608 - root - INFO - Step: 360 | Loss: 6.10 | Tokens per second: 35842.02 | Training tokens per second (%): 6.24 | MFU (%): 33.56 | TFLOPs: 331.95 | Global batch size: 128 | Global tokens/sec: 573472.33 | Global MFU (%): 33.56 | Global TFLOPs: 5311.17 | 
2025-05-19 11:45:19,380 - root - INFO - Step: 370 | Loss: 6.04 | Tokens per second: 34340.87 | Training tokens per second (%): 6.24 | MFU (%): 32.16 | TFLOPs: 318.05 | Global batch size: 128 | Global tokens/sec: 549453.84 | Global MFU (%): 32.16 | Global TFLOPs: 5088.73 | 
2025-05-19 11:45:23,931 - root - INFO - Step: 380 | Loss: 6.12 | Tokens per second: 36005.67 | Training tokens per second (%): 6.24 | MFU (%): 33.72 | TFLOPs: 333.46 | Global batch size: 128 | Global tokens/sec: 576090.75 | Global MFU (%): 33.72 | Global TFLOPs: 5335.42 | 
2025-05-19 11:45:28,575 - root - INFO - Step: 390 | Loss: 6.02 | Tokens per second: 35285.33 | Training tokens per second (%): 6.24 | MFU (%): 33.04 | TFLOPs: 326.79 | Global batch size: 128 | Global tokens/sec: 564565.23 | Global MFU (%): 33.04 | Global TFLOPs: 5228.68 | 
2025-05-19 11:45:33,225 - root - INFO - Step: 400 | Loss: 5.99 | Tokens per second: 35236.40 | Training tokens per second (%): 6.24 | MFU (%): 33.00 | TFLOPs: 326.34 | Global batch size: 128 | Global tokens/sec: 563782.47 | Global MFU (%): 33.00 | Global TFLOPs: 5221.43 | 
2025-05-19 11:45:37,862 - root - INFO - Step: 410 | Loss: 5.98 | Tokens per second: 35341.63 | Training tokens per second (%): 6.24 | MFU (%): 33.10 | TFLOPs: 327.31 | Global batch size: 128 | Global tokens/sec: 565466.06 | Global MFU (%): 33.10 | Global TFLOPs: 5237.02 | 
2025-05-19 11:45:42,432 - root - INFO - Step: 420 | Loss: 5.96 | Tokens per second: 35856.87 | Training tokens per second (%): 6.24 | MFU (%): 33.58 | TFLOPs: 332.09 | Global batch size: 128 | Global tokens/sec: 573709.88 | Global MFU (%): 33.58 | Global TFLOPs: 5313.37 | 
2025-05-19 11:45:47,084 - root - INFO - Step: 430 | Loss: 6.04 | Tokens per second: 35223.47 | Training tokens per second (%): 6.24 | MFU (%): 32.98 | TFLOPs: 326.22 | Global batch size: 128 | Global tokens/sec: 563575.48 | Global MFU (%): 32.98 | Global TFLOPs: 5219.51 | 
2025-05-19 11:45:51,766 - root - INFO - Step: 440 | Loss: 6.06 | Tokens per second: 34996.06 | Training tokens per second (%): 6.24 | MFU (%): 32.77 | TFLOPs: 324.11 | Global batch size: 128 | Global tokens/sec: 559936.90 | Global MFU (%): 32.77 | Global TFLOPs: 5185.82 | 
2025-05-19 11:45:56,353 - root - INFO - Step: 450 | Loss: 5.97 | Tokens per second: 35728.23 | Training tokens per second (%): 6.24 | MFU (%): 33.46 | TFLOPs: 330.89 | Global batch size: 128 | Global tokens/sec: 571651.63 | Global MFU (%): 33.46 | Global TFLOPs: 5294.31 | 
2025-05-19 11:46:00,924 - root - INFO - Step: 460 | Loss: 6.03 | Tokens per second: 35847.68 | Training tokens per second (%): 6.24 | MFU (%): 33.57 | TFLOPs: 332.00 | Global batch size: 128 | Global tokens/sec: 573562.90 | Global MFU (%): 33.57 | Global TFLOPs: 5312.01 | 
2025-05-19 11:46:05,483 - root - INFO - Step: 470 | Loss: 5.90 | Tokens per second: 35939.13 | Training tokens per second (%): 6.24 | MFU (%): 33.65 | TFLOPs: 332.85 | Global batch size: 128 | Global tokens/sec: 575026.15 | Global MFU (%): 33.65 | Global TFLOPs: 5325.56 | 
2025-05-19 11:46:10,108 - root - INFO - Step: 480 | Loss: 5.90 | Tokens per second: 35430.84 | Training tokens per second (%): 6.24 | MFU (%): 33.18 | TFLOPs: 328.14 | Global batch size: 128 | Global tokens/sec: 566893.39 | Global MFU (%): 33.18 | Global TFLOPs: 5250.24 | 
2025-05-19 11:46:14,691 - root - INFO - Step: 490 | Loss: 5.86 | Tokens per second: 35755.88 | Training tokens per second (%): 6.24 | MFU (%): 33.48 | TFLOPs: 331.15 | Global batch size: 128 | Global tokens/sec: 572094.08 | Global MFU (%): 33.48 | Global TFLOPs: 5298.41 | 
2025-05-19 11:46:19,392 - root - INFO - Step: 500 | Loss: 5.86 | Tokens per second: 34852.41 | Training tokens per second (%): 6.24 | MFU (%): 32.64 | TFLOPs: 322.78 | Global batch size: 128 | Global tokens/sec: 557638.55 | Global MFU (%): 32.64 | Global TFLOPs: 5164.53 | 
2025-05-19 11:46:23,972 - root - INFO - Step: 510 | Loss: 5.90 | Tokens per second: 35778.10 | Training tokens per second (%): 6.24 | MFU (%): 33.50 | TFLOPs: 331.36 | Global batch size: 128 | Global tokens/sec: 572449.55 | Global MFU (%): 33.50 | Global TFLOPs: 5301.70 | 
2025-05-19 11:46:28,538 - root - INFO - Step: 520 | Loss: 5.90 | Tokens per second: 35894.15 | Training tokens per second (%): 6.24 | MFU (%): 33.61 | TFLOPs: 332.43 | Global batch size: 128 | Global tokens/sec: 574306.39 | Global MFU (%): 33.61 | Global TFLOPs: 5318.90 | 
2025-05-19 11:46:33,146 - root - INFO - Step: 530 | Loss: 5.88 | Tokens per second: 35555.24 | Training tokens per second (%): 6.24 | MFU (%): 33.30 | TFLOPs: 329.29 | Global batch size: 128 | Global tokens/sec: 568883.77 | Global MFU (%): 33.30 | Global TFLOPs: 5268.68 | 
2025-05-19 11:46:37,748 - root - INFO - Step: 540 | Loss: 5.86 | Tokens per second: 35609.24 | Training tokens per second (%): 6.24 | MFU (%): 33.35 | TFLOPs: 329.79 | Global batch size: 128 | Global tokens/sec: 569747.84 | Global MFU (%): 33.35 | Global TFLOPs: 5276.68 | 
2025-05-19 11:46:42,350 - root - INFO - Step: 550 | Loss: 5.74 | Tokens per second: 35604.87 | Training tokens per second (%): 6.24 | MFU (%): 33.34 | TFLOPs: 329.75 | Global batch size: 128 | Global tokens/sec: 569677.94 | Global MFU (%): 33.34 | Global TFLOPs: 5276.03 | 
2025-05-19 11:46:46,940 - root - INFO - Step: 560 | Loss: 5.78 | Tokens per second: 35703.27 | Training tokens per second (%): 6.24 | MFU (%): 33.43 | TFLOPs: 330.66 | Global batch size: 128 | Global tokens/sec: 571252.25 | Global MFU (%): 33.43 | Global TFLOPs: 5290.61 | 
2025-05-19 11:46:51,574 - root - INFO - Step: 570 | Loss: 5.73 | Tokens per second: 35360.66 | Training tokens per second (%): 6.24 | MFU (%): 33.11 | TFLOPs: 327.49 | Global batch size: 128 | Global tokens/sec: 565770.51 | Global MFU (%): 33.11 | Global TFLOPs: 5239.84 | 
2025-05-19 11:46:56,164 - root - INFO - Step: 580 | Loss: 5.74 | Tokens per second: 35697.27 | Training tokens per second (%): 6.24 | MFU (%): 33.43 | TFLOPs: 330.61 | Global batch size: 128 | Global tokens/sec: 571156.35 | Global MFU (%): 33.43 | Global TFLOPs: 5289.72 | 
2025-05-19 11:47:00,850 - root - INFO - Step: 590 | Loss: 5.80 | Tokens per second: 34971.52 | Training tokens per second (%): 6.24 | MFU (%): 32.75 | TFLOPs: 323.89 | Global batch size: 128 | Global tokens/sec: 559544.28 | Global MFU (%): 32.75 | Global TFLOPs: 5182.18 | 
2025-05-19 11:47:05,393 - root - INFO - Step: 600 | Loss: 5.76 | Tokens per second: 36074.19 | Training tokens per second (%): 6.24 | MFU (%): 33.78 | TFLOPs: 334.10 | Global batch size: 128 | Global tokens/sec: 577186.97 | Global MFU (%): 33.78 | Global TFLOPs: 5345.58 | 
2025-05-19 11:47:09,958 - root - INFO - Step: 610 | Loss: 5.74 | Tokens per second: 35893.06 | Training tokens per second (%): 6.24 | MFU (%): 33.61 | TFLOPs: 332.42 | Global batch size: 128 | Global tokens/sec: 574288.90 | Global MFU (%): 33.61 | Global TFLOPs: 5318.74 | 
2025-05-19 11:47:14,537 - root - INFO - Step: 620 | Loss: 5.70 | Tokens per second: 35782.50 | Training tokens per second (%): 6.24 | MFU (%): 33.51 | TFLOPs: 331.40 | Global batch size: 128 | Global tokens/sec: 572519.97 | Global MFU (%): 33.51 | Global TFLOPs: 5302.35 | 
2025-05-19 11:47:19,191 - root - INFO - Step: 630 | Loss: 5.72 | Tokens per second: 35209.64 | Training tokens per second (%): 6.24 | MFU (%): 32.97 | TFLOPs: 326.09 | Global batch size: 128 | Global tokens/sec: 563354.27 | Global MFU (%): 32.97 | Global TFLOPs: 5217.47 | 
2025-05-19 11:47:23,811 - root - INFO - Step: 640 | Loss: 5.72 | Tokens per second: 35472.34 | Training tokens per second (%): 6.24 | MFU (%): 33.22 | TFLOPs: 328.52 | Global batch size: 128 | Global tokens/sec: 567557.42 | Global MFU (%): 33.22 | Global TFLOPs: 5256.39 | 
2025-05-19 11:47:28,454 - root - INFO - Step: 650 | Loss: 5.73 | Tokens per second: 35293.98 | Training tokens per second (%): 6.24 | MFU (%): 33.05 | TFLOPs: 326.87 | Global batch size: 128 | Global tokens/sec: 564703.63 | Global MFU (%): 33.05 | Global TFLOPs: 5229.96 | 
2025-05-19 11:47:32,987 - root - INFO - Step: 660 | Loss: 5.69 | Tokens per second: 36147.26 | Training tokens per second (%): 6.24 | MFU (%): 33.85 | TFLOPs: 334.78 | Global batch size: 128 | Global tokens/sec: 578356.11 | Global MFU (%): 33.85 | Global TFLOPs: 5356.40 | 
2025-05-19 11:47:37,842 - root - INFO - Step: 670 | Loss: 5.65 | Tokens per second: 33747.62 | Training tokens per second (%): 6.24 | MFU (%): 31.60 | TFLOPs: 312.55 | Global batch size: 128 | Global tokens/sec: 539961.98 | Global MFU (%): 31.60 | Global TFLOPs: 5000.82 | 
2025-05-19 11:47:42,526 - root - INFO - Step: 680 | Loss: 5.70 | Tokens per second: 34983.21 | Training tokens per second (%): 6.24 | MFU (%): 32.76 | TFLOPs: 323.99 | Global batch size: 128 | Global tokens/sec: 559731.31 | Global MFU (%): 32.76 | Global TFLOPs: 5183.91 | 
2025-05-19 11:47:47,321 - root - INFO - Step: 690 | Loss: 5.59 | Tokens per second: 34179.43 | Training tokens per second (%): 6.24 | MFU (%): 32.01 | TFLOPs: 316.55 | Global batch size: 128 | Global tokens/sec: 546870.81 | Global MFU (%): 32.01 | Global TFLOPs: 5064.80 | 
2025-05-19 11:47:51,922 - root - INFO - Step: 700 | Loss: 5.63 | Tokens per second: 35615.29 | Training tokens per second (%): 6.24 | MFU (%): 33.35 | TFLOPs: 329.85 | Global batch size: 128 | Global tokens/sec: 569844.57 | Global MFU (%): 33.35 | Global TFLOPs: 5277.57 | 
2025-05-19 11:47:56,595 - root - INFO - Step: 710 | Loss: 5.68 | Tokens per second: 35059.96 | Training tokens per second (%): 6.24 | MFU (%): 32.83 | TFLOPs: 324.71 | Global batch size: 128 | Global tokens/sec: 560959.32 | Global MFU (%): 32.83 | Global TFLOPs: 5195.28 | 
2025-05-19 11:48:01,139 - root - INFO - Step: 720 | Loss: 5.64 | Tokens per second: 36064.66 | Training tokens per second (%): 6.24 | MFU (%): 33.77 | TFLOPs: 334.01 | Global batch size: 128 | Global tokens/sec: 577034.52 | Global MFU (%): 33.77 | Global TFLOPs: 5344.16 | 
2025-05-19 11:48:05,804 - root - INFO - Step: 730 | Loss: 5.59 | Tokens per second: 35126.16 | Training tokens per second (%): 6.24 | MFU (%): 32.89 | TFLOPs: 325.32 | Global batch size: 128 | Global tokens/sec: 562018.63 | Global MFU (%): 32.89 | Global TFLOPs: 5205.10 | 
2025-05-19 11:48:10,548 - root - INFO - Step: 740 | Loss: 5.64 | Tokens per second: 34541.19 | Training tokens per second (%): 6.24 | MFU (%): 32.35 | TFLOPs: 319.90 | Global batch size: 128 | Global tokens/sec: 552659.04 | Global MFU (%): 32.35 | Global TFLOPs: 5118.41 | 
2025-05-19 11:48:15,207 - root - INFO - Step: 750 | Loss: 5.56 | Tokens per second: 35174.40 | Training tokens per second (%): 6.24 | MFU (%): 32.94 | TFLOPs: 325.77 | Global batch size: 128 | Global tokens/sec: 562790.43 | Global MFU (%): 32.94 | Global TFLOPs: 5212.24 | 
2025-05-19 11:48:19,887 - root - INFO - Step: 760 | Loss: 5.59 | Tokens per second: 35010.53 | Training tokens per second (%): 6.24 | MFU (%): 32.79 | TFLOPs: 324.25 | Global batch size: 128 | Global tokens/sec: 560168.53 | Global MFU (%): 32.79 | Global TFLOPs: 5187.96 | 
2025-05-19 11:48:24,471 - root - INFO - Step: 770 | Loss: 5.47 | Tokens per second: 35746.10 | Training tokens per second (%): 6.24 | MFU (%): 33.47 | TFLOPs: 331.06 | Global batch size: 128 | Global tokens/sec: 571937.61 | Global MFU (%): 33.47 | Global TFLOPs: 5296.96 | 
2025-05-19 11:48:29,080 - root - INFO - Step: 780 | Loss: 5.63 | Tokens per second: 35556.64 | Training tokens per second (%): 6.24 | MFU (%): 33.30 | TFLOPs: 329.31 | Global batch size: 128 | Global tokens/sec: 568906.28 | Global MFU (%): 33.30 | Global TFLOPs: 5268.88 | 
2025-05-19 11:48:33,651 - root - INFO - Step: 790 | Loss: 5.50 | Tokens per second: 35841.87 | Training tokens per second (%): 6.24 | MFU (%): 33.56 | TFLOPs: 331.95 | Global batch size: 128 | Global tokens/sec: 573469.88 | Global MFU (%): 33.56 | Global TFLOPs: 5311.15 | 
2025-05-19 11:48:38,274 - root - INFO - Step: 800 | Loss: 5.60 | Tokens per second: 35446.29 | Training tokens per second (%): 6.24 | MFU (%): 33.19 | TFLOPs: 328.28 | Global batch size: 128 | Global tokens/sec: 567140.59 | Global MFU (%): 33.19 | Global TFLOPs: 5252.53 | 
2025-05-19 11:48:42,897 - root - INFO - Step: 810 | Loss: 5.52 | Tokens per second: 35448.99 | Training tokens per second (%): 6.24 | MFU (%): 33.20 | TFLOPs: 328.31 | Global batch size: 128 | Global tokens/sec: 567183.81 | Global MFU (%): 33.20 | Global TFLOPs: 5252.93 | 
2025-05-19 11:48:47,490 - root - INFO - Step: 820 | Loss: 5.51 | Tokens per second: 35677.36 | Training tokens per second (%): 6.24 | MFU (%): 33.41 | TFLOPs: 330.42 | Global batch size: 128 | Global tokens/sec: 570837.80 | Global MFU (%): 33.41 | Global TFLOPs: 5286.77 | 
2025-05-19 11:48:52,042 - root - INFO - Step: 830 | Loss: 5.55 | Tokens per second: 35998.89 | Training tokens per second (%): 6.24 | MFU (%): 33.71 | TFLOPs: 333.40 | Global batch size: 128 | Global tokens/sec: 575982.24 | Global MFU (%): 33.71 | Global TFLOPs: 5334.42 | 
2025-05-19 11:48:56,699 - root - INFO - Step: 840 | Loss: 5.56 | Tokens per second: 35185.81 | Training tokens per second (%): 6.24 | MFU (%): 32.95 | TFLOPs: 325.87 | Global batch size: 128 | Global tokens/sec: 562973.00 | Global MFU (%): 32.95 | Global TFLOPs: 5213.93 | 
2025-05-19 11:49:01,446 - root - INFO - Step: 850 | Loss: 5.48 | Tokens per second: 34518.00 | Training tokens per second (%): 6.24 | MFU (%): 32.32 | TFLOPs: 319.69 | Global batch size: 128 | Global tokens/sec: 552288.00 | Global MFU (%): 32.32 | Global TFLOPs: 5114.98 | 
2025-05-19 11:49:06,119 - root - INFO - Step: 860 | Loss: 5.45 | Tokens per second: 35062.97 | Training tokens per second (%): 6.24 | MFU (%): 32.83 | TFLOPs: 324.73 | Global batch size: 128 | Global tokens/sec: 561007.46 | Global MFU (%): 32.83 | Global TFLOPs: 5195.73 | 
2025-05-19 11:49:10,774 - root - INFO - Step: 870 | Loss: 5.43 | Tokens per second: 35208.94 | Training tokens per second (%): 6.24 | MFU (%): 32.97 | TFLOPs: 326.09 | Global batch size: 128 | Global tokens/sec: 563342.99 | Global MFU (%): 32.97 | Global TFLOPs: 5217.36 | 
2025-05-19 11:49:15,347 - root - INFO - Step: 880 | Loss: 5.48 | Tokens per second: 35828.54 | Training tokens per second (%): 6.24 | MFU (%): 33.55 | TFLOPs: 331.82 | Global batch size: 128 | Global tokens/sec: 573256.65 | Global MFU (%): 33.55 | Global TFLOPs: 5309.18 | 
2025-05-19 11:49:19,964 - root - INFO - Step: 890 | Loss: 5.42 | Tokens per second: 35490.96 | Training tokens per second (%): 6.24 | MFU (%): 33.24 | TFLOPs: 328.70 | Global batch size: 128 | Global tokens/sec: 567855.34 | Global MFU (%): 33.24 | Global TFLOPs: 5259.15 | 
2025-05-19 11:49:24,572 - root - INFO - Step: 900 | Loss: 5.39 | Tokens per second: 35561.03 | Training tokens per second (%): 6.24 | MFU (%): 33.30 | TFLOPs: 329.35 | Global batch size: 128 | Global tokens/sec: 568976.41 | Global MFU (%): 33.30 | Global TFLOPs: 5269.53 | 
2025-05-19 11:49:29,214 - root - INFO - Step: 910 | Loss: 5.39 | Tokens per second: 35300.49 | Training tokens per second (%): 6.24 | MFU (%): 33.06 | TFLOPs: 326.93 | Global batch size: 128 | Global tokens/sec: 564807.80 | Global MFU (%): 33.06 | Global TFLOPs: 5230.93 | 
2025-05-19 11:49:33,838 - root - INFO - Step: 920 | Loss: 5.46 | Tokens per second: 35440.59 | Training tokens per second (%): 6.24 | MFU (%): 33.19 | TFLOPs: 328.23 | Global batch size: 128 | Global tokens/sec: 567049.40 | Global MFU (%): 33.19 | Global TFLOPs: 5251.69 | 
2025-05-19 11:49:38,497 - root - INFO - Step: 930 | Loss: 5.40 | Tokens per second: 35165.96 | Training tokens per second (%): 6.24 | MFU (%): 32.93 | TFLOPs: 325.69 | Global batch size: 128 | Global tokens/sec: 562655.38 | Global MFU (%): 32.93 | Global TFLOPs: 5210.99 | 
2025-05-19 11:49:43,187 - root - INFO - Step: 940 | Loss: 5.45 | Tokens per second: 34942.01 | Training tokens per second (%): 6.24 | MFU (%): 32.72 | TFLOPs: 323.61 | Global batch size: 128 | Global tokens/sec: 559072.14 | Global MFU (%): 32.72 | Global TFLOPs: 5177.81 | 
2025-05-19 11:49:47,786 - root - INFO - Step: 950 | Loss: 5.42 | Tokens per second: 35629.14 | Training tokens per second (%): 6.24 | MFU (%): 33.36 | TFLOPs: 329.98 | Global batch size: 128 | Global tokens/sec: 570066.19 | Global MFU (%): 33.36 | Global TFLOPs: 5279.63 | 
2025-05-19 11:49:52,531 - root - INFO - Step: 960 | Loss: 5.39 | Tokens per second: 34534.03 | Training tokens per second (%): 6.24 | MFU (%): 32.34 | TFLOPs: 319.83 | Global batch size: 128 | Global tokens/sec: 552544.43 | Global MFU (%): 32.34 | Global TFLOPs: 5117.35 | 
2025-05-19 11:49:57,193 - root - INFO - Step: 970 | Loss: 5.39 | Tokens per second: 35147.78 | Training tokens per second (%): 6.24 | MFU (%): 32.91 | TFLOPs: 325.52 | Global batch size: 128 | Global tokens/sec: 562364.56 | Global MFU (%): 32.91 | Global TFLOPs: 5208.30 | 
2025-05-19 11:50:01,786 - root - INFO - Step: 980 | Loss: 5.48 | Tokens per second: 35676.32 | Training tokens per second (%): 6.24 | MFU (%): 33.41 | TFLOPs: 330.41 | Global batch size: 128 | Global tokens/sec: 570821.06 | Global MFU (%): 33.41 | Global TFLOPs: 5286.62 | 
2025-05-19 11:50:06,387 - root - INFO - Step: 990 | Loss: 5.40 | Tokens per second: 35621.29 | Training tokens per second (%): 6.24 | MFU (%): 33.36 | TFLOPs: 329.90 | Global batch size: 128 | Global tokens/sec: 569940.72 | Global MFU (%): 33.36 | Global TFLOPs: 5278.47 | 
2025-05-19 11:50:10,986 - root - INFO - Step: 1000 | Loss: 5.47 | Tokens per second: 35626.80 | Training tokens per second (%): 6.24 | MFU (%): 33.36 | TFLOPs: 329.96 | Global batch size: 128 | Global tokens/sec: 570028.80 | Global MFU (%): 33.36 | Global TFLOPs: 5279.28 | 
2025-05-19 11:50:10,986 - root - INFO - Training completed
[sbatch-master] task finished
