[sbatch-master] running on nid006548
[sbatch-master] SLURM_NODELIST: nid[006548,006556,006563]
[sbatch-master] SLURM_NNODES: 3
[sbatch-master] SLURM_NODEID: 0
[Master] Master node: nid006548
[Master] World size: 12
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
[srun] rank=0 host=nid006548 noderank=0 localrank=0
W0518 20:56:16.619000 266090 torch/distributed/run.py:792] 
W0518 20:56:16.619000 266090 torch/distributed/run.py:792] *****************************************
W0518 20:56:16.619000 266090 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0518 20:56:16.619000 266090 torch/distributed/run.py:792] *****************************************
[srun] rank=1 host=nid006556 noderank=1 localrank=0
[srun] rank=2 host=nid006563 noderank=2 localrank=0
W0518 20:56:19.988000 284019 torch/distributed/run.py:792] 
W0518 20:56:19.988000 284019 torch/distributed/run.py:792] *****************************************
W0518 20:56:19.988000 284019 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0518 20:56:19.988000 284019 torch/distributed/run.py:792] *****************************************
W0518 20:56:21.016000 257186 torch/distributed/run.py:792] 
W0518 20:56:21.016000 257186 torch/distributed/run.py:792] *****************************************
W0518 20:56:21.016000 257186 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0518 20:56:21.016000 257186 torch/distributed/run.py:792] *****************************************
2025-05-18 20:56:34,029 - root - INFO - [Distributed Init] Rank 0 initialized on node 0 on GPU 0.
[rank0]:[W518 20:56:34.291543027 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 20:56:34,531 - root - INFO - [Distributed Init] Rank 1 initialized on node 0 on GPU 1.
[rank1]:[W518 20:56:34.372414566 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 20:56:34,581 - root - INFO - [Distributed Init] Rank 2 initialized on node 0 on GPU 2.
2025-05-18 20:56:34,581 - root - INFO - [Distributed Init] Rank 3 initialized on node 0 on GPU 3.
[rank3]:[W518 20:56:34.422981681 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W518 20:56:34.422981905 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 20:56:35,187 - root - INFO - [Distributed Init] Rank 4 initialized on node 1 on GPU 0.
2025-05-18 20:56:35,438 - root - INFO - [Distributed Init] Rank 8 initialized on node 2 on GPU 0.
[rank4]:[W518 20:56:35.525159670 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 20:56:35,815 - root - INFO - [Distributed Init] Rank 7 initialized on node 1 on GPU 3.
[rank7]:[W518 20:56:35.623315246 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 7]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 20:56:35,864 - root - INFO - [Distributed Init] Rank 5 initialized on node 1 on GPU 1.
[rank5]:[W518 20:56:35.672424744 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 5]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 20:56:35,873 - root - INFO - [Distributed Init] Rank 6 initialized on node 1 on GPU 2.
[rank6]:[W518 20:56:35.681930974 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 6]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank8]:[W518 20:56:36.331084152 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 8]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 20:56:36,168 - root - INFO - [Distributed Init] Rank 9 initialized on node 2 on GPU 1.
[rank9]:[W518 20:56:36.442413379 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 9]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 20:56:36,229 - root - INFO - [Distributed Init] Rank 10 initialized on node 2 on GPU 2.
2025-05-18 20:56:36,229 - root - INFO - [Distributed Init] Rank 11 initialized on node 2 on GPU 3.
[rank10]:[W518 20:56:36.503209991 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 10]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank11]:[W518 20:56:36.503589723 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 11]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 20:56:41,792 - root - INFO - [Rank 0] All ranks ready!
2025-05-18 20:56:41,793 - root - INFO - Distributed training enabled: 12 processes
2025-05-18 20:56:41,793 - root - INFO - Master process: 0 on cuda:0
2025-05-18 20:56:41,793 - root - INFO - Experiment args: Namespace(dataset='/capstor/scratch/cscs/kasparr/project/train_data.parquet', dataset_type='padded', pretokenized=False, tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=1, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=10, profile=False, profile_step_start=10, profile_step_end=10, grad_max_norm=1, model_dtype='bf16', compile=False, distributed=True, backend='nccl', find_unused_parameters=False)
2025-05-18 20:56:41,793 - root - INFO - Setting up Tokenizer...
2025-05-18 20:56:42,331 - root - INFO - Setting up DataLoaders...
2025-05-18 20:56:42,332 - root - INFO - Using padded ParquetDataset with on-the-fly tokenization
2025-05-18 20:56:51,379 - root - INFO - Setting up Model...
2025-05-18 20:57:27,519 - root - INFO - Model wrapped with DistributedDataParallel
2025-05-18 20:57:27,521 - root - INFO - Global batch size: 12 (local: 1 Ã— 12 processes)
2025-05-18 20:57:27,521 - root - INFO - Starting training!
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
2025-05-18 20:57:29,804 - root - INFO - Step: 1 | Loss: 11.96 | Tokens per second: 897.52 | Training tokens per second (%): 4.09 | MFU (%): 4.68 | TFLOPs: 46.26 | Global batch size: 12 | Global tokens/sec: 10770.22 | Global MFU (%): 4.68 | Global TFLOPs: 555.11 | 
2025-05-18 20:57:34,818 - root - INFO - Step: 10 | Loss: 11.71 | Tokens per second: 3676.58 | Training tokens per second (%): 2.52 | MFU (%): 19.16 | TFLOPs: 189.50 | Global batch size: 12 | Global tokens/sec: 44118.92 | Global MFU (%): 19.16 | Global TFLOPs: 2273.94 | 
2025-05-18 20:57:40,536 - root - INFO - Step: 20 | Loss: 10.49 | Tokens per second: 3582.12 | Training tokens per second (%): 2.55 | MFU (%): 18.67 | TFLOPs: 184.63 | Global batch size: 12 | Global tokens/sec: 42985.45 | Global MFU (%): 18.67 | Global TFLOPs: 2215.52 | 
2025-05-18 20:57:46,082 - root - INFO - Step: 30 | Loss: 9.59 | Tokens per second: 3693.77 | Training tokens per second (%): 1.84 | MFU (%): 19.25 | TFLOPs: 190.38 | Global batch size: 12 | Global tokens/sec: 44325.25 | Global MFU (%): 19.25 | Global TFLOPs: 2284.58 | 
2025-05-18 20:57:51,652 - root - INFO - Step: 40 | Loss: 9.07 | Tokens per second: 3677.17 | Training tokens per second (%): 3.50 | MFU (%): 19.16 | TFLOPs: 189.53 | Global batch size: 12 | Global tokens/sec: 44126.06 | Global MFU (%): 19.16 | Global TFLOPs: 2274.31 | 
2025-05-18 20:57:57,222 - root - INFO - Step: 50 | Loss: 8.53 | Tokens per second: 3677.91 | Training tokens per second (%): 2.40 | MFU (%): 19.17 | TFLOPs: 189.56 | Global batch size: 12 | Global tokens/sec: 44134.94 | Global MFU (%): 19.17 | Global TFLOPs: 2274.77 | 
2025-05-18 20:58:02,854 - root - INFO - Step: 60 | Loss: 8.05 | Tokens per second: 3636.64 | Training tokens per second (%): 2.94 | MFU (%): 18.95 | TFLOPs: 187.44 | Global batch size: 12 | Global tokens/sec: 43639.62 | Global MFU (%): 18.95 | Global TFLOPs: 2249.24 | 
2025-05-18 20:58:08,429 - root - INFO - Step: 70 | Loss: 7.64 | Tokens per second: 3674.32 | Training tokens per second (%): 2.36 | MFU (%): 19.15 | TFLOPs: 189.38 | Global batch size: 12 | Global tokens/sec: 44091.81 | Global MFU (%): 19.15 | Global TFLOPs: 2272.54 | 
2025-05-18 20:58:14,011 - root - INFO - Step: 80 | Loss: 7.36 | Tokens per second: 3669.89 | Training tokens per second (%): 1.72 | MFU (%): 19.13 | TFLOPs: 189.15 | Global batch size: 12 | Global tokens/sec: 44038.67 | Global MFU (%): 19.13 | Global TFLOPs: 2269.81 | 
2025-05-18 20:58:19,567 - root - INFO - Step: 90 | Loss: 7.17 | Tokens per second: 3686.32 | Training tokens per second (%): 2.52 | MFU (%): 19.21 | TFLOPs: 190.00 | Global batch size: 12 | Global tokens/sec: 44235.83 | Global MFU (%): 19.21 | Global TFLOPs: 2279.97 | 
2025-05-18 20:58:25,204 - root - INFO - Step: 100 | Loss: 7.06 | Tokens per second: 3634.00 | Training tokens per second (%): 4.05 | MFU (%): 18.94 | TFLOPs: 187.30 | Global batch size: 12 | Global tokens/sec: 43608.06 | Global MFU (%): 18.94 | Global TFLOPs: 2247.61 | 
2025-05-18 20:58:30,783 - root - INFO - Step: 110 | Loss: 7.04 | Tokens per second: 3671.55 | Training tokens per second (%): 3.52 | MFU (%): 19.13 | TFLOPs: 189.24 | Global batch size: 12 | Global tokens/sec: 44058.59 | Global MFU (%): 19.13 | Global TFLOPs: 2270.83 | 
2025-05-18 20:58:36,324 - root - INFO - Step: 120 | Loss: 6.82 | Tokens per second: 3696.67 | Training tokens per second (%): 3.06 | MFU (%): 19.27 | TFLOPs: 190.53 | Global batch size: 12 | Global tokens/sec: 44360.09 | Global MFU (%): 19.27 | Global TFLOPs: 2286.37 | 
2025-05-18 20:58:41,880 - root - INFO - Step: 130 | Loss: 6.99 | Tokens per second: 3686.85 | Training tokens per second (%): 2.58 | MFU (%): 19.21 | TFLOPs: 190.02 | Global batch size: 12 | Global tokens/sec: 44242.16 | Global MFU (%): 19.21 | Global TFLOPs: 2280.29 | 
2025-05-18 20:58:47,428 - root - INFO - Step: 140 | Loss: 6.87 | Tokens per second: 3691.69 | Training tokens per second (%): 2.48 | MFU (%): 19.24 | TFLOPs: 190.27 | Global batch size: 12 | Global tokens/sec: 44300.30 | Global MFU (%): 19.24 | Global TFLOPs: 2283.29 | 
2025-05-18 20:58:52,997 - root - INFO - Step: 150 | Loss: 6.81 | Tokens per second: 3678.38 | Training tokens per second (%): 3.02 | MFU (%): 19.17 | TFLOPs: 189.59 | Global batch size: 12 | Global tokens/sec: 44140.59 | Global MFU (%): 19.17 | Global TFLOPs: 2275.06 | 
2025-05-18 20:58:58,585 - root - INFO - Step: 160 | Loss: 6.56 | Tokens per second: 3665.64 | Training tokens per second (%): 2.89 | MFU (%): 19.10 | TFLOPs: 188.93 | Global batch size: 12 | Global tokens/sec: 43987.69 | Global MFU (%): 19.10 | Global TFLOPs: 2267.18 | 
2025-05-18 20:59:04,142 - root - INFO - Step: 170 | Loss: 6.13 | Tokens per second: 3686.19 | Training tokens per second (%): 3.24 | MFU (%): 19.21 | TFLOPs: 189.99 | Global batch size: 12 | Global tokens/sec: 44234.25 | Global MFU (%): 19.21 | Global TFLOPs: 2279.89 | 
2025-05-18 20:59:09,693 - root - INFO - Step: 180 | Loss: 5.83 | Tokens per second: 3689.84 | Training tokens per second (%): 3.45 | MFU (%): 19.23 | TFLOPs: 190.18 | Global batch size: 12 | Global tokens/sec: 44278.14 | Global MFU (%): 19.23 | Global TFLOPs: 2282.15 | 
2025-05-18 20:59:15,267 - root - INFO - Step: 190 | Loss: 6.10 | Tokens per second: 3674.57 | Training tokens per second (%): 2.88 | MFU (%): 19.15 | TFLOPs: 189.39 | Global batch size: 12 | Global tokens/sec: 44094.88 | Global MFU (%): 19.15 | Global TFLOPs: 2272.70 | 
2025-05-18 20:59:20,832 - root - INFO - Step: 200 | Loss: 6.36 | Tokens per second: 3680.89 | Training tokens per second (%): 3.03 | MFU (%): 19.18 | TFLOPs: 189.72 | Global batch size: 12 | Global tokens/sec: 44170.65 | Global MFU (%): 19.18 | Global TFLOPs: 2276.61 | 
2025-05-18 20:59:26,396 - root - INFO - Step: 210 | Loss: 5.86 | Tokens per second: 3681.21 | Training tokens per second (%): 3.82 | MFU (%): 19.18 | TFLOPs: 189.73 | Global batch size: 12 | Global tokens/sec: 44174.47 | Global MFU (%): 19.18 | Global TFLOPs: 2276.81 | 
2025-05-18 20:59:31,975 - root - INFO - Step: 220 | Loss: 6.04 | Tokens per second: 3671.53 | Training tokens per second (%): 2.42 | MFU (%): 19.13 | TFLOPs: 189.24 | Global batch size: 12 | Global tokens/sec: 44058.40 | Global MFU (%): 19.13 | Global TFLOPs: 2270.82 | 
2025-05-18 20:59:37,543 - root - INFO - Step: 230 | Loss: 5.94 | Tokens per second: 3679.24 | Training tokens per second (%): 2.80 | MFU (%): 19.17 | TFLOPs: 189.63 | Global batch size: 12 | Global tokens/sec: 44150.85 | Global MFU (%): 19.17 | Global TFLOPs: 2275.59 | 
2025-05-18 20:59:43,115 - root - INFO - Step: 240 | Loss: 6.12 | Tokens per second: 3675.97 | Training tokens per second (%): 3.21 | MFU (%): 19.16 | TFLOPs: 189.46 | Global batch size: 12 | Global tokens/sec: 44111.59 | Global MFU (%): 19.16 | Global TFLOPs: 2273.56 | 
2025-05-18 20:59:48,698 - root - INFO - Step: 250 | Loss: 5.96 | Tokens per second: 3668.62 | Training tokens per second (%): 2.35 | MFU (%): 19.12 | TFLOPs: 189.09 | Global batch size: 12 | Global tokens/sec: 44023.43 | Global MFU (%): 19.12 | Global TFLOPs: 2269.02 | 
2025-05-18 20:59:54,265 - root - INFO - Step: 260 | Loss: 5.60 | Tokens per second: 3679.52 | Training tokens per second (%): 2.86 | MFU (%): 19.18 | TFLOPs: 189.65 | Global batch size: 12 | Global tokens/sec: 44154.26 | Global MFU (%): 19.18 | Global TFLOPs: 2275.76 | 
2025-05-18 20:59:59,818 - root - INFO - Step: 270 | Loss: 5.21 | Tokens per second: 3688.69 | Training tokens per second (%): 3.49 | MFU (%): 19.22 | TFLOPs: 190.12 | Global batch size: 12 | Global tokens/sec: 44264.32 | Global MFU (%): 19.22 | Global TFLOPs: 2281.44 | 
2025-05-18 21:00:05,420 - root - INFO - Step: 280 | Loss: 5.16 | Tokens per second: 3656.58 | Training tokens per second (%): 3.60 | MFU (%): 19.06 | TFLOPs: 188.46 | Global batch size: 12 | Global tokens/sec: 43879.02 | Global MFU (%): 19.06 | Global TFLOPs: 2261.58 | 
2025-05-18 21:00:10,983 - root - INFO - Step: 290 | Loss: 4.74 | Tokens per second: 3681.84 | Training tokens per second (%): 3.13 | MFU (%): 19.19 | TFLOPs: 189.77 | Global batch size: 12 | Global tokens/sec: 44182.09 | Global MFU (%): 19.19 | Global TFLOPs: 2277.20 | 
2025-05-18 21:00:16,563 - root - INFO - Step: 300 | Loss: 5.03 | Tokens per second: 3671.17 | Training tokens per second (%): 2.97 | MFU (%): 19.13 | TFLOPs: 189.22 | Global batch size: 12 | Global tokens/sec: 44053.99 | Global MFU (%): 19.13 | Global TFLOPs: 2270.60 | 
2025-05-18 21:00:22,139 - root - INFO - Step: 310 | Loss: 5.24 | Tokens per second: 3673.76 | Training tokens per second (%): 2.86 | MFU (%): 19.15 | TFLOPs: 189.35 | Global batch size: 12 | Global tokens/sec: 44085.09 | Global MFU (%): 19.15 | Global TFLOPs: 2272.20 | 
2025-05-18 21:00:27,703 - root - INFO - Step: 320 | Loss: 5.60 | Tokens per second: 3681.36 | Training tokens per second (%): 4.73 | MFU (%): 19.19 | TFLOPs: 189.74 | Global batch size: 12 | Global tokens/sec: 44176.37 | Global MFU (%): 19.19 | Global TFLOPs: 2276.90 | 
2025-05-18 21:00:33,261 - root - INFO - Step: 330 | Loss: 5.09 | Tokens per second: 3685.30 | Training tokens per second (%): 2.86 | MFU (%): 19.21 | TFLOPs: 189.94 | Global batch size: 12 | Global tokens/sec: 44223.55 | Global MFU (%): 19.21 | Global TFLOPs: 2279.34 | 
2025-05-18 21:00:38,840 - root - INFO - Step: 340 | Loss: 4.53 | Tokens per second: 3671.48 | Training tokens per second (%): 4.63 | MFU (%): 19.13 | TFLOPs: 189.23 | Global batch size: 12 | Global tokens/sec: 44057.71 | Global MFU (%): 19.13 | Global TFLOPs: 2270.79 | 
2025-05-18 21:00:44,408 - root - INFO - Step: 350 | Loss: 4.67 | Tokens per second: 3678.73 | Training tokens per second (%): 2.76 | MFU (%): 19.17 | TFLOPs: 189.61 | Global batch size: 12 | Global tokens/sec: 44144.72 | Global MFU (%): 19.17 | Global TFLOPs: 2275.27 | 
2025-05-18 21:00:50,001 - root - INFO - Step: 360 | Loss: 5.07 | Tokens per second: 3661.96 | Training tokens per second (%): 3.10 | MFU (%): 19.08 | TFLOPs: 188.74 | Global batch size: 12 | Global tokens/sec: 43943.54 | Global MFU (%): 19.08 | Global TFLOPs: 2264.90 | 
2025-05-18 21:00:55,573 - root - INFO - Step: 370 | Loss: 4.81 | Tokens per second: 3676.74 | Training tokens per second (%): 3.25 | MFU (%): 19.16 | TFLOPs: 189.50 | Global batch size: 12 | Global tokens/sec: 44120.92 | Global MFU (%): 19.16 | Global TFLOPs: 2274.05 | 
2025-05-18 21:01:01,153 - root - INFO - Step: 380 | Loss: 4.40 | Tokens per second: 3670.57 | Training tokens per second (%): 3.45 | MFU (%): 19.13 | TFLOPs: 189.19 | Global batch size: 12 | Global tokens/sec: 44046.90 | Global MFU (%): 19.13 | Global TFLOPs: 2270.23 | 
2025-05-18 21:01:06,715 - root - INFO - Step: 390 | Loss: 4.63 | Tokens per second: 3682.61 | Training tokens per second (%): 1.98 | MFU (%): 19.19 | TFLOPs: 189.81 | Global batch size: 12 | Global tokens/sec: 44191.34 | Global MFU (%): 19.19 | Global TFLOPs: 2277.68 | 
2025-05-18 21:01:12,295 - root - INFO - Step: 400 | Loss: 4.85 | Tokens per second: 3671.27 | Training tokens per second (%): 3.53 | MFU (%): 19.13 | TFLOPs: 189.22 | Global batch size: 12 | Global tokens/sec: 44055.22 | Global MFU (%): 19.13 | Global TFLOPs: 2270.66 | 
2025-05-18 21:01:17,891 - root - INFO - Step: 410 | Loss: 4.62 | Tokens per second: 3660.32 | Training tokens per second (%): 5.64 | MFU (%): 19.08 | TFLOPs: 188.66 | Global batch size: 12 | Global tokens/sec: 43923.85 | Global MFU (%): 19.08 | Global TFLOPs: 2263.89 | 
2025-05-18 21:01:23,472 - root - INFO - Step: 420 | Loss: 4.32 | Tokens per second: 3669.90 | Training tokens per second (%): 4.02 | MFU (%): 19.13 | TFLOPs: 189.15 | Global batch size: 12 | Global tokens/sec: 44038.82 | Global MFU (%): 19.13 | Global TFLOPs: 2269.81 | 
2025-05-18 21:01:29,073 - root - INFO - Step: 430 | Loss: 4.20 | Tokens per second: 3656.90 | Training tokens per second (%): 1.73 | MFU (%): 19.06 | TFLOPs: 188.48 | Global batch size: 12 | Global tokens/sec: 43882.84 | Global MFU (%): 19.06 | Global TFLOPs: 2261.77 | 
2025-05-18 21:01:34,643 - root - INFO - Step: 440 | Loss: 4.14 | Tokens per second: 3677.57 | Training tokens per second (%): 3.12 | MFU (%): 19.17 | TFLOPs: 189.55 | Global batch size: 12 | Global tokens/sec: 44130.81 | Global MFU (%): 19.17 | Global TFLOPs: 2274.56 | 
2025-05-18 21:01:40,207 - root - INFO - Step: 450 | Loss: 3.59 | Tokens per second: 3681.39 | Training tokens per second (%): 2.45 | MFU (%): 19.19 | TFLOPs: 189.74 | Global batch size: 12 | Global tokens/sec: 44176.67 | Global MFU (%): 19.19 | Global TFLOPs: 2276.92 | 
2025-05-18 21:01:45,807 - root - INFO - Step: 460 | Loss: 4.36 | Tokens per second: 3658.10 | Training tokens per second (%): 2.10 | MFU (%): 19.06 | TFLOPs: 188.54 | Global batch size: 12 | Global tokens/sec: 43897.16 | Global MFU (%): 19.06 | Global TFLOPs: 2262.51 | 
2025-05-18 21:01:51,375 - root - INFO - Step: 470 | Loss: 3.78 | Tokens per second: 3678.66 | Training tokens per second (%): 3.70 | MFU (%): 19.17 | TFLOPs: 189.60 | Global batch size: 12 | Global tokens/sec: 44143.98 | Global MFU (%): 19.17 | Global TFLOPs: 2275.23 | 
2025-05-18 21:01:56,960 - root - INFO - Step: 480 | Loss: 3.92 | Tokens per second: 3667.44 | Training tokens per second (%): 3.39 | MFU (%): 19.11 | TFLOPs: 189.02 | Global batch size: 12 | Global tokens/sec: 44009.26 | Global MFU (%): 19.11 | Global TFLOPs: 2268.29 | 
2025-05-18 21:02:02,542 - root - INFO - Step: 490 | Loss: 4.56 | Tokens per second: 3669.94 | Training tokens per second (%): 5.40 | MFU (%): 19.13 | TFLOPs: 189.15 | Global batch size: 12 | Global tokens/sec: 44039.30 | Global MFU (%): 19.13 | Global TFLOPs: 2269.84 | 
2025-05-18 21:02:08,122 - root - INFO - Step: 500 | Loss: 4.07 | Tokens per second: 3670.27 | Training tokens per second (%): 2.98 | MFU (%): 19.13 | TFLOPs: 189.17 | Global batch size: 12 | Global tokens/sec: 44043.26 | Global MFU (%): 19.13 | Global TFLOPs: 2270.04 | 
2025-05-18 21:02:13,698 - root - INFO - Step: 510 | Loss: 3.63 | Tokens per second: 3673.77 | Training tokens per second (%): 3.18 | MFU (%): 19.15 | TFLOPs: 189.35 | Global batch size: 12 | Global tokens/sec: 44085.21 | Global MFU (%): 19.15 | Global TFLOPs: 2272.20 | 
2025-05-18 21:02:19,273 - root - INFO - Step: 520 | Loss: 3.90 | Tokens per second: 3673.99 | Training tokens per second (%): 3.68 | MFU (%): 19.15 | TFLOPs: 189.36 | Global batch size: 12 | Global tokens/sec: 44087.85 | Global MFU (%): 19.15 | Global TFLOPs: 2272.34 | 
2025-05-18 21:02:24,843 - root - INFO - Step: 530 | Loss: 3.17 | Tokens per second: 3677.63 | Training tokens per second (%): 3.06 | MFU (%): 19.17 | TFLOPs: 189.55 | Global batch size: 12 | Global tokens/sec: 44131.55 | Global MFU (%): 19.17 | Global TFLOPs: 2274.59 | 
2025-05-18 21:02:30,402 - root - INFO - Step: 540 | Loss: 3.46 | Tokens per second: 3684.69 | Training tokens per second (%): 2.87 | MFU (%): 19.20 | TFLOPs: 189.91 | Global batch size: 12 | Global tokens/sec: 44216.32 | Global MFU (%): 19.20 | Global TFLOPs: 2278.96 | 
2025-05-18 21:02:35,968 - root - INFO - Step: 550 | Loss: 3.46 | Tokens per second: 3679.90 | Training tokens per second (%): 4.20 | MFU (%): 19.18 | TFLOPs: 189.67 | Global batch size: 12 | Global tokens/sec: 44158.81 | Global MFU (%): 19.18 | Global TFLOPs: 2276.00 | 
2025-05-18 21:02:41,523 - root - INFO - Step: 560 | Loss: 3.27 | Tokens per second: 3687.53 | Training tokens per second (%): 3.71 | MFU (%): 19.22 | TFLOPs: 190.06 | Global batch size: 12 | Global tokens/sec: 44250.35 | Global MFU (%): 19.22 | Global TFLOPs: 2280.72 | 
2025-05-18 21:02:47,089 - root - INFO - Step: 570 | Loss: 4.09 | Tokens per second: 3680.16 | Training tokens per second (%): 4.23 | MFU (%): 19.18 | TFLOPs: 189.68 | Global batch size: 12 | Global tokens/sec: 44161.96 | Global MFU (%): 19.18 | Global TFLOPs: 2276.16 | 
2025-05-18 21:02:52,672 - root - INFO - Step: 580 | Loss: 3.79 | Tokens per second: 3668.70 | Training tokens per second (%): 3.76 | MFU (%): 19.12 | TFLOPs: 189.09 | Global batch size: 12 | Global tokens/sec: 44024.41 | Global MFU (%): 19.12 | Global TFLOPs: 2269.07 | 
2025-05-18 21:02:58,269 - root - INFO - Step: 590 | Loss: 3.00 | Tokens per second: 3659.62 | Training tokens per second (%): 3.18 | MFU (%): 19.07 | TFLOPs: 188.62 | Global batch size: 12 | Global tokens/sec: 43915.45 | Global MFU (%): 19.07 | Global TFLOPs: 2263.46 | 
2025-05-18 21:03:03,825 - root - INFO - Step: 600 | Loss: 3.09 | Tokens per second: 3686.87 | Training tokens per second (%): 4.69 | MFU (%): 19.21 | TFLOPs: 190.03 | Global batch size: 12 | Global tokens/sec: 44242.43 | Global MFU (%): 19.21 | Global TFLOPs: 2280.31 | 
2025-05-18 21:03:09,398 - root - INFO - Step: 610 | Loss: 2.72 | Tokens per second: 3675.64 | Training tokens per second (%): 2.78 | MFU (%): 19.16 | TFLOPs: 189.45 | Global batch size: 12 | Global tokens/sec: 44107.63 | Global MFU (%): 19.16 | Global TFLOPs: 2273.36 | 
2025-05-18 21:03:14,982 - root - INFO - Step: 620 | Loss: 3.46 | Tokens per second: 3667.98 | Training tokens per second (%): 4.28 | MFU (%): 19.12 | TFLOPs: 189.05 | Global batch size: 12 | Global tokens/sec: 44015.80 | Global MFU (%): 19.12 | Global TFLOPs: 2268.63 | 
2025-05-18 21:03:20,550 - root - INFO - Step: 630 | Loss: 2.65 | Tokens per second: 3678.70 | Training tokens per second (%): 4.00 | MFU (%): 19.17 | TFLOPs: 189.60 | Global batch size: 12 | Global tokens/sec: 44144.36 | Global MFU (%): 19.17 | Global TFLOPs: 2275.25 | 
2025-05-18 21:03:26,120 - root - INFO - Step: 640 | Loss: 2.86 | Tokens per second: 3677.43 | Training tokens per second (%): 3.56 | MFU (%): 19.16 | TFLOPs: 189.54 | Global batch size: 12 | Global tokens/sec: 44129.17 | Global MFU (%): 19.16 | Global TFLOPs: 2274.47 | 
2025-05-18 21:03:31,699 - root - INFO - Step: 650 | Loss: 2.68 | Tokens per second: 3671.97 | Training tokens per second (%): 3.39 | MFU (%): 19.14 | TFLOPs: 189.26 | Global batch size: 12 | Global tokens/sec: 44063.68 | Global MFU (%): 19.14 | Global TFLOPs: 2271.10 | 
2025-05-18 21:03:37,267 - root - INFO - Step: 660 | Loss: 2.94 | Tokens per second: 3678.81 | Training tokens per second (%): 3.07 | MFU (%): 19.17 | TFLOPs: 189.61 | Global batch size: 12 | Global tokens/sec: 44145.75 | Global MFU (%): 19.17 | Global TFLOPs: 2275.33 | 
2025-05-18 21:03:42,836 - root - INFO - Step: 670 | Loss: 2.70 | Tokens per second: 3677.54 | Training tokens per second (%): 3.46 | MFU (%): 19.17 | TFLOPs: 189.54 | Global batch size: 12 | Global tokens/sec: 44130.50 | Global MFU (%): 19.17 | Global TFLOPs: 2274.54 | 
2025-05-18 21:03:48,408 - root - INFO - Step: 680 | Loss: 2.06 | Tokens per second: 3676.17 | Training tokens per second (%): 3.73 | MFU (%): 19.16 | TFLOPs: 189.47 | Global batch size: 12 | Global tokens/sec: 44114.09 | Global MFU (%): 19.16 | Global TFLOPs: 2273.69 | 
2025-05-18 21:03:53,983 - root - INFO - Step: 690 | Loss: 2.36 | Tokens per second: 3674.34 | Training tokens per second (%): 2.85 | MFU (%): 19.15 | TFLOPs: 189.38 | Global batch size: 12 | Global tokens/sec: 44092.02 | Global MFU (%): 19.15 | Global TFLOPs: 2272.56 | 
2025-05-18 21:03:59,547 - root - INFO - Step: 700 | Loss: 2.08 | Tokens per second: 3681.48 | Training tokens per second (%): 3.27 | MFU (%): 19.19 | TFLOPs: 189.75 | Global batch size: 12 | Global tokens/sec: 44177.71 | Global MFU (%): 19.19 | Global TFLOPs: 2276.97 | 
2025-05-18 21:04:05,108 - root - INFO - Step: 710 | Loss: 2.36 | Tokens per second: 3683.61 | Training tokens per second (%): 2.50 | MFU (%): 19.20 | TFLOPs: 189.86 | Global batch size: 12 | Global tokens/sec: 44203.33 | Global MFU (%): 19.20 | Global TFLOPs: 2278.29 | 
2025-05-18 21:04:10,661 - root - INFO - Step: 720 | Loss: 2.07 | Tokens per second: 3688.54 | Training tokens per second (%): 2.84 | MFU (%): 19.22 | TFLOPs: 190.11 | Global batch size: 12 | Global tokens/sec: 44262.45 | Global MFU (%): 19.22 | Global TFLOPs: 2281.34 | 
2025-05-18 21:04:16,233 - root - INFO - Step: 730 | Loss: 2.61 | Tokens per second: 3676.32 | Training tokens per second (%): 3.86 | MFU (%): 19.16 | TFLOPs: 189.48 | Global batch size: 12 | Global tokens/sec: 44115.80 | Global MFU (%): 19.16 | Global TFLOPs: 2273.78 | 
2025-05-18 21:04:21,810 - root - INFO - Step: 740 | Loss: 2.54 | Tokens per second: 3672.75 | Training tokens per second (%): 3.87 | MFU (%): 19.14 | TFLOPs: 189.30 | Global batch size: 12 | Global tokens/sec: 44073.03 | Global MFU (%): 19.14 | Global TFLOPs: 2271.58 | 
2025-05-18 21:04:27,395 - root - INFO - Step: 750 | Loss: 2.28 | Tokens per second: 3667.07 | Training tokens per second (%): 3.15 | MFU (%): 19.11 | TFLOPs: 189.01 | Global batch size: 12 | Global tokens/sec: 44004.80 | Global MFU (%): 19.11 | Global TFLOPs: 2268.06 | 
2025-05-18 21:04:32,973 - root - INFO - Step: 760 | Loss: 1.69 | Tokens per second: 3672.75 | Training tokens per second (%): 4.14 | MFU (%): 19.14 | TFLOPs: 189.30 | Global batch size: 12 | Global tokens/sec: 44073.05 | Global MFU (%): 19.14 | Global TFLOPs: 2271.58 | 
2025-05-18 21:04:38,549 - root - INFO - Step: 770 | Loss: 1.94 | Tokens per second: 3673.27 | Training tokens per second (%): 3.77 | MFU (%): 19.14 | TFLOPs: 189.32 | Global batch size: 12 | Global tokens/sec: 44079.22 | Global MFU (%): 19.14 | Global TFLOPs: 2271.90 | 
2025-05-18 21:04:44,136 - root - INFO - Step: 780 | Loss: 1.56 | Tokens per second: 3666.37 | Training tokens per second (%): 2.44 | MFU (%): 19.11 | TFLOPs: 188.97 | Global batch size: 12 | Global tokens/sec: 43996.42 | Global MFU (%): 19.11 | Global TFLOPs: 2267.63 | 
2025-05-18 21:04:49,683 - root - INFO - Step: 790 | Loss: 1.64 | Tokens per second: 3692.76 | Training tokens per second (%): 1.21 | MFU (%): 19.24 | TFLOPs: 190.33 | Global batch size: 12 | Global tokens/sec: 44313.06 | Global MFU (%): 19.24 | Global TFLOPs: 2283.95 | 
2025-05-18 21:04:55,248 - root - INFO - Step: 800 | Loss: 2.05 | Tokens per second: 3680.52 | Training tokens per second (%): 4.14 | MFU (%): 19.18 | TFLOPs: 189.70 | Global batch size: 12 | Global tokens/sec: 44166.18 | Global MFU (%): 19.18 | Global TFLOPs: 2276.38 | 
2025-05-18 21:05:00,828 - root - INFO - Step: 810 | Loss: 1.92 | Tokens per second: 3670.99 | Training tokens per second (%): 2.61 | MFU (%): 19.13 | TFLOPs: 189.21 | Global batch size: 12 | Global tokens/sec: 44051.92 | Global MFU (%): 19.13 | Global TFLOPs: 2270.49 | 
2025-05-18 21:05:06,414 - root - INFO - Step: 820 | Loss: 2.42 | Tokens per second: 3666.55 | Training tokens per second (%): 3.91 | MFU (%): 19.11 | TFLOPs: 188.98 | Global batch size: 12 | Global tokens/sec: 43998.59 | Global MFU (%): 19.11 | Global TFLOPs: 2267.74 | 
2025-05-18 21:05:11,980 - root - INFO - Step: 830 | Loss: 2.17 | Tokens per second: 3680.30 | Training tokens per second (%): 3.68 | MFU (%): 19.18 | TFLOPs: 189.69 | Global batch size: 12 | Global tokens/sec: 44163.60 | Global MFU (%): 19.18 | Global TFLOPs: 2276.25 | 
2025-05-18 21:05:17,570 - root - INFO - Step: 840 | Loss: 2.00 | Tokens per second: 3664.25 | Training tokens per second (%): 4.46 | MFU (%): 19.10 | TFLOPs: 188.86 | Global batch size: 12 | Global tokens/sec: 43970.94 | Global MFU (%): 19.10 | Global TFLOPs: 2266.32 | 
2025-05-18 21:05:23,142 - root - INFO - Step: 850 | Loss: 1.13 | Tokens per second: 3676.18 | Training tokens per second (%): 1.50 | MFU (%): 19.16 | TFLOPs: 189.47 | Global batch size: 12 | Global tokens/sec: 44114.10 | Global MFU (%): 19.16 | Global TFLOPs: 2273.69 | 
2025-05-18 21:05:28,702 - root - INFO - Step: 860 | Loss: 1.22 | Tokens per second: 3683.99 | Training tokens per second (%): 3.68 | MFU (%): 19.20 | TFLOPs: 189.88 | Global batch size: 12 | Global tokens/sec: 44207.84 | Global MFU (%): 19.20 | Global TFLOPs: 2278.53 | 
2025-05-18 21:05:34,287 - root - INFO - Step: 870 | Loss: 1.44 | Tokens per second: 3667.39 | Training tokens per second (%): 2.78 | MFU (%): 19.11 | TFLOPs: 189.02 | Global batch size: 12 | Global tokens/sec: 44008.66 | Global MFU (%): 19.11 | Global TFLOPs: 2268.26 | 
2025-05-18 21:05:39,856 - root - INFO - Step: 880 | Loss: 1.30 | Tokens per second: 3678.52 | Training tokens per second (%): 3.51 | MFU (%): 19.17 | TFLOPs: 189.60 | Global batch size: 12 | Global tokens/sec: 44142.18 | Global MFU (%): 19.17 | Global TFLOPs: 2275.14 | 
2025-05-18 21:05:45,429 - root - INFO - Step: 890 | Loss: 1.36 | Tokens per second: 3675.15 | Training tokens per second (%): 4.76 | MFU (%): 19.15 | TFLOPs: 189.42 | Global batch size: 12 | Global tokens/sec: 44101.82 | Global MFU (%): 19.15 | Global TFLOPs: 2273.06 | 
2025-05-18 21:05:51,001 - root - INFO - Step: 900 | Loss: 1.57 | Tokens per second: 3676.59 | Training tokens per second (%): 3.70 | MFU (%): 19.16 | TFLOPs: 189.50 | Global batch size: 12 | Global tokens/sec: 44119.12 | Global MFU (%): 19.16 | Global TFLOPs: 2273.95 | 
2025-05-18 21:05:56,579 - root - INFO - Step: 910 | Loss: 1.51 | Tokens per second: 3672.11 | Training tokens per second (%): 3.16 | MFU (%): 19.14 | TFLOPs: 189.27 | Global batch size: 12 | Global tokens/sec: 44065.36 | Global MFU (%): 19.14 | Global TFLOPs: 2271.18 | 
2025-05-18 21:06:02,142 - root - INFO - Step: 920 | Loss: 1.45 | Tokens per second: 3681.64 | Training tokens per second (%): 3.48 | MFU (%): 19.19 | TFLOPs: 189.76 | Global batch size: 12 | Global tokens/sec: 44179.73 | Global MFU (%): 19.19 | Global TFLOPs: 2277.08 | 
2025-05-18 21:06:07,731 - root - INFO - Step: 930 | Loss: 0.89 | Tokens per second: 3665.49 | Training tokens per second (%): 3.23 | MFU (%): 19.10 | TFLOPs: 188.92 | Global batch size: 12 | Global tokens/sec: 43985.93 | Global MFU (%): 19.10 | Global TFLOPs: 2267.09 | 
2025-05-18 21:06:13,313 - root - INFO - Step: 940 | Loss: 1.66 | Tokens per second: 3668.95 | Training tokens per second (%): 3.08 | MFU (%): 19.12 | TFLOPs: 189.10 | Global batch size: 12 | Global tokens/sec: 44027.39 | Global MFU (%): 19.12 | Global TFLOPs: 2269.22 | 
2025-05-18 21:06:18,869 - root - INFO - Step: 950 | Loss: 0.76 | Tokens per second: 3686.98 | Training tokens per second (%): 2.72 | MFU (%): 19.21 | TFLOPs: 190.03 | Global batch size: 12 | Global tokens/sec: 44243.80 | Global MFU (%): 19.21 | Global TFLOPs: 2280.38 | 
2025-05-18 21:06:24,445 - root - INFO - Step: 960 | Loss: 1.10 | Tokens per second: 3673.84 | Training tokens per second (%): 4.05 | MFU (%): 19.15 | TFLOPs: 189.35 | Global batch size: 12 | Global tokens/sec: 44086.03 | Global MFU (%): 19.15 | Global TFLOPs: 2272.25 | 
2025-05-18 21:06:30,012 - root - INFO - Step: 970 | Loss: 0.59 | Tokens per second: 3678.91 | Training tokens per second (%): 2.27 | MFU (%): 19.17 | TFLOPs: 189.62 | Global batch size: 12 | Global tokens/sec: 44146.95 | Global MFU (%): 19.17 | Global TFLOPs: 2275.39 | 
2025-05-18 21:06:35,586 - root - INFO - Step: 980 | Loss: 0.88 | Tokens per second: 3674.94 | Training tokens per second (%): 3.44 | MFU (%): 19.15 | TFLOPs: 189.41 | Global batch size: 12 | Global tokens/sec: 44099.24 | Global MFU (%): 19.15 | Global TFLOPs: 2272.93 | 
2025-05-18 21:06:41,158 - root - INFO - Step: 990 | Loss: 0.78 | Tokens per second: 3676.25 | Training tokens per second (%): 2.49 | MFU (%): 19.16 | TFLOPs: 189.48 | Global batch size: 12 | Global tokens/sec: 44115.05 | Global MFU (%): 19.16 | Global TFLOPs: 2273.74 | 
2025-05-18 21:06:46,721 - root - INFO - Step: 1000 | Loss: 0.82 | Tokens per second: 3682.07 | Training tokens per second (%): 3.57 | MFU (%): 19.19 | TFLOPs: 189.78 | Global batch size: 12 | Global tokens/sec: 44184.86 | Global MFU (%): 19.19 | Global TFLOPs: 2277.34 | 
2025-05-18 21:06:46,721 - root - INFO - Training completed
[sbatch-master] task finished
