[sbatch-master] running on nid006596
[sbatch-master] SLURM_NODELIST: nid[006596,006600]
[sbatch-master] SLURM_NNODES: 2
[sbatch-master] SLURM_NODEID: 0
[Master] Master node: nid006596
[Master] World size: 8
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
[srun] rank=0 host=nid006596 noderank=0 localrank=0
[srun] rank=1 host=nid006600 noderank=1 localrank=0
W0519 09:03:10.603000 213517 torch/distributed/run.py:792] 
W0519 09:03:10.603000 213517 torch/distributed/run.py:792] *****************************************
W0519 09:03:10.603000 213517 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0519 09:03:10.603000 213517 torch/distributed/run.py:792] *****************************************
W0519 09:03:10.613000 209308 torch/distributed/run.py:792] 
W0519 09:03:10.613000 209308 torch/distributed/run.py:792] *****************************************
W0519 09:03:10.613000 209308 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0519 09:03:10.613000 209308 torch/distributed/run.py:792] *****************************************
2025-05-19 09:03:22,815 - root - INFO - [Distributed Init] Rank 4 initialized on node 1 on GPU 0.
2025-05-19 09:03:23,011 - root - INFO - [Distributed Init] Rank 0 initialized on node 0 on GPU 0.
[rank4]:[W519 09:03:23.645036777 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 09:03:23,348 - root - INFO - [Distributed Init] Rank 7 initialized on node 1 on GPU 3.
[rank7]:[W519 09:03:23.762331431 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 7]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 09:03:23,408 - root - INFO - [Distributed Init] Rank 6 initialized on node 1 on GPU 2.
[rank6]:[W519 09:03:23.822030856 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 6]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 09:03:23,418 - root - INFO - [Distributed Init] Rank 5 initialized on node 1 on GPU 1.
[rank5]:[W519 09:03:23.832059281 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 5]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank0]:[W519 09:03:23.349318300 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 09:03:23,480 - root - INFO - [Distributed Init] Rank 1 initialized on node 0 on GPU 1.
2025-05-19 09:03:23,480 - root - INFO - [Distributed Init] Rank 2 initialized on node 0 on GPU 2.
2025-05-19 09:03:23,480 - root - INFO - [Distributed Init] Rank 3 initialized on node 0 on GPU 3.
[rank2]:[W519 09:03:23.403469473 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W519 09:03:23.403470401 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W519 09:03:23.403478529 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-19 09:03:28,473 - root - INFO - [Rank 0] All ranks ready!
2025-05-19 09:03:28,473 - root - INFO - Distributed training enabled: 8 processes
2025-05-19 09:03:28,473 - root - INFO - Master process: 0 on cuda:0
2025-05-19 09:03:28,473 - root - INFO - Experiment args: Namespace(dataset='/capstor/scratch/cscs/kasparr/project/train_data_tokenized_padded_snappy.parquet', dataset_type='padded', pretokenized=True, tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=10, profile=False, profile_step_start=10, profile_step_end=10, grad_max_norm=1, model_dtype='bf16', compile=False, distributed=True, backend='nccl', find_unused_parameters=False)
2025-05-19 09:03:28,473 - root - INFO - Setting up Tokenizer...
2025-05-19 09:03:29,034 - root - INFO - Setting up DataLoaders...
2025-05-19 09:03:29,034 - root - INFO - Using pretokenized data: /capstor/scratch/cscs/kasparr/project/train_data_tokenized_padded_snappy.parquet
Loaded pretokenized dataset with 785906 samplesLoaded pretokenized dataset with 785906 samples

Loaded pretokenized dataset with 785906 samples
Loaded pretokenized dataset with 785906 samples
Loaded pretokenized dataset with 785906 samples
Loaded pretokenized dataset with 785906 samples
Loaded pretokenized dataset with 785906 samples
2025-05-19 09:04:01,319 - root - INFO - Setting up Model...
Loaded pretokenized dataset with 785906 samples
2025-05-19 09:04:10,208 - root - INFO - Model wrapped with DistributedDataParallel
2025-05-19 09:04:10,209 - root - INFO - Global batch size: 64 (local: 8 Ã— 8 processes)
2025-05-19 09:04:10,209 - root - INFO - Starting training!
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
2025-05-19 09:04:11,785 - root - INFO - Step: 1 | Loss: 11.96 | Tokens per second: 10403.68 | Training tokens per second (%): 3.37 | MFU (%): 9.74 | TFLOPs: 96.35 | Global batch size: 64 | Global tokens/sec: 83229.41 | Global MFU (%): 9.74 | Global TFLOPs: 770.82 | 
2025-05-19 09:04:15,914 - root - INFO - Step: 10 | Loss: 11.88 | Tokens per second: 35717.68 | Training tokens per second (%): 4.30 | MFU (%): 33.45 | TFLOPs: 330.80 | Global batch size: 64 | Global tokens/sec: 285741.46 | Global MFU (%): 33.45 | Global TFLOPs: 2646.37 | 
2025-05-19 09:04:20,098 - root - INFO - Step: 20 | Loss: 11.39 | Tokens per second: 39165.07 | Training tokens per second (%): 4.30 | MFU (%): 36.68 | TFLOPs: 362.72 | Global batch size: 64 | Global tokens/sec: 313320.57 | Global MFU (%): 36.68 | Global TFLOPs: 2901.80 | 
2025-05-19 09:04:24,391 - root - INFO - Step: 30 | Loss: 10.73 | Tokens per second: 38171.92 | Training tokens per second (%): 4.73 | MFU (%): 35.75 | TFLOPs: 353.53 | Global batch size: 64 | Global tokens/sec: 305375.37 | Global MFU (%): 35.75 | Global TFLOPs: 2828.21 | 
2025-05-19 09:04:28,569 - root - INFO - Step: 40 | Loss: 10.26 | Tokens per second: 39222.62 | Training tokens per second (%): 5.06 | MFU (%): 36.73 | TFLOPs: 363.26 | Global batch size: 64 | Global tokens/sec: 313780.96 | Global MFU (%): 36.73 | Global TFLOPs: 2906.06 | 
2025-05-19 09:04:32,742 - root - INFO - Step: 50 | Loss: 9.62 | Tokens per second: 39269.12 | Training tokens per second (%): 5.21 | MFU (%): 36.77 | TFLOPs: 363.69 | Global batch size: 64 | Global tokens/sec: 314152.99 | Global MFU (%): 36.77 | Global TFLOPs: 2909.51 | 
2025-05-19 09:04:36,943 - root - INFO - Step: 60 | Loss: 9.07 | Tokens per second: 39003.40 | Training tokens per second (%): 4.69 | MFU (%): 36.52 | TFLOPs: 361.23 | Global batch size: 64 | Global tokens/sec: 312027.22 | Global MFU (%): 36.52 | Global TFLOPs: 2889.82 | 
2025-05-19 09:04:41,113 - root - INFO - Step: 70 | Loss: 8.59 | Tokens per second: 39301.30 | Training tokens per second (%): 4.04 | MFU (%): 36.80 | TFLOPs: 363.99 | Global batch size: 64 | Global tokens/sec: 314410.42 | Global MFU (%): 36.80 | Global TFLOPs: 2911.89 | 
2025-05-19 09:04:45,281 - root - INFO - Step: 80 | Loss: 8.14 | Tokens per second: 39313.82 | Training tokens per second (%): 4.61 | MFU (%): 36.82 | TFLOPs: 364.10 | Global batch size: 64 | Global tokens/sec: 314510.59 | Global MFU (%): 36.82 | Global TFLOPs: 2912.82 | 
2025-05-19 09:04:49,478 - root - INFO - Step: 90 | Loss: 7.76 | Tokens per second: 39040.36 | Training tokens per second (%): 4.75 | MFU (%): 36.56 | TFLOPs: 361.57 | Global batch size: 64 | Global tokens/sec: 312322.89 | Global MFU (%): 36.56 | Global TFLOPs: 2892.56 | 
2025-05-19 09:04:53,650 - root - INFO - Step: 100 | Loss: 7.59 | Tokens per second: 39282.15 | Training tokens per second (%): 4.16 | MFU (%): 36.79 | TFLOPs: 363.81 | Global batch size: 64 | Global tokens/sec: 314257.19 | Global MFU (%): 36.79 | Global TFLOPs: 2910.47 | 
2025-05-19 09:04:57,855 - root - INFO - Step: 110 | Loss: 7.33 | Tokens per second: 38971.83 | Training tokens per second (%): 4.73 | MFU (%): 36.49 | TFLOPs: 360.93 | Global batch size: 64 | Global tokens/sec: 311774.66 | Global MFU (%): 36.49 | Global TFLOPs: 2887.48 | 
2025-05-19 09:05:02,039 - root - INFO - Step: 120 | Loss: 7.29 | Tokens per second: 39165.93 | Training tokens per second (%): 4.71 | MFU (%): 36.68 | TFLOPs: 362.73 | Global batch size: 64 | Global tokens/sec: 313327.41 | Global MFU (%): 36.68 | Global TFLOPs: 2901.86 | 
2025-05-19 09:05:06,243 - root - INFO - Step: 130 | Loss: 7.21 | Tokens per second: 38974.04 | Training tokens per second (%): 4.62 | MFU (%): 36.50 | TFLOPs: 360.96 | Global batch size: 64 | Global tokens/sec: 311792.36 | Global MFU (%): 36.50 | Global TFLOPs: 2887.64 | 
2025-05-19 09:05:10,437 - root - INFO - Step: 140 | Loss: 7.15 | Tokens per second: 39076.37 | Training tokens per second (%): 5.29 | MFU (%): 36.59 | TFLOPs: 361.90 | Global batch size: 64 | Global tokens/sec: 312610.93 | Global MFU (%): 36.59 | Global TFLOPs: 2895.22 | 
2025-05-19 09:05:14,623 - root - INFO - Step: 150 | Loss: 7.08 | Tokens per second: 39145.46 | Training tokens per second (%): 4.45 | MFU (%): 36.66 | TFLOPs: 362.54 | Global batch size: 64 | Global tokens/sec: 313163.70 | Global MFU (%): 36.66 | Global TFLOPs: 2900.34 | 
2025-05-19 09:05:18,796 - root - INFO - Step: 160 | Loss: 7.08 | Tokens per second: 39264.55 | Training tokens per second (%): 5.22 | MFU (%): 36.77 | TFLOPs: 363.65 | Global batch size: 64 | Global tokens/sec: 314116.36 | Global MFU (%): 36.77 | Global TFLOPs: 2909.17 | 
2025-05-19 09:05:22,964 - root - INFO - Step: 170 | Loss: 7.00 | Tokens per second: 39319.74 | Training tokens per second (%): 4.73 | MFU (%): 36.82 | TFLOPs: 364.16 | Global batch size: 64 | Global tokens/sec: 314557.92 | Global MFU (%): 36.82 | Global TFLOPs: 2913.26 | 
2025-05-19 09:05:27,138 - root - INFO - Step: 180 | Loss: 6.91 | Tokens per second: 39258.14 | Training tokens per second (%): 4.50 | MFU (%): 36.76 | TFLOPs: 363.59 | Global batch size: 64 | Global tokens/sec: 314065.15 | Global MFU (%): 36.76 | Global TFLOPs: 2908.69 | 
2025-05-19 09:05:31,315 - root - INFO - Step: 190 | Loss: 6.87 | Tokens per second: 39232.18 | Training tokens per second (%): 4.12 | MFU (%): 36.74 | TFLOPs: 363.35 | Global batch size: 64 | Global tokens/sec: 313857.44 | Global MFU (%): 36.74 | Global TFLOPs: 2906.77 | 
2025-05-19 09:05:35,498 - root - INFO - Step: 200 | Loss: 6.85 | Tokens per second: 39174.77 | Training tokens per second (%): 4.74 | MFU (%): 36.68 | TFLOPs: 362.81 | Global batch size: 64 | Global tokens/sec: 313398.13 | Global MFU (%): 36.68 | Global TFLOPs: 2902.51 | 
2025-05-19 09:05:39,687 - root - INFO - Step: 210 | Loss: 6.82 | Tokens per second: 39126.60 | Training tokens per second (%): 4.15 | MFU (%): 36.64 | TFLOPs: 362.37 | Global batch size: 64 | Global tokens/sec: 313012.82 | Global MFU (%): 36.64 | Global TFLOPs: 2898.95 | 
2025-05-19 09:05:43,849 - root - INFO - Step: 220 | Loss: 6.92 | Tokens per second: 39371.18 | Training tokens per second (%): 4.58 | MFU (%): 36.87 | TFLOPs: 364.63 | Global batch size: 64 | Global tokens/sec: 314969.40 | Global MFU (%): 36.87 | Global TFLOPs: 2917.07 | 
2025-05-19 09:05:48,022 - root - INFO - Step: 230 | Loss: 6.83 | Tokens per second: 39267.54 | Training tokens per second (%): 5.11 | MFU (%): 36.77 | TFLOPs: 363.67 | Global batch size: 64 | Global tokens/sec: 314140.35 | Global MFU (%): 36.77 | Global TFLOPs: 2909.39 | 
2025-05-19 09:05:52,198 - root - INFO - Step: 240 | Loss: 6.69 | Tokens per second: 39244.75 | Training tokens per second (%): 5.21 | MFU (%): 36.75 | TFLOPs: 363.46 | Global batch size: 64 | Global tokens/sec: 313958.02 | Global MFU (%): 36.75 | Global TFLOPs: 2907.70 | 
2025-05-19 09:05:56,363 - root - INFO - Step: 250 | Loss: 6.64 | Tokens per second: 39343.18 | Training tokens per second (%): 4.82 | MFU (%): 36.84 | TFLOPs: 364.37 | Global batch size: 64 | Global tokens/sec: 314745.46 | Global MFU (%): 36.84 | Global TFLOPs: 2914.99 | 
2025-05-19 09:06:00,545 - root - INFO - Step: 260 | Loss: 6.66 | Tokens per second: 39185.22 | Training tokens per second (%): 4.62 | MFU (%): 36.69 | TFLOPs: 362.91 | Global batch size: 64 | Global tokens/sec: 313481.75 | Global MFU (%): 36.69 | Global TFLOPs: 2903.29 | 
2025-05-19 09:06:04,710 - root - INFO - Step: 270 | Loss: 6.67 | Tokens per second: 39342.55 | Training tokens per second (%): 5.07 | MFU (%): 36.84 | TFLOPs: 364.37 | Global batch size: 64 | Global tokens/sec: 314740.43 | Global MFU (%): 36.84 | Global TFLOPs: 2914.95 | 
2025-05-19 09:06:08,893 - root - INFO - Step: 280 | Loss: 6.64 | Tokens per second: 39176.03 | Training tokens per second (%): 4.41 | MFU (%): 36.69 | TFLOPs: 362.83 | Global batch size: 64 | Global tokens/sec: 313408.28 | Global MFU (%): 36.69 | Global TFLOPs: 2902.61 | 
2025-05-19 09:06:13,074 - root - INFO - Step: 290 | Loss: 6.45 | Tokens per second: 39203.62 | Training tokens per second (%): 4.99 | MFU (%): 36.71 | TFLOPs: 363.08 | Global batch size: 64 | Global tokens/sec: 313628.97 | Global MFU (%): 36.71 | Global TFLOPs: 2904.65 | 
2025-05-19 09:06:17,243 - root - INFO - Step: 300 | Loss: 6.54 | Tokens per second: 39300.45 | Training tokens per second (%): 4.92 | MFU (%): 36.80 | TFLOPs: 363.98 | Global batch size: 64 | Global tokens/sec: 314403.64 | Global MFU (%): 36.80 | Global TFLOPs: 2911.83 | 
2025-05-19 09:06:21,422 - root - INFO - Step: 310 | Loss: 6.49 | Tokens per second: 39221.07 | Training tokens per second (%): 4.68 | MFU (%): 36.73 | TFLOPs: 363.24 | Global batch size: 64 | Global tokens/sec: 313768.54 | Global MFU (%): 36.73 | Global TFLOPs: 2905.94 | 
2025-05-19 09:06:25,617 - root - INFO - Step: 320 | Loss: 6.46 | Tokens per second: 39060.28 | Training tokens per second (%): 5.02 | MFU (%): 36.58 | TFLOPs: 361.75 | Global batch size: 64 | Global tokens/sec: 312482.28 | Global MFU (%): 36.58 | Global TFLOPs: 2894.03 | 
2025-05-19 09:06:29,790 - root - INFO - Step: 330 | Loss: 6.46 | Tokens per second: 39275.01 | Training tokens per second (%): 4.87 | MFU (%): 36.78 | TFLOPs: 363.74 | Global batch size: 64 | Global tokens/sec: 314200.12 | Global MFU (%): 36.78 | Global TFLOPs: 2909.94 | 
2025-05-19 09:06:33,956 - root - INFO - Step: 340 | Loss: 6.35 | Tokens per second: 39328.11 | Training tokens per second (%): 4.54 | MFU (%): 36.83 | TFLOPs: 364.23 | Global batch size: 64 | Global tokens/sec: 314624.88 | Global MFU (%): 36.83 | Global TFLOPs: 2913.88 | 
2025-05-19 09:06:38,223 - root - INFO - Step: 350 | Loss: 6.38 | Tokens per second: 38414.14 | Training tokens per second (%): 5.11 | MFU (%): 35.97 | TFLOPs: 355.77 | Global batch size: 64 | Global tokens/sec: 307313.13 | Global MFU (%): 35.97 | Global TFLOPs: 2846.16 | 
2025-05-19 09:06:42,406 - root - INFO - Step: 360 | Loss: 6.41 | Tokens per second: 39170.66 | Training tokens per second (%): 4.42 | MFU (%): 36.68 | TFLOPs: 362.78 | Global batch size: 64 | Global tokens/sec: 313365.30 | Global MFU (%): 36.68 | Global TFLOPs: 2902.21 | 
2025-05-19 09:06:46,580 - root - INFO - Step: 370 | Loss: 6.33 | Tokens per second: 39266.60 | Training tokens per second (%): 4.48 | MFU (%): 36.77 | TFLOPs: 363.66 | Global batch size: 64 | Global tokens/sec: 314132.77 | Global MFU (%): 36.77 | Global TFLOPs: 2909.32 | 
2025-05-19 09:06:50,749 - root - INFO - Step: 380 | Loss: 6.22 | Tokens per second: 39303.31 | Training tokens per second (%): 4.55 | MFU (%): 36.81 | TFLOPs: 364.00 | Global batch size: 64 | Global tokens/sec: 314426.49 | Global MFU (%): 36.81 | Global TFLOPs: 2912.04 | 
2025-05-19 09:06:54,922 - root - INFO - Step: 390 | Loss: 6.25 | Tokens per second: 39275.28 | Training tokens per second (%): 4.86 | MFU (%): 36.78 | TFLOPs: 363.75 | Global batch size: 64 | Global tokens/sec: 314202.22 | Global MFU (%): 36.78 | Global TFLOPs: 2909.96 | 
2025-05-19 09:06:59,099 - root - INFO - Step: 400 | Loss: 6.17 | Tokens per second: 39231.47 | Training tokens per second (%): 5.22 | MFU (%): 36.74 | TFLOPs: 363.34 | Global batch size: 64 | Global tokens/sec: 313851.79 | Global MFU (%): 36.74 | Global TFLOPs: 2906.72 | 
2025-05-19 09:07:03,272 - root - INFO - Step: 410 | Loss: 6.08 | Tokens per second: 39270.37 | Training tokens per second (%): 4.86 | MFU (%): 36.77 | TFLOPs: 363.70 | Global batch size: 64 | Global tokens/sec: 314162.93 | Global MFU (%): 36.77 | Global TFLOPs: 2909.60 | 
2025-05-19 09:07:07,478 - root - INFO - Step: 420 | Loss: 6.17 | Tokens per second: 38961.82 | Training tokens per second (%): 5.45 | MFU (%): 36.49 | TFLOPs: 360.84 | Global batch size: 64 | Global tokens/sec: 311694.52 | Global MFU (%): 36.49 | Global TFLOPs: 2886.74 | 
2025-05-19 09:07:11,667 - root - INFO - Step: 430 | Loss: 6.16 | Tokens per second: 39121.69 | Training tokens per second (%): 4.96 | MFU (%): 36.64 | TFLOPs: 362.32 | Global batch size: 64 | Global tokens/sec: 312973.53 | Global MFU (%): 36.64 | Global TFLOPs: 2898.58 | 
2025-05-19 09:07:15,837 - root - INFO - Step: 440 | Loss: 6.19 | Tokens per second: 39294.32 | Training tokens per second (%): 5.34 | MFU (%): 36.80 | TFLOPs: 363.92 | Global batch size: 64 | Global tokens/sec: 314354.55 | Global MFU (%): 36.80 | Global TFLOPs: 2911.37 | 
2025-05-19 09:07:20,016 - root - INFO - Step: 450 | Loss: 6.15 | Tokens per second: 39217.67 | Training tokens per second (%): 4.34 | MFU (%): 36.73 | TFLOPs: 363.21 | Global batch size: 64 | Global tokens/sec: 313741.33 | Global MFU (%): 36.73 | Global TFLOPs: 2905.69 | 
2025-05-19 09:07:24,200 - root - INFO - Step: 460 | Loss: 6.20 | Tokens per second: 39163.55 | Training tokens per second (%): 4.69 | MFU (%): 36.67 | TFLOPs: 362.71 | Global batch size: 64 | Global tokens/sec: 313308.38 | Global MFU (%): 36.67 | Global TFLOPs: 2901.68 | 
2025-05-19 09:07:28,370 - root - INFO - Step: 470 | Loss: 6.14 | Tokens per second: 39298.25 | Training tokens per second (%): 4.95 | MFU (%): 36.80 | TFLOPs: 363.96 | Global batch size: 64 | Global tokens/sec: 314386.03 | Global MFU (%): 36.80 | Global TFLOPs: 2911.66 | 
2025-05-19 09:07:32,544 - root - INFO - Step: 480 | Loss: 6.14 | Tokens per second: 39262.89 | Training tokens per second (%): 4.81 | MFU (%): 36.77 | TFLOPs: 363.63 | Global batch size: 64 | Global tokens/sec: 314103.10 | Global MFU (%): 36.77 | Global TFLOPs: 2909.04 | 
2025-05-19 09:07:36,730 - root - INFO - Step: 490 | Loss: 6.14 | Tokens per second: 39143.62 | Training tokens per second (%): 5.47 | MFU (%): 36.66 | TFLOPs: 362.53 | Global batch size: 64 | Global tokens/sec: 313148.95 | Global MFU (%): 36.66 | Global TFLOPs: 2900.21 | 
2025-05-19 09:07:40,918 - root - INFO - Step: 500 | Loss: 6.09 | Tokens per second: 39133.74 | Training tokens per second (%): 5.17 | MFU (%): 36.65 | TFLOPs: 362.43 | Global batch size: 64 | Global tokens/sec: 313069.93 | Global MFU (%): 36.65 | Global TFLOPs: 2899.47 | 
2025-05-19 09:07:45,102 - root - INFO - Step: 510 | Loss: 6.04 | Tokens per second: 39164.09 | Training tokens per second (%): 5.14 | MFU (%): 36.67 | TFLOPs: 362.72 | Global batch size: 64 | Global tokens/sec: 313312.73 | Global MFU (%): 36.67 | Global TFLOPs: 2901.72 | 
2025-05-19 09:07:49,269 - root - INFO - Step: 520 | Loss: 6.03 | Tokens per second: 39325.08 | Training tokens per second (%): 4.63 | MFU (%): 36.83 | TFLOPs: 364.21 | Global batch size: 64 | Global tokens/sec: 314600.61 | Global MFU (%): 36.83 | Global TFLOPs: 2913.65 | 
2025-05-19 09:07:53,452 - root - INFO - Step: 530 | Loss: 6.00 | Tokens per second: 39181.74 | Training tokens per second (%): 5.58 | MFU (%): 36.69 | TFLOPs: 362.88 | Global batch size: 64 | Global tokens/sec: 313453.91 | Global MFU (%): 36.69 | Global TFLOPs: 2903.03 | 
2025-05-19 09:07:57,634 - root - INFO - Step: 540 | Loss: 6.08 | Tokens per second: 39180.41 | Training tokens per second (%): 4.63 | MFU (%): 36.69 | TFLOPs: 362.87 | Global batch size: 64 | Global tokens/sec: 313443.27 | Global MFU (%): 36.69 | Global TFLOPs: 2902.93 | 
2025-05-19 09:08:01,817 - root - INFO - Step: 550 | Loss: 5.96 | Tokens per second: 39177.86 | Training tokens per second (%): 5.22 | MFU (%): 36.69 | TFLOPs: 362.84 | Global batch size: 64 | Global tokens/sec: 313422.91 | Global MFU (%): 36.69 | Global TFLOPs: 2902.74 | 
2025-05-19 09:08:05,981 - root - INFO - Step: 560 | Loss: 5.89 | Tokens per second: 39356.18 | Training tokens per second (%): 4.40 | MFU (%): 36.85 | TFLOPs: 364.49 | Global batch size: 64 | Global tokens/sec: 314849.46 | Global MFU (%): 36.85 | Global TFLOPs: 2915.96 | 
2025-05-19 09:08:10,161 - root - INFO - Step: 570 | Loss: 5.94 | Tokens per second: 39204.87 | Training tokens per second (%): 4.26 | MFU (%): 36.71 | TFLOPs: 363.09 | Global batch size: 64 | Global tokens/sec: 313638.92 | Global MFU (%): 36.71 | Global TFLOPs: 2904.74 | 
2025-05-19 09:08:14,331 - root - INFO - Step: 580 | Loss: 5.91 | Tokens per second: 39300.61 | Training tokens per second (%): 4.16 | MFU (%): 36.80 | TFLOPs: 363.98 | Global batch size: 64 | Global tokens/sec: 314404.84 | Global MFU (%): 36.80 | Global TFLOPs: 2911.84 | 
2025-05-19 09:08:18,511 - root - INFO - Step: 590 | Loss: 6.02 | Tokens per second: 39200.50 | Training tokens per second (%): 4.62 | MFU (%): 36.71 | TFLOPs: 363.05 | Global batch size: 64 | Global tokens/sec: 313603.99 | Global MFU (%): 36.71 | Global TFLOPs: 2904.42 | 
2025-05-19 09:08:22,691 - root - INFO - Step: 600 | Loss: 6.03 | Tokens per second: 39206.98 | Training tokens per second (%): 4.91 | MFU (%): 36.72 | TFLOPs: 363.11 | Global batch size: 64 | Global tokens/sec: 313655.86 | Global MFU (%): 36.72 | Global TFLOPs: 2904.90 | 
2025-05-19 09:08:26,869 - root - INFO - Step: 610 | Loss: 5.86 | Tokens per second: 39220.05 | Training tokens per second (%): 4.49 | MFU (%): 36.73 | TFLOPs: 363.23 | Global batch size: 64 | Global tokens/sec: 313760.42 | Global MFU (%): 36.73 | Global TFLOPs: 2905.87 | 
2025-05-19 09:08:31,057 - root - INFO - Step: 620 | Loss: 5.94 | Tokens per second: 39132.06 | Training tokens per second (%): 5.10 | MFU (%): 36.64 | TFLOPs: 362.42 | Global batch size: 64 | Global tokens/sec: 313056.48 | Global MFU (%): 36.64 | Global TFLOPs: 2899.35 | 
2025-05-19 09:08:35,253 - root - INFO - Step: 630 | Loss: 5.78 | Tokens per second: 39052.67 | Training tokens per second (%): 4.80 | MFU (%): 36.57 | TFLOPs: 361.68 | Global batch size: 64 | Global tokens/sec: 312421.38 | Global MFU (%): 36.57 | Global TFLOPs: 2893.47 | 
2025-05-19 09:08:39,442 - root - INFO - Step: 640 | Loss: 5.71 | Tokens per second: 39122.94 | Training tokens per second (%): 5.12 | MFU (%): 36.64 | TFLOPs: 362.33 | Global batch size: 64 | Global tokens/sec: 312983.56 | Global MFU (%): 36.64 | Global TFLOPs: 2898.67 | 
2025-05-19 09:08:43,622 - root - INFO - Step: 650 | Loss: 5.81 | Tokens per second: 39198.99 | Training tokens per second (%): 3.83 | MFU (%): 36.71 | TFLOPs: 363.04 | Global batch size: 64 | Global tokens/sec: 313591.90 | Global MFU (%): 36.71 | Global TFLOPs: 2904.31 | 
2025-05-19 09:08:47,885 - root - INFO - Step: 660 | Loss: 5.82 | Tokens per second: 38440.19 | Training tokens per second (%): 4.36 | MFU (%): 36.00 | TFLOPs: 356.01 | Global batch size: 64 | Global tokens/sec: 307521.51 | Global MFU (%): 36.00 | Global TFLOPs: 2848.09 | 
2025-05-19 09:08:52,063 - root - INFO - Step: 670 | Loss: 5.71 | Tokens per second: 39229.42 | Training tokens per second (%): 4.47 | MFU (%): 36.74 | TFLOPs: 363.32 | Global batch size: 64 | Global tokens/sec: 313835.39 | Global MFU (%): 36.74 | Global TFLOPs: 2906.56 | 
2025-05-19 09:08:56,247 - root - INFO - Step: 680 | Loss: 5.80 | Tokens per second: 39162.22 | Training tokens per second (%): 4.70 | MFU (%): 36.67 | TFLOPs: 362.70 | Global batch size: 64 | Global tokens/sec: 313297.75 | Global MFU (%): 36.67 | Global TFLOPs: 2901.58 | 
2025-05-19 09:09:00,443 - root - INFO - Step: 690 | Loss: 5.74 | Tokens per second: 39055.51 | Training tokens per second (%): 4.72 | MFU (%): 36.57 | TFLOPs: 361.71 | Global batch size: 64 | Global tokens/sec: 312444.11 | Global MFU (%): 36.57 | Global TFLOPs: 2893.68 | 
2025-05-19 09:09:04,618 - root - INFO - Step: 700 | Loss: 5.68 | Tokens per second: 39248.31 | Training tokens per second (%): 5.18 | MFU (%): 36.75 | TFLOPs: 363.50 | Global batch size: 64 | Global tokens/sec: 313986.44 | Global MFU (%): 36.75 | Global TFLOPs: 2907.96 | 
2025-05-19 09:09:08,801 - root - INFO - Step: 710 | Loss: 5.88 | Tokens per second: 39183.27 | Training tokens per second (%): 4.95 | MFU (%): 36.69 | TFLOPs: 362.89 | Global batch size: 64 | Global tokens/sec: 313466.20 | Global MFU (%): 36.69 | Global TFLOPs: 2903.14 | 
2025-05-19 09:09:12,995 - root - INFO - Step: 720 | Loss: 5.77 | Tokens per second: 39073.87 | Training tokens per second (%): 5.16 | MFU (%): 36.59 | TFLOPs: 361.88 | Global batch size: 64 | Global tokens/sec: 312590.98 | Global MFU (%): 36.59 | Global TFLOPs: 2895.04 | 
2025-05-19 09:09:17,178 - root - INFO - Step: 730 | Loss: 5.78 | Tokens per second: 39172.73 | Training tokens per second (%): 5.19 | MFU (%): 36.68 | TFLOPs: 362.80 | Global batch size: 64 | Global tokens/sec: 313381.85 | Global MFU (%): 36.68 | Global TFLOPs: 2902.36 | 
2025-05-19 09:09:21,360 - root - INFO - Step: 740 | Loss: 5.68 | Tokens per second: 39187.73 | Training tokens per second (%): 5.26 | MFU (%): 36.70 | TFLOPs: 362.93 | Global batch size: 64 | Global tokens/sec: 313501.85 | Global MFU (%): 36.70 | Global TFLOPs: 2903.48 | 
2025-05-19 09:09:25,533 - root - INFO - Step: 750 | Loss: 5.79 | Tokens per second: 39263.64 | Training tokens per second (%): 4.92 | MFU (%): 36.77 | TFLOPs: 363.64 | Global batch size: 64 | Global tokens/sec: 314109.13 | Global MFU (%): 36.77 | Global TFLOPs: 2909.10 | 
2025-05-19 09:09:29,715 - root - INFO - Step: 760 | Loss: 5.62 | Tokens per second: 39187.78 | Training tokens per second (%): 4.79 | MFU (%): 36.70 | TFLOPs: 362.93 | Global batch size: 64 | Global tokens/sec: 313502.26 | Global MFU (%): 36.70 | Global TFLOPs: 2903.48 | 
2025-05-19 09:09:33,902 - root - INFO - Step: 770 | Loss: 5.60 | Tokens per second: 39134.41 | Training tokens per second (%): 4.06 | MFU (%): 36.65 | TFLOPs: 362.44 | Global batch size: 64 | Global tokens/sec: 313075.29 | Global MFU (%): 36.65 | Global TFLOPs: 2899.52 | 
2025-05-19 09:09:38,089 - root - INFO - Step: 780 | Loss: 5.68 | Tokens per second: 39136.45 | Training tokens per second (%): 4.71 | MFU (%): 36.65 | TFLOPs: 362.46 | Global batch size: 64 | Global tokens/sec: 313091.56 | Global MFU (%): 36.65 | Global TFLOPs: 2899.68 | 
2025-05-19 09:09:42,271 - root - INFO - Step: 790 | Loss: 5.66 | Tokens per second: 39186.56 | Training tokens per second (%): 4.81 | MFU (%): 36.70 | TFLOPs: 362.92 | Global batch size: 64 | Global tokens/sec: 313492.48 | Global MFU (%): 36.70 | Global TFLOPs: 2903.39 | 
2025-05-19 09:09:46,454 - root - INFO - Step: 800 | Loss: 5.65 | Tokens per second: 39175.29 | Training tokens per second (%): 4.71 | MFU (%): 36.69 | TFLOPs: 362.82 | Global batch size: 64 | Global tokens/sec: 313402.34 | Global MFU (%): 36.69 | Global TFLOPs: 2902.55 | 
2025-05-19 09:09:50,634 - root - INFO - Step: 810 | Loss: 5.60 | Tokens per second: 39207.10 | Training tokens per second (%): 5.28 | MFU (%): 36.72 | TFLOPs: 363.11 | Global batch size: 64 | Global tokens/sec: 313656.83 | Global MFU (%): 36.72 | Global TFLOPs: 2904.91 | 
2025-05-19 09:09:54,808 - root - INFO - Step: 820 | Loss: 5.55 | Tokens per second: 39262.27 | Training tokens per second (%): 4.72 | MFU (%): 36.77 | TFLOPs: 363.62 | Global batch size: 64 | Global tokens/sec: 314098.19 | Global MFU (%): 36.77 | Global TFLOPs: 2909.00 | 
2025-05-19 09:09:58,979 - root - INFO - Step: 830 | Loss: 5.56 | Tokens per second: 39279.97 | Training tokens per second (%): 5.39 | MFU (%): 36.78 | TFLOPs: 363.79 | Global batch size: 64 | Global tokens/sec: 314239.74 | Global MFU (%): 36.78 | Global TFLOPs: 2910.31 | 
2025-05-19 09:10:03,173 - root - INFO - Step: 840 | Loss: 5.56 | Tokens per second: 39077.66 | Training tokens per second (%): 4.82 | MFU (%): 36.59 | TFLOPs: 361.91 | Global batch size: 64 | Global tokens/sec: 312621.27 | Global MFU (%): 36.59 | Global TFLOPs: 2895.32 | 
2025-05-19 09:10:07,374 - root - INFO - Step: 850 | Loss: 5.49 | Tokens per second: 39002.73 | Training tokens per second (%): 5.91 | MFU (%): 36.52 | TFLOPs: 361.22 | Global batch size: 64 | Global tokens/sec: 312021.87 | Global MFU (%): 36.52 | Global TFLOPs: 2889.77 | 
2025-05-19 09:10:11,573 - root - INFO - Step: 860 | Loss: 5.63 | Tokens per second: 39030.85 | Training tokens per second (%): 3.99 | MFU (%): 36.55 | TFLOPs: 361.48 | Global batch size: 64 | Global tokens/sec: 312246.83 | Global MFU (%): 36.55 | Global TFLOPs: 2891.85 | 
2025-05-19 09:10:15,756 - root - INFO - Step: 870 | Loss: 5.57 | Tokens per second: 39171.28 | Training tokens per second (%): 4.48 | MFU (%): 36.68 | TFLOPs: 362.78 | Global batch size: 64 | Global tokens/sec: 313370.24 | Global MFU (%): 36.68 | Global TFLOPs: 2902.26 | 
2025-05-19 09:10:19,946 - root - INFO - Step: 880 | Loss: 5.55 | Tokens per second: 39112.99 | Training tokens per second (%): 4.32 | MFU (%): 36.63 | TFLOPs: 362.24 | Global batch size: 64 | Global tokens/sec: 312903.96 | Global MFU (%): 36.63 | Global TFLOPs: 2897.94 | 
2025-05-19 09:10:24,128 - root - INFO - Step: 890 | Loss: 5.47 | Tokens per second: 39187.59 | Training tokens per second (%): 5.25 | MFU (%): 36.70 | TFLOPs: 362.93 | Global batch size: 64 | Global tokens/sec: 313500.70 | Global MFU (%): 36.70 | Global TFLOPs: 2903.46 | 
2025-05-19 09:10:28,306 - root - INFO - Step: 900 | Loss: 5.54 | Tokens per second: 39219.46 | Training tokens per second (%): 4.48 | MFU (%): 36.73 | TFLOPs: 363.23 | Global batch size: 64 | Global tokens/sec: 313755.68 | Global MFU (%): 36.73 | Global TFLOPs: 2905.83 | 
2025-05-19 09:10:32,498 - root - INFO - Step: 910 | Loss: 5.47 | Tokens per second: 39087.25 | Training tokens per second (%): 5.15 | MFU (%): 36.60 | TFLOPs: 362.00 | Global batch size: 64 | Global tokens/sec: 312698.01 | Global MFU (%): 36.60 | Global TFLOPs: 2896.03 | 
2025-05-19 09:10:36,700 - root - INFO - Step: 920 | Loss: 5.51 | Tokens per second: 39007.48 | Training tokens per second (%): 4.50 | MFU (%): 36.53 | TFLOPs: 361.27 | Global batch size: 64 | Global tokens/sec: 312059.87 | Global MFU (%): 36.53 | Global TFLOPs: 2890.12 | 
2025-05-19 09:10:40,893 - root - INFO - Step: 930 | Loss: 5.53 | Tokens per second: 39077.00 | Training tokens per second (%): 4.56 | MFU (%): 36.59 | TFLOPs: 361.91 | Global batch size: 64 | Global tokens/sec: 312616.01 | Global MFU (%): 36.59 | Global TFLOPs: 2895.27 | 
2025-05-19 09:10:45,080 - root - INFO - Step: 940 | Loss: 5.43 | Tokens per second: 39142.71 | Training tokens per second (%): 4.80 | MFU (%): 36.65 | TFLOPs: 362.52 | Global batch size: 64 | Global tokens/sec: 313141.66 | Global MFU (%): 36.65 | Global TFLOPs: 2900.14 | 
2025-05-19 09:10:49,248 - root - INFO - Step: 950 | Loss: 5.44 | Tokens per second: 39309.30 | Training tokens per second (%): 5.08 | MFU (%): 36.81 | TFLOPs: 364.06 | Global batch size: 64 | Global tokens/sec: 314474.37 | Global MFU (%): 36.81 | Global TFLOPs: 2912.48 | 
2025-05-19 09:10:53,431 - root - INFO - Step: 960 | Loss: 5.45 | Tokens per second: 39180.88 | Training tokens per second (%): 5.28 | MFU (%): 36.69 | TFLOPs: 362.87 | Global batch size: 64 | Global tokens/sec: 313447.04 | Global MFU (%): 36.69 | Global TFLOPs: 2902.97 | 
2025-05-19 09:10:57,626 - root - INFO - Step: 970 | Loss: 5.45 | Tokens per second: 39063.01 | Training tokens per second (%): 4.46 | MFU (%): 36.58 | TFLOPs: 361.78 | Global batch size: 64 | Global tokens/sec: 312504.08 | Global MFU (%): 36.58 | Global TFLOPs: 2894.23 | 
2025-05-19 09:11:01,826 - root - INFO - Step: 980 | Loss: 5.45 | Tokens per second: 39014.16 | Training tokens per second (%): 4.34 | MFU (%): 36.53 | TFLOPs: 361.33 | Global batch size: 64 | Global tokens/sec: 312113.32 | Global MFU (%): 36.53 | Global TFLOPs: 2890.62 | 
2025-05-19 09:11:06,030 - root - INFO - Step: 990 | Loss: 5.41 | Tokens per second: 38981.01 | Training tokens per second (%): 5.28 | MFU (%): 36.50 | TFLOPs: 361.02 | Global batch size: 64 | Global tokens/sec: 311848.08 | Global MFU (%): 36.50 | Global TFLOPs: 2888.16 | 
2025-05-19 09:11:10,232 - root - INFO - Step: 1000 | Loss: 5.49 | Tokens per second: 38994.69 | Training tokens per second (%): 4.76 | MFU (%): 36.52 | TFLOPs: 361.15 | Global batch size: 64 | Global tokens/sec: 311957.54 | Global MFU (%): 36.52 | Global TFLOPs: 2889.17 | 
2025-05-19 09:11:10,232 - root - INFO - Training completed
[sbatch-master] task finished
