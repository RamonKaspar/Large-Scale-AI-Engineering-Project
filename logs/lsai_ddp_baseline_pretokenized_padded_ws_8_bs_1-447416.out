[sbatch-master] running on nid006548
[sbatch-master] SLURM_NODELIST: nid[006548,006552]
[sbatch-master] SLURM_NNODES: 2
[sbatch-master] SLURM_NODEID: 0
[Master] Master node: nid006548
[Master] World size: 8
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
[srun] rank=0 host=nid006548 noderank=0 localrank=0
W0518 22:00:58.174000 288849 torch/distributed/run.py:792] 
W0518 22:00:58.174000 288849 torch/distributed/run.py:792] *****************************************
W0518 22:00:58.174000 288849 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0518 22:00:58.174000 288849 torch/distributed/run.py:792] *****************************************
[srun] rank=1 host=nid006552 noderank=1 localrank=0
W0518 22:01:03.475000 35557 torch/distributed/run.py:792] 
W0518 22:01:03.475000 35557 torch/distributed/run.py:792] *****************************************
W0518 22:01:03.475000 35557 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0518 22:01:03.475000 35557 torch/distributed/run.py:792] *****************************************
2025-05-18 22:01:16,039 - root - INFO - [Distributed Init] Rank 4 initialized on node 1 on GPU 0.
2025-05-18 22:01:16,045 - root - INFO - [Distributed Init] Rank 0 initialized on node 0 on GPU 0.
[rank4]:[W518 22:01:16.999401747 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank0]:[W518 22:01:16.347142172 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 22:01:16,551 - root - INFO - [Distributed Init] Rank 3 initialized on node 0 on GPU 3.
[rank3]:[W518 22:01:16.392434764 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 22:01:16,566 - root - INFO - [Distributed Init] Rank 5 initialized on node 1 on GPU 1.
[rank5]:[W518 22:01:16.072451689 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 5]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 22:01:16,580 - root - INFO - [Distributed Init] Rank 2 initialized on node 0 on GPU 2.
2025-05-18 22:01:16,580 - root - INFO - [Distributed Init] Rank 1 initialized on node 0 on GPU 1.
[rank1]:[W518 22:01:16.422126458 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W518 22:01:16.422126426 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 22:01:16,627 - root - INFO - [Distributed Init] Rank 6 initialized on node 1 on GPU 2.
[rank6]:[W518 22:01:16.132814971 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 6]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 22:01:16,688 - root - INFO - [Distributed Init] Rank 7 initialized on node 1 on GPU 3.
[rank7]:[W518 22:01:16.194037395 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 7]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 22:01:21,604 - root - INFO - [Rank 0] All ranks ready!
2025-05-18 22:01:21,604 - root - INFO - Distributed training enabled: 8 processes
2025-05-18 22:01:21,604 - root - INFO - Master process: 0 on cuda:0
2025-05-18 22:01:21,604 - root - INFO - Experiment args: Namespace(dataset='/capstor/scratch/cscs/kasparr/project/train_data_tokenized_padded_snappy.parquet', dataset_type='padded', pretokenized=True, tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=1, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=10, profile=False, profile_step_start=10, profile_step_end=10, grad_max_norm=1, model_dtype='bf16', compile=False, distributed=True, backend='nccl', find_unused_parameters=False)
2025-05-18 22:01:21,604 - root - INFO - Setting up Tokenizer...
2025-05-18 22:01:22,154 - root - INFO - Setting up DataLoaders...
2025-05-18 22:01:22,154 - root - INFO - Using pretokenized data: /capstor/scratch/cscs/kasparr/project/train_data_tokenized_padded_snappy.parquet
Loaded pretokenized dataset with 785906 samples
Loaded pretokenized dataset with 785906 samples
Loaded pretokenized dataset with 785906 samples
Loaded pretokenized dataset with 785906 samples
Loaded pretokenized dataset with 785906 samples
2025-05-18 22:01:53,274 - root - INFO - Setting up Model...
Loaded pretokenized dataset with 785906 samples
Loaded pretokenized dataset with 785906 samples
Loaded pretokenized dataset with 785906 samples
2025-05-18 22:02:29,507 - root - INFO - Model wrapped with DistributedDataParallel
2025-05-18 22:02:29,509 - root - INFO - Global batch size: 8 (local: 1 Ã— 8 processes)
2025-05-18 22:02:29,510 - root - INFO - Starting training!
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
2025-05-18 22:02:31,259 - root - INFO - Step: 1 | Loss: 12.05 | Tokens per second: 1171.11 | Training tokens per second (%): 6.13 | MFU (%): 6.10 | TFLOPs: 60.36 | Global batch size: 8 | Global tokens/sec: 9368.87 | Global MFU (%): 6.10 | Global TFLOPs: 482.88 | 
2025-05-18 22:02:35,983 - root - INFO - Step: 10 | Loss: 11.58 | Tokens per second: 3902.95 | Training tokens per second (%): 4.79 | MFU (%): 20.34 | TFLOPs: 201.16 | Global batch size: 8 | Global tokens/sec: 31223.64 | Global MFU (%): 20.34 | Global TFLOPs: 1609.30 | 
2025-05-18 22:02:41,214 - root - INFO - Step: 20 | Loss: 10.03 | Tokens per second: 3915.34 | Training tokens per second (%): 7.45 | MFU (%): 20.40 | TFLOPs: 201.80 | Global batch size: 8 | Global tokens/sec: 31322.74 | Global MFU (%): 20.40 | Global TFLOPs: 1614.41 | 
2025-05-18 22:02:46,462 - root - INFO - Step: 30 | Loss: 9.73 | Tokens per second: 3903.27 | Training tokens per second (%): 5.80 | MFU (%): 20.34 | TFLOPs: 201.18 | Global batch size: 8 | Global tokens/sec: 31226.19 | Global MFU (%): 20.34 | Global TFLOPs: 1609.44 | 
2025-05-18 22:02:51,702 - root - INFO - Step: 40 | Loss: 9.19 | Tokens per second: 3909.22 | Training tokens per second (%): 4.15 | MFU (%): 20.37 | TFLOPs: 201.49 | Global batch size: 8 | Global tokens/sec: 31273.77 | Global MFU (%): 20.37 | Global TFLOPs: 1611.89 | 
2025-05-18 22:02:56,945 - root - INFO - Step: 50 | Loss: 8.68 | Tokens per second: 3907.21 | Training tokens per second (%): 4.12 | MFU (%): 20.36 | TFLOPs: 201.38 | Global batch size: 8 | Global tokens/sec: 31257.65 | Global MFU (%): 20.36 | Global TFLOPs: 1611.06 | 
2025-05-18 22:03:02,193 - root - INFO - Step: 60 | Loss: 8.26 | Tokens per second: 3902.83 | Training tokens per second (%): 6.01 | MFU (%): 20.34 | TFLOPs: 201.16 | Global batch size: 8 | Global tokens/sec: 31222.66 | Global MFU (%): 20.34 | Global TFLOPs: 1609.25 | 
2025-05-18 22:03:07,449 - root - INFO - Step: 70 | Loss: 7.96 | Tokens per second: 3897.01 | Training tokens per second (%): 2.99 | MFU (%): 20.31 | TFLOPs: 200.86 | Global batch size: 8 | Global tokens/sec: 31176.09 | Global MFU (%): 20.31 | Global TFLOPs: 1606.85 | 
2025-05-18 22:03:12,705 - root - INFO - Step: 80 | Loss: 7.34 | Tokens per second: 3897.23 | Training tokens per second (%): 6.46 | MFU (%): 20.31 | TFLOPs: 200.87 | Global batch size: 8 | Global tokens/sec: 31177.82 | Global MFU (%): 20.31 | Global TFLOPs: 1606.94 | 
2025-05-18 22:03:17,963 - root - INFO - Step: 90 | Loss: 7.35 | Tokens per second: 3895.76 | Training tokens per second (%): 3.94 | MFU (%): 20.30 | TFLOPs: 200.79 | Global batch size: 8 | Global tokens/sec: 31166.04 | Global MFU (%): 20.30 | Global TFLOPs: 1606.34 | 
2025-05-18 22:03:23,340 - root - INFO - Step: 100 | Loss: 7.31 | Tokens per second: 3809.73 | Training tokens per second (%): 5.36 | MFU (%): 19.85 | TFLOPs: 196.36 | Global batch size: 8 | Global tokens/sec: 30477.81 | Global MFU (%): 19.85 | Global TFLOPs: 1570.86 | 
2025-05-18 22:03:28,606 - root - INFO - Step: 110 | Loss: 7.27 | Tokens per second: 3889.33 | Training tokens per second (%): 6.78 | MFU (%): 20.27 | TFLOPs: 200.46 | Global batch size: 8 | Global tokens/sec: 31114.62 | Global MFU (%): 20.27 | Global TFLOPs: 1603.69 | 
2025-05-18 22:03:33,872 - root - INFO - Step: 120 | Loss: 7.10 | Tokens per second: 3889.75 | Training tokens per second (%): 3.38 | MFU (%): 20.27 | TFLOPs: 200.48 | Global batch size: 8 | Global tokens/sec: 31118.02 | Global MFU (%): 20.27 | Global TFLOPs: 1603.86 | 
2025-05-18 22:03:39,208 - root - INFO - Step: 130 | Loss: 7.04 | Tokens per second: 3838.88 | Training tokens per second (%): 2.56 | MFU (%): 20.01 | TFLOPs: 197.86 | Global batch size: 8 | Global tokens/sec: 30711.07 | Global MFU (%): 20.01 | Global TFLOPs: 1582.89 | 
2025-05-18 22:03:44,484 - root - INFO - Step: 140 | Loss: 6.94 | Tokens per second: 3882.86 | Training tokens per second (%): 3.35 | MFU (%): 20.24 | TFLOPs: 200.13 | Global batch size: 8 | Global tokens/sec: 31062.84 | Global MFU (%): 20.24 | Global TFLOPs: 1601.02 | 
2025-05-18 22:03:49,821 - root - INFO - Step: 150 | Loss: 6.67 | Tokens per second: 3837.56 | Training tokens per second (%): 4.52 | MFU (%): 20.00 | TFLOPs: 197.79 | Global batch size: 8 | Global tokens/sec: 30700.51 | Global MFU (%): 20.00 | Global TFLOPs: 1582.34 | 
2025-05-18 22:03:55,097 - root - INFO - Step: 160 | Loss: 6.95 | Tokens per second: 3882.53 | Training tokens per second (%): 4.36 | MFU (%): 20.23 | TFLOPs: 200.11 | Global batch size: 8 | Global tokens/sec: 31060.24 | Global MFU (%): 20.23 | Global TFLOPs: 1600.88 | 
2025-05-18 22:04:00,400 - root - INFO - Step: 170 | Loss: 7.08 | Tokens per second: 3862.98 | Training tokens per second (%): 4.51 | MFU (%): 20.13 | TFLOPs: 199.10 | Global batch size: 8 | Global tokens/sec: 30903.81 | Global MFU (%): 20.13 | Global TFLOPs: 1592.82 | 
2025-05-18 22:04:05,666 - root - INFO - Step: 180 | Loss: 6.85 | Tokens per second: 3889.42 | Training tokens per second (%): 3.79 | MFU (%): 20.27 | TFLOPs: 200.47 | Global batch size: 8 | Global tokens/sec: 31115.35 | Global MFU (%): 20.27 | Global TFLOPs: 1603.72 | 
2025-05-18 22:04:10,946 - root - INFO - Step: 190 | Loss: 6.42 | Tokens per second: 3879.31 | Training tokens per second (%): 3.79 | MFU (%): 20.22 | TFLOPs: 199.94 | Global batch size: 8 | Global tokens/sec: 31034.52 | Global MFU (%): 20.22 | Global TFLOPs: 1599.56 | 
2025-05-18 22:04:16,215 - root - INFO - Step: 200 | Loss: 6.88 | Tokens per second: 3887.86 | Training tokens per second (%): 4.19 | MFU (%): 20.26 | TFLOPs: 200.39 | Global batch size: 8 | Global tokens/sec: 31102.89 | Global MFU (%): 20.26 | Global TFLOPs: 1603.08 | 
2025-05-18 22:04:21,491 - root - INFO - Step: 210 | Loss: 6.58 | Tokens per second: 3882.22 | Training tokens per second (%): 4.25 | MFU (%): 20.23 | TFLOPs: 200.09 | Global batch size: 8 | Global tokens/sec: 31057.80 | Global MFU (%): 20.23 | Global TFLOPs: 1600.76 | 
2025-05-18 22:04:26,791 - root - INFO - Step: 220 | Loss: 6.51 | Tokens per second: 3865.36 | Training tokens per second (%): 2.74 | MFU (%): 20.14 | TFLOPs: 199.23 | Global batch size: 8 | Global tokens/sec: 30922.85 | Global MFU (%): 20.14 | Global TFLOPs: 1593.80 | 
2025-05-18 22:04:32,082 - root - INFO - Step: 230 | Loss: 6.63 | Tokens per second: 3870.80 | Training tokens per second (%): 4.91 | MFU (%): 20.17 | TFLOPs: 199.51 | Global batch size: 8 | Global tokens/sec: 30966.43 | Global MFU (%): 20.17 | Global TFLOPs: 1596.05 | 
2025-05-18 22:04:37,364 - root - INFO - Step: 240 | Loss: 6.42 | Tokens per second: 3878.13 | Training tokens per second (%): 3.44 | MFU (%): 20.21 | TFLOPs: 199.88 | Global batch size: 8 | Global tokens/sec: 31025.06 | Global MFU (%): 20.21 | Global TFLOPs: 1599.07 | 
2025-05-18 22:04:42,628 - root - INFO - Step: 250 | Loss: 6.52 | Tokens per second: 3891.07 | Training tokens per second (%): 7.18 | MFU (%): 20.28 | TFLOPs: 200.55 | Global batch size: 8 | Global tokens/sec: 31128.59 | Global MFU (%): 20.28 | Global TFLOPs: 1604.41 | 
2025-05-18 22:04:47,902 - root - INFO - Step: 260 | Loss: 5.98 | Tokens per second: 3884.19 | Training tokens per second (%): 7.71 | MFU (%): 20.24 | TFLOPs: 200.20 | Global batch size: 8 | Global tokens/sec: 31073.55 | Global MFU (%): 20.24 | Global TFLOPs: 1601.57 | 
2025-05-18 22:04:53,177 - root - INFO - Step: 270 | Loss: 5.73 | Tokens per second: 3882.84 | Training tokens per second (%): 5.68 | MFU (%): 20.24 | TFLOPs: 200.13 | Global batch size: 8 | Global tokens/sec: 31062.75 | Global MFU (%): 20.24 | Global TFLOPs: 1601.01 | 
2025-05-18 22:04:58,457 - root - INFO - Step: 280 | Loss: 6.12 | Tokens per second: 3879.36 | Training tokens per second (%): 4.49 | MFU (%): 20.22 | TFLOPs: 199.95 | Global batch size: 8 | Global tokens/sec: 31034.85 | Global MFU (%): 20.22 | Global TFLOPs: 1599.57 | 
2025-05-18 22:05:03,756 - root - INFO - Step: 290 | Loss: 6.04 | Tokens per second: 3865.96 | Training tokens per second (%): 3.83 | MFU (%): 20.15 | TFLOPs: 199.26 | Global batch size: 8 | Global tokens/sec: 30927.70 | Global MFU (%): 20.15 | Global TFLOPs: 1594.05 | 
2025-05-18 22:05:09,031 - root - INFO - Step: 300 | Loss: 5.70 | Tokens per second: 3882.68 | Training tokens per second (%): 4.34 | MFU (%): 20.23 | TFLOPs: 200.12 | Global batch size: 8 | Global tokens/sec: 31061.45 | Global MFU (%): 20.23 | Global TFLOPs: 1600.94 | 
2025-05-18 22:05:14,306 - root - INFO - Step: 310 | Loss: 5.72 | Tokens per second: 3883.56 | Training tokens per second (%): 3.19 | MFU (%): 20.24 | TFLOPs: 200.16 | Global batch size: 8 | Global tokens/sec: 31068.46 | Global MFU (%): 20.24 | Global TFLOPs: 1601.31 | 
2025-05-18 22:05:19,574 - root - INFO - Step: 320 | Loss: 5.82 | Tokens per second: 3887.94 | Training tokens per second (%): 3.56 | MFU (%): 20.26 | TFLOPs: 200.39 | Global batch size: 8 | Global tokens/sec: 31103.50 | Global MFU (%): 20.26 | Global TFLOPs: 1603.11 | 
2025-05-18 22:05:24,849 - root - INFO - Step: 330 | Loss: 5.67 | Tokens per second: 3883.28 | Training tokens per second (%): 4.60 | MFU (%): 20.24 | TFLOPs: 200.15 | Global batch size: 8 | Global tokens/sec: 31066.21 | Global MFU (%): 20.24 | Global TFLOPs: 1601.19 | 
2025-05-18 22:05:30,125 - root - INFO - Step: 340 | Loss: 5.64 | Tokens per second: 3882.87 | Training tokens per second (%): 3.14 | MFU (%): 20.24 | TFLOPs: 200.13 | Global batch size: 8 | Global tokens/sec: 31062.94 | Global MFU (%): 20.24 | Global TFLOPs: 1601.02 | 
2025-05-18 22:05:35,416 - root - INFO - Step: 350 | Loss: 5.92 | Tokens per second: 3871.22 | Training tokens per second (%): 5.54 | MFU (%): 20.17 | TFLOPs: 199.53 | Global batch size: 8 | Global tokens/sec: 30969.73 | Global MFU (%): 20.17 | Global TFLOPs: 1596.22 | 
2025-05-18 22:05:40,684 - root - INFO - Step: 360 | Loss: 5.97 | Tokens per second: 3888.11 | Training tokens per second (%): 3.86 | MFU (%): 20.26 | TFLOPs: 200.40 | Global batch size: 8 | Global tokens/sec: 31104.89 | Global MFU (%): 20.26 | Global TFLOPs: 1603.18 | 
2025-05-18 22:05:45,958 - root - INFO - Step: 370 | Loss: 5.88 | Tokens per second: 3884.13 | Training tokens per second (%): 5.61 | MFU (%): 20.24 | TFLOPs: 200.19 | Global batch size: 8 | Global tokens/sec: 31073.03 | Global MFU (%): 20.24 | Global TFLOPs: 1601.54 | 
2025-05-18 22:05:51,234 - root - INFO - Step: 380 | Loss: 5.28 | Tokens per second: 3881.90 | Training tokens per second (%): 5.07 | MFU (%): 20.23 | TFLOPs: 200.08 | Global batch size: 8 | Global tokens/sec: 31055.21 | Global MFU (%): 20.23 | Global TFLOPs: 1600.62 | 
2025-05-18 22:05:56,517 - root - INFO - Step: 390 | Loss: 4.64 | Tokens per second: 3877.44 | Training tokens per second (%): 5.81 | MFU (%): 20.21 | TFLOPs: 199.85 | Global batch size: 8 | Global tokens/sec: 31019.54 | Global MFU (%): 20.21 | Global TFLOPs: 1598.78 | 
2025-05-18 22:06:01,804 - root - INFO - Step: 400 | Loss: 5.21 | Tokens per second: 3874.29 | Training tokens per second (%): 4.95 | MFU (%): 20.19 | TFLOPs: 199.69 | Global batch size: 8 | Global tokens/sec: 30994.36 | Global MFU (%): 20.19 | Global TFLOPs: 1597.49 | 
2025-05-18 22:06:07,102 - root - INFO - Step: 410 | Loss: 5.42 | Tokens per second: 3866.51 | Training tokens per second (%): 3.52 | MFU (%): 20.15 | TFLOPs: 199.28 | Global batch size: 8 | Global tokens/sec: 30932.12 | Global MFU (%): 20.15 | Global TFLOPs: 1594.28 | 
2025-05-18 22:06:12,380 - root - INFO - Step: 420 | Loss: 5.29 | Tokens per second: 3880.59 | Training tokens per second (%): 3.12 | MFU (%): 20.22 | TFLOPs: 200.01 | Global batch size: 8 | Global tokens/sec: 31044.75 | Global MFU (%): 20.22 | Global TFLOPs: 1600.08 | 
2025-05-18 22:06:17,661 - root - INFO - Step: 430 | Loss: 4.67 | Tokens per second: 3878.98 | Training tokens per second (%): 2.84 | MFU (%): 20.22 | TFLOPs: 199.93 | Global batch size: 8 | Global tokens/sec: 31031.83 | Global MFU (%): 20.22 | Global TFLOPs: 1599.42 | 
2025-05-18 22:06:22,947 - root - INFO - Step: 440 | Loss: 4.93 | Tokens per second: 3874.94 | Training tokens per second (%): 4.34 | MFU (%): 20.19 | TFLOPs: 199.72 | Global batch size: 8 | Global tokens/sec: 30999.55 | Global MFU (%): 20.19 | Global TFLOPs: 1597.75 | 
2025-05-18 22:06:28,227 - root - INFO - Step: 450 | Loss: 4.76 | Tokens per second: 3879.87 | Training tokens per second (%): 4.89 | MFU (%): 20.22 | TFLOPs: 199.97 | Global batch size: 8 | Global tokens/sec: 31038.94 | Global MFU (%): 20.22 | Global TFLOPs: 1599.78 | 
2025-05-18 22:06:33,525 - root - INFO - Step: 460 | Loss: 4.73 | Tokens per second: 3865.76 | Training tokens per second (%): 6.64 | MFU (%): 20.15 | TFLOPs: 199.25 | Global batch size: 8 | Global tokens/sec: 30926.07 | Global MFU (%): 20.15 | Global TFLOPs: 1593.97 | 
2025-05-18 22:06:38,832 - root - INFO - Step: 470 | Loss: 4.63 | Tokens per second: 3859.71 | Training tokens per second (%): 5.41 | MFU (%): 20.11 | TFLOPs: 198.93 | Global batch size: 8 | Global tokens/sec: 30877.69 | Global MFU (%): 20.11 | Global TFLOPs: 1591.47 | 
2025-05-18 22:06:44,112 - root - INFO - Step: 480 | Loss: 5.19 | Tokens per second: 3879.50 | Training tokens per second (%): 5.34 | MFU (%): 20.22 | TFLOPs: 199.95 | Global batch size: 8 | Global tokens/sec: 31036.00 | Global MFU (%): 20.22 | Global TFLOPs: 1599.63 | 
2025-05-18 22:06:49,391 - root - INFO - Step: 490 | Loss: 4.96 | Tokens per second: 3880.33 | Training tokens per second (%): 4.56 | MFU (%): 20.22 | TFLOPs: 200.00 | Global batch size: 8 | Global tokens/sec: 31042.62 | Global MFU (%): 20.22 | Global TFLOPs: 1599.97 | 
2025-05-18 22:06:54,664 - root - INFO - Step: 500 | Loss: 5.10 | Tokens per second: 3884.37 | Training tokens per second (%): 4.10 | MFU (%): 20.24 | TFLOPs: 200.20 | Global batch size: 8 | Global tokens/sec: 31074.92 | Global MFU (%): 20.24 | Global TFLOPs: 1601.64 | 
2025-05-18 22:06:59,941 - root - INFO - Step: 510 | Loss: 4.22 | Tokens per second: 3882.24 | Training tokens per second (%): 2.82 | MFU (%): 20.23 | TFLOPs: 200.10 | Global batch size: 8 | Global tokens/sec: 31057.90 | Global MFU (%): 20.23 | Global TFLOPs: 1600.76 | 
2025-05-18 22:07:05,224 - root - INFO - Step: 520 | Loss: 3.73 | Tokens per second: 3877.25 | Training tokens per second (%): 4.17 | MFU (%): 20.21 | TFLOPs: 199.84 | Global batch size: 8 | Global tokens/sec: 31018.00 | Global MFU (%): 20.21 | Global TFLOPs: 1598.70 | 
2025-05-18 22:07:10,514 - root - INFO - Step: 530 | Loss: 3.66 | Tokens per second: 3871.46 | Training tokens per second (%): 4.79 | MFU (%): 20.18 | TFLOPs: 199.54 | Global batch size: 8 | Global tokens/sec: 30971.64 | Global MFU (%): 20.18 | Global TFLOPs: 1596.32 | 
2025-05-18 22:07:15,803 - root - INFO - Step: 540 | Loss: 3.94 | Tokens per second: 3872.91 | Training tokens per second (%): 4.80 | MFU (%): 20.18 | TFLOPs: 199.61 | Global batch size: 8 | Global tokens/sec: 30983.29 | Global MFU (%): 20.18 | Global TFLOPs: 1596.92 | 
2025-05-18 22:07:21,079 - root - INFO - Step: 550 | Loss: 4.98 | Tokens per second: 3882.72 | Training tokens per second (%): 4.57 | MFU (%): 20.23 | TFLOPs: 200.12 | Global batch size: 8 | Global tokens/sec: 31061.72 | Global MFU (%): 20.23 | Global TFLOPs: 1600.96 | 
2025-05-18 22:07:26,357 - root - INFO - Step: 560 | Loss: 4.04 | Tokens per second: 3880.60 | Training tokens per second (%): 4.92 | MFU (%): 20.22 | TFLOPs: 200.01 | Global batch size: 8 | Global tokens/sec: 31044.79 | Global MFU (%): 20.22 | Global TFLOPs: 1600.09 | 
2025-05-18 22:07:31,634 - root - INFO - Step: 570 | Loss: 4.64 | Tokens per second: 3881.74 | Training tokens per second (%): 5.17 | MFU (%): 20.23 | TFLOPs: 200.07 | Global batch size: 8 | Global tokens/sec: 31053.90 | Global MFU (%): 20.23 | Global TFLOPs: 1600.56 | 
2025-05-18 22:07:36,914 - root - INFO - Step: 580 | Loss: 4.62 | Tokens per second: 3879.83 | Training tokens per second (%): 6.28 | MFU (%): 20.22 | TFLOPs: 199.97 | Global batch size: 8 | Global tokens/sec: 31038.67 | Global MFU (%): 20.22 | Global TFLOPs: 1599.77 | 
2025-05-18 22:07:42,192 - root - INFO - Step: 590 | Loss: 4.13 | Tokens per second: 3881.14 | Training tokens per second (%): 6.14 | MFU (%): 20.23 | TFLOPs: 200.04 | Global batch size: 8 | Global tokens/sec: 31049.09 | Global MFU (%): 20.23 | Global TFLOPs: 1600.31 | 
2025-05-18 22:07:47,504 - root - INFO - Step: 600 | Loss: 3.72 | Tokens per second: 3855.97 | Training tokens per second (%): 5.04 | MFU (%): 20.10 | TFLOPs: 198.74 | Global batch size: 8 | Global tokens/sec: 30847.79 | Global MFU (%): 20.10 | Global TFLOPs: 1589.93 | 
2025-05-18 22:07:52,781 - root - INFO - Step: 610 | Loss: 4.32 | Tokens per second: 3881.46 | Training tokens per second (%): 5.14 | MFU (%): 20.23 | TFLOPs: 200.06 | Global batch size: 8 | Global tokens/sec: 31051.68 | Global MFU (%): 20.23 | Global TFLOPs: 1600.44 | 
2025-05-18 22:07:58,058 - root - INFO - Step: 620 | Loss: 3.99 | Tokens per second: 3881.47 | Training tokens per second (%): 3.22 | MFU (%): 20.23 | TFLOPs: 200.06 | Global batch size: 8 | Global tokens/sec: 31051.72 | Global MFU (%): 20.23 | Global TFLOPs: 1600.44 | 
2025-05-18 22:08:03,338 - root - INFO - Step: 630 | Loss: 3.23 | Tokens per second: 3879.89 | Training tokens per second (%): 4.14 | MFU (%): 20.22 | TFLOPs: 199.97 | Global batch size: 8 | Global tokens/sec: 31039.08 | Global MFU (%): 20.22 | Global TFLOPs: 1599.79 | 
2025-05-18 22:08:08,627 - root - INFO - Step: 640 | Loss: 3.93 | Tokens per second: 3872.72 | Training tokens per second (%): 4.54 | MFU (%): 20.18 | TFLOPs: 199.60 | Global batch size: 8 | Global tokens/sec: 30981.76 | Global MFU (%): 20.18 | Global TFLOPs: 1596.84 | 
2025-05-18 22:08:13,907 - root - INFO - Step: 650 | Loss: 3.31 | Tokens per second: 3879.30 | Training tokens per second (%): 3.15 | MFU (%): 20.22 | TFLOPs: 199.94 | Global batch size: 8 | Global tokens/sec: 31034.36 | Global MFU (%): 20.22 | Global TFLOPs: 1599.55 | 
2025-05-18 22:08:19,217 - root - INFO - Step: 660 | Loss: 3.83 | Tokens per second: 3857.60 | Training tokens per second (%): 4.77 | MFU (%): 20.10 | TFLOPs: 198.83 | Global batch size: 8 | Global tokens/sec: 30860.76 | Global MFU (%): 20.10 | Global TFLOPs: 1590.60 | 
2025-05-18 22:08:24,509 - root - INFO - Step: 670 | Loss: 2.97 | Tokens per second: 3870.54 | Training tokens per second (%): 5.39 | MFU (%): 20.17 | TFLOPs: 199.49 | Global batch size: 8 | Global tokens/sec: 30964.34 | Global MFU (%): 20.17 | Global TFLOPs: 1595.94 | 
2025-05-18 22:08:29,789 - root - INFO - Step: 680 | Loss: 3.89 | Tokens per second: 3880.06 | Training tokens per second (%): 6.40 | MFU (%): 20.22 | TFLOPs: 199.98 | Global batch size: 8 | Global tokens/sec: 31040.50 | Global MFU (%): 20.22 | Global TFLOPs: 1599.86 | 
2025-05-18 22:08:35,065 - root - INFO - Step: 690 | Loss: 3.45 | Tokens per second: 3882.06 | Training tokens per second (%): 6.36 | MFU (%): 20.23 | TFLOPs: 200.09 | Global batch size: 8 | Global tokens/sec: 31056.52 | Global MFU (%): 20.23 | Global TFLOPs: 1600.69 | 
2025-05-18 22:08:40,343 - root - INFO - Step: 700 | Loss: 2.98 | Tokens per second: 3881.42 | Training tokens per second (%): 4.09 | MFU (%): 20.23 | TFLOPs: 200.05 | Global batch size: 8 | Global tokens/sec: 31051.32 | Global MFU (%): 20.23 | Global TFLOPs: 1600.42 | 
2025-05-18 22:08:45,635 - root - INFO - Step: 710 | Loss: 3.51 | Tokens per second: 3870.33 | Training tokens per second (%): 4.57 | MFU (%): 20.17 | TFLOPs: 199.48 | Global batch size: 8 | Global tokens/sec: 30962.61 | Global MFU (%): 20.17 | Global TFLOPs: 1595.85 | 
2025-05-18 22:08:50,943 - root - INFO - Step: 720 | Loss: 3.29 | Tokens per second: 3859.30 | Training tokens per second (%): 5.05 | MFU (%): 20.11 | TFLOPs: 198.91 | Global batch size: 8 | Global tokens/sec: 30874.42 | Global MFU (%): 20.11 | Global TFLOPs: 1591.30 | 
2025-05-18 22:08:56,218 - root - INFO - Step: 730 | Loss: 3.60 | Tokens per second: 3882.79 | Training tokens per second (%): 3.79 | MFU (%): 20.23 | TFLOPs: 200.12 | Global batch size: 8 | Global tokens/sec: 31062.28 | Global MFU (%): 20.23 | Global TFLOPs: 1600.99 | 
2025-05-18 22:09:01,501 - root - INFO - Step: 740 | Loss: 3.21 | Tokens per second: 3877.59 | Training tokens per second (%): 4.22 | MFU (%): 20.21 | TFLOPs: 199.86 | Global batch size: 8 | Global tokens/sec: 31020.74 | Global MFU (%): 20.21 | Global TFLOPs: 1598.85 | 
2025-05-18 22:09:06,789 - root - INFO - Step: 750 | Loss: 3.97 | Tokens per second: 3872.96 | Training tokens per second (%): 4.13 | MFU (%): 20.18 | TFLOPs: 199.62 | Global batch size: 8 | Global tokens/sec: 30983.71 | Global MFU (%): 20.18 | Global TFLOPs: 1596.94 | 
2025-05-18 22:09:12,073 - root - INFO - Step: 760 | Loss: 2.78 | Tokens per second: 3877.14 | Training tokens per second (%): 6.67 | MFU (%): 20.21 | TFLOPs: 199.83 | Global batch size: 8 | Global tokens/sec: 31017.13 | Global MFU (%): 20.21 | Global TFLOPs: 1598.66 | 
2025-05-18 22:09:17,354 - root - INFO - Step: 770 | Loss: 2.38 | Tokens per second: 3878.45 | Training tokens per second (%): 5.06 | MFU (%): 20.21 | TFLOPs: 199.90 | Global batch size: 8 | Global tokens/sec: 31027.62 | Global MFU (%): 20.21 | Global TFLOPs: 1599.20 | 
2025-05-18 22:09:22,641 - root - INFO - Step: 780 | Loss: 2.33 | Tokens per second: 3874.44 | Training tokens per second (%): 4.43 | MFU (%): 20.19 | TFLOPs: 199.69 | Global batch size: 8 | Global tokens/sec: 30995.52 | Global MFU (%): 20.19 | Global TFLOPs: 1597.55 | 
2025-05-18 22:09:27,944 - root - INFO - Step: 790 | Loss: 3.17 | Tokens per second: 3862.49 | Training tokens per second (%): 4.11 | MFU (%): 20.13 | TFLOPs: 199.08 | Global batch size: 8 | Global tokens/sec: 30899.92 | Global MFU (%): 20.13 | Global TFLOPs: 1592.62 | 
2025-05-18 22:09:33,222 - root - INFO - Step: 800 | Loss: 2.25 | Tokens per second: 3881.17 | Training tokens per second (%): 4.59 | MFU (%): 20.23 | TFLOPs: 200.04 | Global batch size: 8 | Global tokens/sec: 31049.39 | Global MFU (%): 20.23 | Global TFLOPs: 1600.32 | 
2025-05-18 22:09:38,510 - root - INFO - Step: 810 | Loss: 2.91 | Tokens per second: 3873.19 | Training tokens per second (%): 4.34 | MFU (%): 20.18 | TFLOPs: 199.63 | Global batch size: 8 | Global tokens/sec: 30985.53 | Global MFU (%): 20.18 | Global TFLOPs: 1597.03 | 
2025-05-18 22:09:43,798 - root - INFO - Step: 820 | Loss: 2.46 | Tokens per second: 3874.06 | Training tokens per second (%): 4.48 | MFU (%): 20.19 | TFLOPs: 199.67 | Global batch size: 8 | Global tokens/sec: 30992.44 | Global MFU (%): 20.19 | Global TFLOPs: 1597.39 | 
2025-05-18 22:09:49,074 - root - INFO - Step: 830 | Loss: 2.40 | Tokens per second: 3881.66 | Training tokens per second (%): 6.84 | MFU (%): 20.23 | TFLOPs: 200.07 | Global batch size: 8 | Global tokens/sec: 31053.25 | Global MFU (%): 20.23 | Global TFLOPs: 1600.52 | 
2025-05-18 22:09:54,365 - root - INFO - Step: 840 | Loss: 2.59 | Tokens per second: 3871.47 | Training tokens per second (%): 6.01 | MFU (%): 20.18 | TFLOPs: 199.54 | Global batch size: 8 | Global tokens/sec: 30971.73 | Global MFU (%): 20.18 | Global TFLOPs: 1596.32 | 
2025-05-18 22:09:59,671 - root - INFO - Step: 850 | Loss: 2.86 | Tokens per second: 3860.79 | Training tokens per second (%): 7.29 | MFU (%): 20.12 | TFLOPs: 198.99 | Global batch size: 8 | Global tokens/sec: 30886.34 | Global MFU (%): 20.12 | Global TFLOPs: 1591.92 | 
2025-05-18 22:10:04,964 - root - INFO - Step: 860 | Loss: 2.23 | Tokens per second: 3869.93 | Training tokens per second (%): 5.61 | MFU (%): 20.17 | TFLOPs: 199.46 | Global batch size: 8 | Global tokens/sec: 30959.47 | Global MFU (%): 20.17 | Global TFLOPs: 1595.69 | 
2025-05-18 22:10:10,252 - root - INFO - Step: 870 | Loss: 2.82 | Tokens per second: 3873.37 | Training tokens per second (%): 2.96 | MFU (%): 20.19 | TFLOPs: 199.64 | Global batch size: 8 | Global tokens/sec: 30986.93 | Global MFU (%): 20.19 | Global TFLOPs: 1597.10 | 
2025-05-18 22:10:15,531 - root - INFO - Step: 880 | Loss: 1.65 | Tokens per second: 3880.70 | Training tokens per second (%): 7.08 | MFU (%): 20.22 | TFLOPs: 200.02 | Global batch size: 8 | Global tokens/sec: 31045.59 | Global MFU (%): 20.22 | Global TFLOPs: 1600.13 | 
2025-05-18 22:10:20,822 - root - INFO - Step: 890 | Loss: 2.46 | Tokens per second: 3870.98 | Training tokens per second (%): 6.01 | MFU (%): 20.17 | TFLOPs: 199.52 | Global batch size: 8 | Global tokens/sec: 30967.87 | Global MFU (%): 20.17 | Global TFLOPs: 1596.12 | 
2025-05-18 22:10:26,114 - root - INFO - Step: 900 | Loss: 2.71 | Tokens per second: 3871.05 | Training tokens per second (%): 4.01 | MFU (%): 20.17 | TFLOPs: 199.52 | Global batch size: 8 | Global tokens/sec: 30968.41 | Global MFU (%): 20.17 | Global TFLOPs: 1596.15 | 
2025-05-18 22:10:31,409 - root - INFO - Step: 910 | Loss: 0.76 | Tokens per second: 3867.84 | Training tokens per second (%): 5.07 | MFU (%): 20.16 | TFLOPs: 199.35 | Global batch size: 8 | Global tokens/sec: 30942.69 | Global MFU (%): 20.16 | Global TFLOPs: 1594.82 | 
2025-05-18 22:10:36,693 - root - INFO - Step: 920 | Loss: 1.80 | Tokens per second: 3876.74 | Training tokens per second (%): 7.57 | MFU (%): 20.20 | TFLOPs: 199.81 | Global batch size: 8 | Global tokens/sec: 31013.94 | Global MFU (%): 20.20 | Global TFLOPs: 1598.50 | 
2025-05-18 22:10:41,972 - root - INFO - Step: 930 | Loss: 0.91 | Tokens per second: 3880.42 | Training tokens per second (%): 3.14 | MFU (%): 20.22 | TFLOPs: 200.00 | Global batch size: 8 | Global tokens/sec: 31043.39 | Global MFU (%): 20.22 | Global TFLOPs: 1600.01 | 
2025-05-18 22:10:47,248 - root - INFO - Step: 940 | Loss: 1.60 | Tokens per second: 3882.55 | Training tokens per second (%): 4.38 | MFU (%): 20.23 | TFLOPs: 200.11 | Global batch size: 8 | Global tokens/sec: 31060.36 | Global MFU (%): 20.23 | Global TFLOPs: 1600.89 | 
2025-05-18 22:10:52,522 - root - INFO - Step: 950 | Loss: 2.44 | Tokens per second: 3883.50 | Training tokens per second (%): 6.24 | MFU (%): 20.24 | TFLOPs: 200.16 | Global batch size: 8 | Global tokens/sec: 31067.97 | Global MFU (%): 20.24 | Global TFLOPs: 1601.28 | 
2025-05-18 22:10:57,811 - root - INFO - Step: 960 | Loss: 1.75 | Tokens per second: 3873.06 | Training tokens per second (%): 4.21 | MFU (%): 20.18 | TFLOPs: 199.62 | Global batch size: 8 | Global tokens/sec: 30984.52 | Global MFU (%): 20.18 | Global TFLOPs: 1596.98 | 
2025-05-18 22:11:03,117 - root - INFO - Step: 970 | Loss: 2.24 | Tokens per second: 3860.17 | Training tokens per second (%): 5.34 | MFU (%): 20.12 | TFLOPs: 198.96 | Global batch size: 8 | Global tokens/sec: 30881.34 | Global MFU (%): 20.12 | Global TFLOPs: 1591.66 | 
2025-05-18 22:11:08,403 - root - INFO - Step: 980 | Loss: 1.51 | Tokens per second: 3874.88 | Training tokens per second (%): 3.37 | MFU (%): 20.19 | TFLOPs: 199.72 | Global batch size: 8 | Global tokens/sec: 30999.01 | Global MFU (%): 20.19 | Global TFLOPs: 1597.73 | 
2025-05-18 22:11:13,686 - root - INFO - Step: 990 | Loss: 2.11 | Tokens per second: 3877.74 | Training tokens per second (%): 4.84 | MFU (%): 20.21 | TFLOPs: 199.86 | Global batch size: 8 | Global tokens/sec: 31021.95 | Global MFU (%): 20.21 | Global TFLOPs: 1598.91 | 
2025-05-18 22:11:18,967 - root - INFO - Step: 1000 | Loss: 1.47 | Tokens per second: 3878.32 | Training tokens per second (%): 7.13 | MFU (%): 20.21 | TFLOPs: 199.89 | Global batch size: 8 | Global tokens/sec: 31026.57 | Global MFU (%): 20.21 | Global TFLOPs: 1599.15 | 
2025-05-18 22:11:18,967 - root - INFO - Training completed
[sbatch-master] task finished
