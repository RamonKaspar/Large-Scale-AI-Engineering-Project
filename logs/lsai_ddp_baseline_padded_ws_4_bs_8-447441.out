[sbatch-master] running on nid006931
[sbatch-master] SLURM_NODELIST: nid006931
[sbatch-master] SLURM_NNODES: 1
[sbatch-master] SLURM_NODEID: 0
[Master] Master node: nid006931
[Master] World size: 4
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
slurmstepd: error: couldn't chdir to `/workspace': No such file or directory: going to /tmp instead
[srun] rank=0 host=nid006931 noderank=0 localrank=0
W0518 22:17:21.153000 97962 torch/distributed/run.py:792] 
W0518 22:17:21.153000 97962 torch/distributed/run.py:792] *****************************************
W0518 22:17:21.153000 97962 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0518 22:17:21.153000 97962 torch/distributed/run.py:792] *****************************************
2025-05-18 22:17:33,553 - root - INFO - [Distributed Init] Rank 0 initialized on node 0 on GPU 0.
[rank0]:[W518 22:17:34.771858962 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 22:17:34,162 - root - INFO - [Distributed Init] Rank 2 initialized on node 0 on GPU 2.
[rank2]:[W518 22:17:34.892641843 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 22:17:34,181 - root - INFO - [Distributed Init] Rank 1 initialized on node 0 on GPU 1.
[rank1]:[W518 22:17:34.912028131 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 22:17:39,158 - root - INFO - [Distributed Init] Rank 3 initialized on node 0 on GPU 3.
[rank3]:[W518 22:17:39.889005410 ProcessGroupNCCL.cpp:4454] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
2025-05-18 22:17:44,243 - root - INFO - [Rank 0] All ranks ready!
2025-05-18 22:17:44,243 - root - INFO - Distributed training enabled: 4 processes
2025-05-18 22:17:44,243 - root - INFO - Master process: 0 on cuda:0
2025-05-18 22:17:44,243 - root - INFO - Experiment args: Namespace(dataset='/capstor/scratch/cscs/kasparr/project/train_data.parquet', dataset_type='padded', pretokenized=False, tokenizer_name_or_path='unsloth/Mistral-Nemo-Base-2407-bnb-4bit', sequence_length=2048, batch_size=8, fused_optimizer=False, learning_rate=5e-05, lr_warmup_steps=100, training_steps=1000, logging_frequency=10, profile=False, profile_step_start=10, profile_step_end=10, grad_max_norm=1, model_dtype='bf16', compile=False, distributed=True, backend='nccl', find_unused_parameters=False)
2025-05-18 22:17:44,243 - root - INFO - Setting up Tokenizer...
2025-05-18 22:17:44,811 - root - INFO - Setting up DataLoaders...
2025-05-18 22:17:44,811 - root - INFO - Using padded ParquetDataset with on-the-fly tokenization
2025-05-18 22:18:01,622 - root - INFO - Setting up Model...
2025-05-18 22:18:09,643 - root - INFO - Model wrapped with DistributedDataParallel
2025-05-18 22:18:09,644 - root - INFO - Global batch size: 32 (local: 8 Ã— 4 processes)
2025-05-18 22:18:09,644 - root - INFO - Starting training!
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
/iopsstor/scratch/cscs/kasparr/project/src/train.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  num_items_tensor = torch.tensor(num_items_in_batch, device=device)
2025-05-18 22:18:11,106 - root - INFO - Step: 1 | Loss: 11.94 | Tokens per second: 11215.59 | Training tokens per second (%): 4.85 | MFU (%): 10.50 | TFLOPs: 103.87 | Global batch size: 32 | Global tokens/sec: 44862.38 | Global MFU (%): 10.50 | Global TFLOPs: 415.49 | 
2025-05-18 22:18:14,742 - root - INFO - Step: 10 | Loss: 11.91 | Tokens per second: 40559.12 | Training tokens per second (%): 9.41 | MFU (%): 37.98 | TFLOPs: 375.64 | Global batch size: 32 | Global tokens/sec: 162236.48 | Global MFU (%): 37.98 | Global TFLOPs: 1502.54 | 
2025-05-18 22:18:18,749 - root - INFO - Step: 20 | Loss: 11.71 | Tokens per second: 40898.60 | Training tokens per second (%): 9.71 | MFU (%): 38.30 | TFLOPs: 378.78 | Global batch size: 32 | Global tokens/sec: 163594.41 | Global MFU (%): 38.30 | Global TFLOPs: 1515.12 | 
2025-05-18 22:18:22,797 - root - INFO - Step: 30 | Loss: 11.18 | Tokens per second: 40483.56 | Training tokens per second (%): 9.61 | MFU (%): 37.91 | TFLOPs: 374.94 | Global batch size: 32 | Global tokens/sec: 161934.23 | Global MFU (%): 37.91 | Global TFLOPs: 1499.74 | 
2025-05-18 22:18:26,866 - root - INFO - Step: 40 | Loss: 10.20 | Tokens per second: 40266.41 | Training tokens per second (%): 8.42 | MFU (%): 37.71 | TFLOPs: 372.92 | Global batch size: 32 | Global tokens/sec: 161065.64 | Global MFU (%): 37.71 | Global TFLOPs: 1491.70 | 
2025-05-18 22:18:30,950 - root - INFO - Step: 50 | Loss: 9.48 | Tokens per second: 40126.95 | Training tokens per second (%): 9.74 | MFU (%): 37.58 | TFLOPs: 371.63 | Global batch size: 32 | Global tokens/sec: 160507.82 | Global MFU (%): 37.58 | Global TFLOPs: 1486.53 | 
2025-05-18 22:18:35,046 - root - INFO - Step: 60 | Loss: 9.08 | Tokens per second: 40010.21 | Training tokens per second (%): 8.89 | MFU (%): 37.47 | TFLOPs: 370.55 | Global batch size: 32 | Global tokens/sec: 160040.85 | Global MFU (%): 37.47 | Global TFLOPs: 1482.21 | 
2025-05-18 22:18:39,106 - root - INFO - Step: 70 | Loss: 8.52 | Tokens per second: 40356.47 | Training tokens per second (%): 9.64 | MFU (%): 37.79 | TFLOPs: 373.76 | Global batch size: 32 | Global tokens/sec: 161425.87 | Global MFU (%): 37.79 | Global TFLOPs: 1495.03 | 
2025-05-18 22:18:43,101 - root - INFO - Step: 80 | Loss: 8.06 | Tokens per second: 41020.52 | Training tokens per second (%): 9.36 | MFU (%): 38.41 | TFLOPs: 379.91 | Global batch size: 32 | Global tokens/sec: 164082.09 | Global MFU (%): 38.41 | Global TFLOPs: 1519.63 | 
2025-05-18 22:18:47,127 - root - INFO - Step: 90 | Loss: 7.70 | Tokens per second: 40703.31 | Training tokens per second (%): 9.31 | MFU (%): 38.12 | TFLOPs: 376.97 | Global batch size: 32 | Global tokens/sec: 162813.24 | Global MFU (%): 38.12 | Global TFLOPs: 1507.88 | 
2025-05-18 22:18:51,172 - root - INFO - Step: 100 | Loss: 7.57 | Tokens per second: 40512.58 | Training tokens per second (%): 9.45 | MFU (%): 37.94 | TFLOPs: 375.20 | Global batch size: 32 | Global tokens/sec: 162050.31 | Global MFU (%): 37.94 | Global TFLOPs: 1500.82 | 
2025-05-18 22:18:55,225 - root - INFO - Step: 110 | Loss: 7.34 | Tokens per second: 40433.01 | Training tokens per second (%): 9.60 | MFU (%): 37.86 | TFLOPs: 374.47 | Global batch size: 32 | Global tokens/sec: 161732.04 | Global MFU (%): 37.86 | Global TFLOPs: 1497.87 | 
2025-05-18 22:18:59,253 - root - INFO - Step: 120 | Loss: 7.34 | Tokens per second: 40680.99 | Training tokens per second (%): 9.10 | MFU (%): 38.10 | TFLOPs: 376.76 | Global batch size: 32 | Global tokens/sec: 162723.96 | Global MFU (%): 38.10 | Global TFLOPs: 1507.06 | 
2025-05-18 22:19:03,238 - root - INFO - Step: 130 | Loss: 7.23 | Tokens per second: 41115.33 | Training tokens per second (%): 9.70 | MFU (%): 38.50 | TFLOPs: 380.79 | Global batch size: 32 | Global tokens/sec: 164461.33 | Global MFU (%): 38.50 | Global TFLOPs: 1523.15 | 
2025-05-18 22:19:07,487 - root - INFO - Step: 140 | Loss: 7.25 | Tokens per second: 38564.61 | Training tokens per second (%): 8.93 | MFU (%): 36.11 | TFLOPs: 357.16 | Global batch size: 32 | Global tokens/sec: 154258.44 | Global MFU (%): 36.11 | Global TFLOPs: 1428.65 | 
2025-05-18 22:19:11,502 - root - INFO - Step: 150 | Loss: 7.09 | Tokens per second: 40822.15 | Training tokens per second (%): 9.77 | MFU (%): 38.23 | TFLOPs: 378.07 | Global batch size: 32 | Global tokens/sec: 163288.59 | Global MFU (%): 38.23 | Global TFLOPs: 1512.29 | 
2025-05-18 22:19:15,533 - root - INFO - Step: 160 | Loss: 7.24 | Tokens per second: 40647.51 | Training tokens per second (%): 8.53 | MFU (%): 38.06 | TFLOPs: 376.45 | Global batch size: 32 | Global tokens/sec: 162590.04 | Global MFU (%): 38.06 | Global TFLOPs: 1505.82 | 
2025-05-18 22:19:19,542 - root - INFO - Step: 170 | Loss: 7.12 | Tokens per second: 40877.24 | Training tokens per second (%): 9.06 | MFU (%): 38.28 | TFLOPs: 378.58 | Global batch size: 32 | Global tokens/sec: 163508.97 | Global MFU (%): 38.28 | Global TFLOPs: 1514.33 | 
2025-05-18 22:19:23,552 - root - INFO - Step: 180 | Loss: 7.04 | Tokens per second: 40859.13 | Training tokens per second (%): 8.96 | MFU (%): 38.26 | TFLOPs: 378.41 | Global batch size: 32 | Global tokens/sec: 163436.50 | Global MFU (%): 38.26 | Global TFLOPs: 1513.66 | 
2025-05-18 22:19:27,623 - root - INFO - Step: 190 | Loss: 6.95 | Tokens per second: 40259.56 | Training tokens per second (%): 9.49 | MFU (%): 37.70 | TFLOPs: 372.86 | Global batch size: 32 | Global tokens/sec: 161038.23 | Global MFU (%): 37.70 | Global TFLOPs: 1491.44 | 
2025-05-18 22:19:31,635 - root - INFO - Step: 200 | Loss: 7.03 | Tokens per second: 40841.67 | Training tokens per second (%): 9.24 | MFU (%): 38.25 | TFLOPs: 378.25 | Global batch size: 32 | Global tokens/sec: 163366.69 | Global MFU (%): 38.25 | Global TFLOPs: 1513.01 | 
2025-05-18 22:19:35,657 - root - INFO - Step: 210 | Loss: 7.01 | Tokens per second: 40742.08 | Training tokens per second (%): 10.28 | MFU (%): 38.15 | TFLOPs: 377.33 | Global batch size: 32 | Global tokens/sec: 162968.31 | Global MFU (%): 38.15 | Global TFLOPs: 1509.32 | 
2025-05-18 22:19:39,795 - root - INFO - Step: 220 | Loss: 6.88 | Tokens per second: 39605.12 | Training tokens per second (%): 9.01 | MFU (%): 37.09 | TFLOPs: 366.80 | Global batch size: 32 | Global tokens/sec: 158420.48 | Global MFU (%): 37.09 | Global TFLOPs: 1467.20 | 
2025-05-18 22:19:43,979 - root - INFO - Step: 230 | Loss: 6.80 | Tokens per second: 39160.38 | Training tokens per second (%): 10.00 | MFU (%): 36.67 | TFLOPs: 362.68 | Global batch size: 32 | Global tokens/sec: 156641.51 | Global MFU (%): 36.67 | Global TFLOPs: 1450.72 | 
2025-05-18 22:19:47,971 - root - INFO - Step: 240 | Loss: 6.89 | Tokens per second: 41049.44 | Training tokens per second (%): 10.08 | MFU (%): 38.44 | TFLOPs: 380.18 | Global batch size: 32 | Global tokens/sec: 164197.78 | Global MFU (%): 38.44 | Global TFLOPs: 1520.71 | 
2025-05-18 22:19:52,006 - root - INFO - Step: 250 | Loss: 6.81 | Tokens per second: 40613.98 | Training tokens per second (%): 9.44 | MFU (%): 38.03 | TFLOPs: 376.14 | Global batch size: 32 | Global tokens/sec: 162455.90 | Global MFU (%): 38.03 | Global TFLOPs: 1504.57 | 
2025-05-18 22:19:56,008 - root - INFO - Step: 260 | Loss: 6.67 | Tokens per second: 40947.71 | Training tokens per second (%): 10.13 | MFU (%): 38.35 | TFLOPs: 379.23 | Global batch size: 32 | Global tokens/sec: 163790.84 | Global MFU (%): 38.35 | Global TFLOPs: 1516.94 | 
2025-05-18 22:20:00,011 - root - INFO - Step: 270 | Loss: 6.63 | Tokens per second: 40935.33 | Training tokens per second (%): 8.52 | MFU (%): 38.33 | TFLOPs: 379.12 | Global batch size: 32 | Global tokens/sec: 163741.31 | Global MFU (%): 38.33 | Global TFLOPs: 1516.48 | 
2025-05-18 22:20:04,072 - root - INFO - Step: 280 | Loss: 6.67 | Tokens per second: 40345.44 | Training tokens per second (%): 10.48 | MFU (%): 37.78 | TFLOPs: 373.66 | Global batch size: 32 | Global tokens/sec: 161381.76 | Global MFU (%): 37.78 | Global TFLOPs: 1494.63 | 
2025-05-18 22:20:08,089 - root - INFO - Step: 290 | Loss: 6.74 | Tokens per second: 40799.94 | Training tokens per second (%): 10.73 | MFU (%): 38.21 | TFLOPs: 377.87 | Global batch size: 32 | Global tokens/sec: 163199.75 | Global MFU (%): 38.21 | Global TFLOPs: 1511.46 | 
2025-05-18 22:20:12,079 - root - INFO - Step: 300 | Loss: 6.68 | Tokens per second: 41061.64 | Training tokens per second (%): 9.62 | MFU (%): 38.45 | TFLOPs: 380.29 | Global batch size: 32 | Global tokens/sec: 164246.56 | Global MFU (%): 38.45 | Global TFLOPs: 1521.16 | 
2025-05-18 22:20:16,122 - root - INFO - Step: 310 | Loss: 6.59 | Tokens per second: 40540.49 | Training tokens per second (%): 9.03 | MFU (%): 37.96 | TFLOPs: 375.46 | Global batch size: 32 | Global tokens/sec: 162161.97 | Global MFU (%): 37.96 | Global TFLOPs: 1501.85 | 
2025-05-18 22:20:20,374 - root - INFO - Step: 320 | Loss: 6.53 | Tokens per second: 38535.77 | Training tokens per second (%): 8.73 | MFU (%): 36.09 | TFLOPs: 356.90 | Global batch size: 32 | Global tokens/sec: 154143.10 | Global MFU (%): 36.09 | Global TFLOPs: 1427.59 | 
2025-05-18 22:20:24,387 - root - INFO - Step: 330 | Loss: 6.46 | Tokens per second: 40829.84 | Training tokens per second (%): 11.22 | MFU (%): 38.23 | TFLOPs: 378.14 | Global batch size: 32 | Global tokens/sec: 163319.35 | Global MFU (%): 38.23 | Global TFLOPs: 1512.57 | 
2025-05-18 22:20:28,452 - root - INFO - Step: 340 | Loss: 6.51 | Tokens per second: 40313.17 | Training tokens per second (%): 9.13 | MFU (%): 37.75 | TFLOPs: 373.36 | Global batch size: 32 | Global tokens/sec: 161252.66 | Global MFU (%): 37.75 | Global TFLOPs: 1493.43 | 
2025-05-18 22:20:32,579 - root - INFO - Step: 350 | Loss: 6.56 | Tokens per second: 39710.89 | Training tokens per second (%): 9.45 | MFU (%): 37.19 | TFLOPs: 367.78 | Global batch size: 32 | Global tokens/sec: 158843.57 | Global MFU (%): 37.19 | Global TFLOPs: 1471.12 | 
2025-05-18 22:20:36,634 - root - INFO - Step: 360 | Loss: 6.48 | Tokens per second: 40402.78 | Training tokens per second (%): 10.12 | MFU (%): 37.83 | TFLOPs: 374.19 | Global batch size: 32 | Global tokens/sec: 161611.14 | Global MFU (%): 37.83 | Global TFLOPs: 1496.75 | 
2025-05-18 22:20:40,676 - root - INFO - Step: 370 | Loss: 6.55 | Tokens per second: 40546.93 | Training tokens per second (%): 9.15 | MFU (%): 37.97 | TFLOPs: 375.52 | Global batch size: 32 | Global tokens/sec: 162187.73 | Global MFU (%): 37.97 | Global TFLOPs: 1502.09 | 
2025-05-18 22:20:44,663 - root - INFO - Step: 380 | Loss: 6.50 | Tokens per second: 41096.60 | Training tokens per second (%): 8.72 | MFU (%): 38.48 | TFLOPs: 380.61 | Global batch size: 32 | Global tokens/sec: 164386.40 | Global MFU (%): 38.48 | Global TFLOPs: 1522.45 | 
2025-05-18 22:20:48,736 - root - INFO - Step: 390 | Loss: 6.34 | Tokens per second: 40235.93 | Training tokens per second (%): 9.93 | MFU (%): 37.68 | TFLOPs: 372.64 | Global batch size: 32 | Global tokens/sec: 160943.72 | Global MFU (%): 37.68 | Global TFLOPs: 1490.57 | 
2025-05-18 22:20:52,739 - root - INFO - Step: 400 | Loss: 6.36 | Tokens per second: 40939.99 | Training tokens per second (%): 8.77 | MFU (%): 38.34 | TFLOPs: 379.16 | Global batch size: 32 | Global tokens/sec: 163759.96 | Global MFU (%): 38.34 | Global TFLOPs: 1516.65 | 
2025-05-18 22:20:56,755 - root - INFO - Step: 410 | Loss: 6.46 | Tokens per second: 40800.02 | Training tokens per second (%): 9.21 | MFU (%): 38.21 | TFLOPs: 377.87 | Global batch size: 32 | Global tokens/sec: 163200.07 | Global MFU (%): 38.21 | Global TFLOPs: 1511.47 | 
2025-05-18 22:21:00,788 - root - INFO - Step: 420 | Loss: 6.40 | Tokens per second: 40633.63 | Training tokens per second (%): 9.28 | MFU (%): 38.05 | TFLOPs: 376.33 | Global batch size: 32 | Global tokens/sec: 162534.51 | Global MFU (%): 38.05 | Global TFLOPs: 1505.30 | 
2025-05-18 22:21:04,861 - root - INFO - Step: 430 | Loss: 6.26 | Tokens per second: 40231.69 | Training tokens per second (%): 9.74 | MFU (%): 37.67 | TFLOPs: 372.60 | Global batch size: 32 | Global tokens/sec: 160926.75 | Global MFU (%): 37.67 | Global TFLOPs: 1490.41 | 
2025-05-18 22:21:08,942 - root - INFO - Step: 440 | Loss: 6.34 | Tokens per second: 40147.82 | Training tokens per second (%): 9.30 | MFU (%): 37.60 | TFLOPs: 371.83 | Global batch size: 32 | Global tokens/sec: 160591.26 | Global MFU (%): 37.60 | Global TFLOPs: 1487.30 | 
2025-05-18 22:21:13,024 - root - INFO - Step: 450 | Loss: 6.30 | Tokens per second: 40152.27 | Training tokens per second (%): 10.10 | MFU (%): 37.60 | TFLOPs: 371.87 | Global batch size: 32 | Global tokens/sec: 160609.10 | Global MFU (%): 37.60 | Global TFLOPs: 1487.47 | 
2025-05-18 22:21:17,059 - root - INFO - Step: 460 | Loss: 6.33 | Tokens per second: 40601.80 | Training tokens per second (%): 9.54 | MFU (%): 38.02 | TFLOPs: 376.03 | Global batch size: 32 | Global tokens/sec: 162407.19 | Global MFU (%): 38.02 | Global TFLOPs: 1504.12 | 
2025-05-18 22:21:21,105 - root - INFO - Step: 470 | Loss: 6.40 | Tokens per second: 40506.29 | Training tokens per second (%): 8.63 | MFU (%): 37.93 | TFLOPs: 375.15 | Global batch size: 32 | Global tokens/sec: 162025.14 | Global MFU (%): 37.93 | Global TFLOPs: 1500.58 | 
2025-05-18 22:21:25,160 - root - INFO - Step: 480 | Loss: 6.27 | Tokens per second: 40408.61 | Training tokens per second (%): 8.66 | MFU (%): 37.84 | TFLOPs: 374.24 | Global batch size: 32 | Global tokens/sec: 161634.43 | Global MFU (%): 37.84 | Global TFLOPs: 1496.97 | 
2025-05-18 22:21:29,187 - root - INFO - Step: 490 | Loss: 6.24 | Tokens per second: 40690.74 | Training tokens per second (%): 8.49 | MFU (%): 38.10 | TFLOPs: 376.85 | Global batch size: 32 | Global tokens/sec: 162762.96 | Global MFU (%): 38.10 | Global TFLOPs: 1507.42 | 
2025-05-18 22:21:33,207 - root - INFO - Step: 500 | Loss: 6.13 | Tokens per second: 40768.70 | Training tokens per second (%): 9.45 | MFU (%): 38.18 | TFLOPs: 377.58 | Global batch size: 32 | Global tokens/sec: 163074.79 | Global MFU (%): 38.18 | Global TFLOPs: 1510.31 | 
2025-05-18 22:21:37,232 - root - INFO - Step: 510 | Loss: 6.18 | Tokens per second: 40713.15 | Training tokens per second (%): 10.33 | MFU (%): 38.13 | TFLOPs: 377.06 | Global batch size: 32 | Global tokens/sec: 162852.61 | Global MFU (%): 38.13 | Global TFLOPs: 1508.25 | 
2025-05-18 22:21:41,224 - root - INFO - Step: 520 | Loss: 6.08 | Tokens per second: 41048.45 | Training tokens per second (%): 8.84 | MFU (%): 38.44 | TFLOPs: 380.17 | Global batch size: 32 | Global tokens/sec: 164193.79 | Global MFU (%): 38.44 | Global TFLOPs: 1520.67 | 
2025-05-18 22:21:45,220 - root - INFO - Step: 530 | Loss: 6.13 | Tokens per second: 41006.17 | Training tokens per second (%): 9.20 | MFU (%): 38.40 | TFLOPs: 379.78 | Global batch size: 32 | Global tokens/sec: 164024.70 | Global MFU (%): 38.40 | Global TFLOPs: 1519.10 | 
2025-05-18 22:21:49,264 - root - INFO - Step: 540 | Loss: 6.06 | Tokens per second: 40525.50 | Training tokens per second (%): 9.17 | MFU (%): 37.95 | TFLOPs: 375.32 | Global batch size: 32 | Global tokens/sec: 162101.98 | Global MFU (%): 37.95 | Global TFLOPs: 1501.30 | 
2025-05-18 22:21:53,252 - root - INFO - Step: 550 | Loss: 6.13 | Tokens per second: 41085.83 | Training tokens per second (%): 10.08 | MFU (%): 38.47 | TFLOPs: 380.51 | Global batch size: 32 | Global tokens/sec: 164343.31 | Global MFU (%): 38.47 | Global TFLOPs: 1522.05 | 
2025-05-18 22:21:57,291 - root - INFO - Step: 560 | Loss: 6.07 | Tokens per second: 40571.56 | Training tokens per second (%): 10.97 | MFU (%): 37.99 | TFLOPs: 375.75 | Global batch size: 32 | Global tokens/sec: 162286.25 | Global MFU (%): 37.99 | Global TFLOPs: 1503.00 | 
2025-05-18 22:22:01,337 - root - INFO - Step: 570 | Loss: 6.04 | Tokens per second: 40496.51 | Training tokens per second (%): 9.70 | MFU (%): 37.92 | TFLOPs: 375.06 | Global batch size: 32 | Global tokens/sec: 161986.05 | Global MFU (%): 37.92 | Global TFLOPs: 1500.22 | 
2025-05-18 22:22:05,355 - root - INFO - Step: 580 | Loss: 6.26 | Tokens per second: 40784.51 | Training tokens per second (%): 9.36 | MFU (%): 38.19 | TFLOPs: 377.72 | Global batch size: 32 | Global tokens/sec: 163138.02 | Global MFU (%): 38.19 | Global TFLOPs: 1510.89 | 
2025-05-18 22:22:09,346 - root - INFO - Step: 590 | Loss: 6.14 | Tokens per second: 41059.74 | Training tokens per second (%): 9.12 | MFU (%): 38.45 | TFLOPs: 380.27 | Global batch size: 32 | Global tokens/sec: 164238.95 | Global MFU (%): 38.45 | Global TFLOPs: 1521.09 | 
2025-05-18 22:22:13,398 - root - INFO - Step: 600 | Loss: 6.02 | Tokens per second: 40439.16 | Training tokens per second (%): 9.04 | MFU (%): 37.87 | TFLOPs: 374.52 | Global batch size: 32 | Global tokens/sec: 161756.64 | Global MFU (%): 37.87 | Global TFLOPs: 1498.10 | 
2025-05-18 22:22:17,413 - root - INFO - Step: 610 | Loss: 5.99 | Tokens per second: 40816.76 | Training tokens per second (%): 9.82 | MFU (%): 38.22 | TFLOPs: 378.02 | Global batch size: 32 | Global tokens/sec: 163267.06 | Global MFU (%): 38.22 | Global TFLOPs: 1512.09 | 
2025-05-18 22:22:21,437 - root - INFO - Step: 620 | Loss: 5.96 | Tokens per second: 40721.85 | Training tokens per second (%): 8.71 | MFU (%): 38.13 | TFLOPs: 377.14 | Global batch size: 32 | Global tokens/sec: 162887.42 | Global MFU (%): 38.13 | Global TFLOPs: 1508.57 | 
2025-05-18 22:22:25,510 - root - INFO - Step: 630 | Loss: 6.00 | Tokens per second: 40235.47 | Training tokens per second (%): 9.14 | MFU (%): 37.68 | TFLOPs: 372.64 | Global batch size: 32 | Global tokens/sec: 160941.87 | Global MFU (%): 37.68 | Global TFLOPs: 1490.55 | 
2025-05-18 22:22:29,520 - root - INFO - Step: 640 | Loss: 5.95 | Tokens per second: 40865.75 | Training tokens per second (%): 7.74 | MFU (%): 38.27 | TFLOPs: 378.48 | Global batch size: 32 | Global tokens/sec: 163463.01 | Global MFU (%): 38.27 | Global TFLOPs: 1513.90 | 
2025-05-18 22:22:33,636 - root - INFO - Step: 650 | Loss: 6.01 | Tokens per second: 39805.75 | Training tokens per second (%): 8.66 | MFU (%): 37.28 | TFLOPs: 368.66 | Global batch size: 32 | Global tokens/sec: 159222.99 | Global MFU (%): 37.28 | Global TFLOPs: 1474.63 | 
2025-05-18 22:22:37,651 - root - INFO - Step: 660 | Loss: 5.93 | Tokens per second: 40817.73 | Training tokens per second (%): 9.66 | MFU (%): 38.22 | TFLOPs: 378.03 | Global batch size: 32 | Global tokens/sec: 163270.92 | Global MFU (%): 38.22 | Global TFLOPs: 1512.12 | 
2025-05-18 22:22:41,729 - root - INFO - Step: 670 | Loss: 5.95 | Tokens per second: 40179.09 | Training tokens per second (%): 10.10 | MFU (%): 37.63 | TFLOPs: 372.12 | Global batch size: 32 | Global tokens/sec: 160716.38 | Global MFU (%): 37.63 | Global TFLOPs: 1488.46 | 
2025-05-18 22:22:45,787 - root - INFO - Step: 680 | Loss: 6.07 | Tokens per second: 40384.33 | Training tokens per second (%): 9.50 | MFU (%): 37.82 | TFLOPs: 374.02 | Global batch size: 32 | Global tokens/sec: 161537.30 | Global MFU (%): 37.82 | Global TFLOPs: 1496.07 | 
2025-05-18 22:22:49,812 - root - INFO - Step: 690 | Loss: 5.76 | Tokens per second: 40716.95 | Training tokens per second (%): 9.68 | MFU (%): 38.13 | TFLOPs: 377.10 | Global batch size: 32 | Global tokens/sec: 162867.78 | Global MFU (%): 38.13 | Global TFLOPs: 1508.39 | 
2025-05-18 22:22:53,871 - root - INFO - Step: 700 | Loss: 6.06 | Tokens per second: 40370.93 | Training tokens per second (%): 10.61 | MFU (%): 37.81 | TFLOPs: 373.89 | Global batch size: 32 | Global tokens/sec: 161483.71 | Global MFU (%): 37.81 | Global TFLOPs: 1495.57 | 
2025-05-18 22:22:57,865 - root - INFO - Step: 710 | Loss: 5.91 | Tokens per second: 41021.44 | Training tokens per second (%): 9.00 | MFU (%): 38.41 | TFLOPs: 379.92 | Global batch size: 32 | Global tokens/sec: 164085.76 | Global MFU (%): 38.41 | Global TFLOPs: 1519.67 | 
2025-05-18 22:23:02,136 - root - INFO - Step: 720 | Loss: 5.85 | Tokens per second: 38369.20 | Training tokens per second (%): 9.53 | MFU (%): 35.93 | TFLOPs: 355.35 | Global batch size: 32 | Global tokens/sec: 153476.80 | Global MFU (%): 35.93 | Global TFLOPs: 1421.41 | 
2025-05-18 22:23:06,136 - root - INFO - Step: 730 | Loss: 5.77 | Tokens per second: 40967.43 | Training tokens per second (%): 9.91 | MFU (%): 38.36 | TFLOPs: 379.42 | Global batch size: 32 | Global tokens/sec: 163869.73 | Global MFU (%): 38.36 | Global TFLOPs: 1517.67 | 
2025-05-18 22:23:10,304 - root - INFO - Step: 740 | Loss: 5.94 | Tokens per second: 39313.84 | Training tokens per second (%): 9.53 | MFU (%): 36.82 | TFLOPs: 364.10 | Global batch size: 32 | Global tokens/sec: 157255.35 | Global MFU (%): 36.82 | Global TFLOPs: 1456.41 | 
2025-05-18 22:23:14,327 - root - INFO - Step: 750 | Loss: 5.84 | Tokens per second: 40739.63 | Training tokens per second (%): 9.37 | MFU (%): 38.15 | TFLOPs: 377.31 | Global batch size: 32 | Global tokens/sec: 162958.54 | Global MFU (%): 38.15 | Global TFLOPs: 1509.23 | 
2025-05-18 22:23:18,381 - root - INFO - Step: 760 | Loss: 5.85 | Tokens per second: 40411.85 | Training tokens per second (%): 10.42 | MFU (%): 37.84 | TFLOPs: 374.27 | Global batch size: 32 | Global tokens/sec: 161647.39 | Global MFU (%): 37.84 | Global TFLOPs: 1497.09 | 
2025-05-18 22:23:22,405 - root - INFO - Step: 770 | Loss: 5.75 | Tokens per second: 40724.87 | Training tokens per second (%): 9.29 | MFU (%): 38.14 | TFLOPs: 377.17 | Global batch size: 32 | Global tokens/sec: 162899.48 | Global MFU (%): 38.14 | Global TFLOPs: 1508.68 | 
2025-05-18 22:23:26,404 - root - INFO - Step: 780 | Loss: 5.81 | Tokens per second: 40979.98 | Training tokens per second (%): 7.66 | MFU (%): 38.38 | TFLOPs: 379.53 | Global batch size: 32 | Global tokens/sec: 163919.92 | Global MFU (%): 38.38 | Global TFLOPs: 1518.13 | 
2025-05-18 22:23:30,436 - root - INFO - Step: 790 | Loss: 5.69 | Tokens per second: 40642.42 | Training tokens per second (%): 8.83 | MFU (%): 38.06 | TFLOPs: 376.41 | Global batch size: 32 | Global tokens/sec: 162569.70 | Global MFU (%): 38.06 | Global TFLOPs: 1505.63 | 
2025-05-18 22:23:34,445 - root - INFO - Step: 800 | Loss: 5.75 | Tokens per second: 40873.26 | Training tokens per second (%): 9.90 | MFU (%): 38.28 | TFLOPs: 378.54 | Global batch size: 32 | Global tokens/sec: 163493.05 | Global MFU (%): 38.28 | Global TFLOPs: 1514.18 | 
2025-05-18 22:23:38,517 - root - INFO - Step: 810 | Loss: 5.94 | Tokens per second: 40245.06 | Training tokens per second (%): 10.65 | MFU (%): 37.69 | TFLOPs: 372.73 | Global batch size: 32 | Global tokens/sec: 160980.26 | Global MFU (%): 37.69 | Global TFLOPs: 1490.91 | 
2025-05-18 22:23:42,555 - root - INFO - Step: 820 | Loss: 5.83 | Tokens per second: 40581.39 | Training tokens per second (%): 8.70 | MFU (%): 38.00 | TFLOPs: 375.84 | Global batch size: 32 | Global tokens/sec: 162325.57 | Global MFU (%): 38.00 | Global TFLOPs: 1503.37 | 
2025-05-18 22:23:46,578 - root - INFO - Step: 830 | Loss: 5.80 | Tokens per second: 40732.07 | Training tokens per second (%): 10.27 | MFU (%): 38.14 | TFLOPs: 377.24 | Global batch size: 32 | Global tokens/sec: 162928.28 | Global MFU (%): 38.14 | Global TFLOPs: 1508.95 | 
2025-05-18 22:23:50,599 - root - INFO - Step: 840 | Loss: 5.74 | Tokens per second: 40754.01 | Training tokens per second (%): 10.81 | MFU (%): 38.16 | TFLOPs: 377.44 | Global batch size: 32 | Global tokens/sec: 163016.04 | Global MFU (%): 38.16 | Global TFLOPs: 1509.76 | 
2025-05-18 22:23:54,615 - root - INFO - Step: 850 | Loss: 5.76 | Tokens per second: 40801.03 | Training tokens per second (%): 9.40 | MFU (%): 38.21 | TFLOPs: 377.88 | Global batch size: 32 | Global tokens/sec: 163204.11 | Global MFU (%): 38.21 | Global TFLOPs: 1511.50 | 
2025-05-18 22:23:58,640 - root - INFO - Step: 860 | Loss: 5.71 | Tokens per second: 40714.54 | Training tokens per second (%): 9.69 | MFU (%): 38.13 | TFLOPs: 377.07 | Global batch size: 32 | Global tokens/sec: 162858.14 | Global MFU (%): 38.13 | Global TFLOPs: 1508.30 | 
2025-05-18 22:24:02,937 - root - INFO - Step: 870 | Loss: 5.63 | Tokens per second: 38129.99 | Training tokens per second (%): 9.69 | MFU (%): 35.71 | TFLOPs: 353.14 | Global batch size: 32 | Global tokens/sec: 152519.96 | Global MFU (%): 35.71 | Global TFLOPs: 1412.55 | 
2025-05-18 22:24:07,039 - root - INFO - Step: 880 | Loss: 5.58 | Tokens per second: 39949.43 | Training tokens per second (%): 9.95 | MFU (%): 37.41 | TFLOPs: 369.99 | Global batch size: 32 | Global tokens/sec: 159797.73 | Global MFU (%): 37.41 | Global TFLOPs: 1479.96 | 
2025-05-18 22:24:11,053 - root - INFO - Step: 890 | Loss: 5.72 | Tokens per second: 40827.98 | Training tokens per second (%): 9.76 | MFU (%): 38.23 | TFLOPs: 378.13 | Global batch size: 32 | Global tokens/sec: 163311.91 | Global MFU (%): 38.23 | Global TFLOPs: 1512.50 | 
2025-05-18 22:24:15,151 - root - INFO - Step: 900 | Loss: 5.58 | Tokens per second: 39986.12 | Training tokens per second (%): 10.85 | MFU (%): 37.44 | TFLOPs: 370.33 | Global batch size: 32 | Global tokens/sec: 159944.48 | Global MFU (%): 37.44 | Global TFLOPs: 1481.31 | 
2025-05-18 22:24:19,285 - root - INFO - Step: 910 | Loss: 5.63 | Tokens per second: 39633.79 | Training tokens per second (%): 9.44 | MFU (%): 37.11 | TFLOPs: 367.07 | Global batch size: 32 | Global tokens/sec: 158535.14 | Global MFU (%): 37.11 | Global TFLOPs: 1468.26 | 
2025-05-18 22:24:23,349 - root - INFO - Step: 920 | Loss: 5.78 | Tokens per second: 40321.39 | Training tokens per second (%): 10.11 | MFU (%): 37.76 | TFLOPs: 373.43 | Global batch size: 32 | Global tokens/sec: 161285.58 | Global MFU (%): 37.76 | Global TFLOPs: 1493.73 | 
2025-05-18 22:24:27,377 - root - INFO - Step: 930 | Loss: 5.67 | Tokens per second: 40684.63 | Training tokens per second (%): 9.81 | MFU (%): 38.10 | TFLOPs: 376.80 | Global batch size: 32 | Global tokens/sec: 162738.52 | Global MFU (%): 38.10 | Global TFLOPs: 1507.19 | 
2025-05-18 22:24:31,411 - root - INFO - Step: 940 | Loss: 5.66 | Tokens per second: 40621.47 | Training tokens per second (%): 10.01 | MFU (%): 38.04 | TFLOPs: 376.21 | Global batch size: 32 | Global tokens/sec: 162485.87 | Global MFU (%): 38.04 | Global TFLOPs: 1504.85 | 
2025-05-18 22:24:35,458 - root - INFO - Step: 950 | Loss: 5.67 | Tokens per second: 40497.05 | Training tokens per second (%): 10.51 | MFU (%): 37.92 | TFLOPs: 375.06 | Global batch size: 32 | Global tokens/sec: 161988.21 | Global MFU (%): 37.92 | Global TFLOPs: 1500.24 | 
2025-05-18 22:24:39,482 - root - INFO - Step: 960 | Loss: 5.80 | Tokens per second: 40719.21 | Training tokens per second (%): 8.09 | MFU (%): 38.13 | TFLOPs: 377.12 | Global batch size: 32 | Global tokens/sec: 162876.84 | Global MFU (%): 38.13 | Global TFLOPs: 1508.47 | 
2025-05-18 22:24:43,515 - root - INFO - Step: 970 | Loss: 5.69 | Tokens per second: 40632.76 | Training tokens per second (%): 8.87 | MFU (%): 38.05 | TFLOPs: 376.32 | Global batch size: 32 | Global tokens/sec: 162531.04 | Global MFU (%): 38.05 | Global TFLOPs: 1505.27 | 
2025-05-18 22:24:47,564 - root - INFO - Step: 980 | Loss: 5.68 | Tokens per second: 40465.79 | Training tokens per second (%): 8.19 | MFU (%): 37.89 | TFLOPs: 374.77 | Global batch size: 32 | Global tokens/sec: 161863.15 | Global MFU (%): 37.89 | Global TFLOPs: 1499.08 | 
2025-05-18 22:24:51,576 - root - INFO - Step: 990 | Loss: 5.58 | Tokens per second: 40851.25 | Training tokens per second (%): 9.13 | MFU (%): 38.25 | TFLOPs: 378.34 | Global batch size: 32 | Global tokens/sec: 163405.00 | Global MFU (%): 38.25 | Global TFLOPs: 1513.36 | 
2025-05-18 22:24:55,727 - root - INFO - Step: 1000 | Loss: 5.61 | Tokens per second: 39469.41 | Training tokens per second (%): 8.58 | MFU (%): 36.96 | TFLOPs: 365.54 | Global batch size: 32 | Global tokens/sec: 157877.63 | Global MFU (%): 36.96 | Global TFLOPs: 1462.17 | 
2025-05-18 22:24:55,728 - root - INFO - Training completed
[sbatch-master] task finished
